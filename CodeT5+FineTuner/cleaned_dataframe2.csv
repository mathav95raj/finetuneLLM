question,answers
How do I find the price of a group at 50 percentile of units in pandas?,"'you can try:\ndf[df.Unit == df.Unit.quantile(0.5)][\'Price\']\n\nBut it won\'t work most of the times if the data is sparse.\nAnother suggestion would be rank all the values in the Unit column, and select the middle one:\ndf[\'rank\'] = df[\'Unit\'].rank(method = \'first\') # rank values of \'unit\' column\ndf[df[\'rank\'] == len(df)//2] # filter by middle value \n\n'"
How to proceed with `None` value in pandas fillna,"""Solution: use pandas pd.NA not base Python None\ndf = pd.DataFrame({'first_name':pd.NA, 'last_name':pd.NA, 'created_at':pd.NA})\n\ndf.fillna(value={'first_name':'Andrii', 'last_name':'Furmanets', 'created_at':pd.NA})\n\nGenerally it's better to leave pandas NA as-is. Do not try to change it. The presence of NA is a feature, not an issue. NA gets handled correctly in other pandas functions (but not numpy)\n\nIf you insist that python None should replace pandas NA's for some downstream reason, show us the missing code that follows where NA is causing an issue; that's usually an XY problem.\n\n"", ""In case you want to normalize all of the nulls with python's None.\ndf.fillna(np.nan).replace([np.nan], [None])\n\nThe first fillna will replace all of (None, NAT, np.nan, etc) with Numpy's NaN, then replace Numpy's NaN with python's None.\n"", ""An alternative method to fillna with None. I am on pandas 0.24.0 and I am doing this to insert NULL values to POSTGRES database.\n\n# Stealing @pIRSquared dataframe\ndf = pd.DataFrame(dict(A=[1, None], B=[None, 2], C=[None, 'D']))\n\ndf\n\n     A    B     C\n0  1.0  NaN  None\n1  NaN  2.0     D\n\n# fill NaN with None. Basically it says, fill with None whenever you see NULL value.\ndf['A'] = np.where(df['A'].isnull(), None, df['A'])\ndf['B'] = np.where(df['B'].isnull(), None, df['B'])\n\n# Result\ndf\n\n     A    B     C\n0  1.0  None  None\n1  None  2.0     D\n\n\n"", 'Setup\nConsider the sample dataframe df\n\ndf = pd.DataFrame(dict(A=[1, None], B=[None, 2], C=[None, \'D\']))\n\ndf\n\n     A    B     C\n0  1.0  NaN  None\n1  NaN  2.0     D\n\n\nI can confirm the error\n\ndf.fillna(dict(A=1, B=None, C=4))\n\n\n\nValueError: must specify a fill method or value\n\n\n\nThis happens because pandas is cycling through keys in the dictionary and executing a fillna for each relevant column.  If you look at the signature of the pd.Series.fillna method\n\n\nSeries.fillna(value=None, method=None, axis=None, inplace=False, limit=None, downcast=None, **kwargs)\n\n\n\nYou\'ll see the default value is None.  So we can replicate this error with\n\ndf.A.fillna(None)\n\n\nOr equivalently\n\ndf.A.fillna()\n\n\nI\'ll add that I\'m not terribly surprised considering that you are attempting to fill a null value with a null value.\n\n\n\nWhat you need is a work around\n\nSolution\nUse pd.DataFrame.fillna over columns that you want to fill with non-null values.  Then follow that up with a pd.DataFrame.replace on the specific columns you want to swap one null value with another.\n\ndf.fillna(dict(A=1, C=2)).replace(dict(B={np.nan: None}))\n\n     A     B  C\n0  1.0  None  2\n1  1.0     2  D\n\n', ""What type of data structure are you using? This works for a pandas Series:\n\nimport pandas as pd\n\nd = pd.Series({'first_name': 'Andrii', 'last_name':'Furmanets', 'created_at':None})\nd = d.fillna('DATE')\n\n"""
"How to open a .tsv file in Jupyter? Jupyter.Notebook tried suggestions, but it doesn&#39;t work","""If you're opening .tsv file from your local, please attempt the below code:\nimport pandas as pd\ndf=pd.read_csv(r'c:\\User\\anna\\train.tsv', sep='\\t')\nprint(df)\n"", ""it sounds lame, but finally i opened it - my first attempt to open anything in anaconda.: )\n\nthanks. \n\ndf=pd.read_csv('C:/User/anna/train.tsv',sep='\\t')\n\n"", ""it's actually to do with pandas, by default the separator is comma, not tab.\ntry the code below:\n\ndf=pd.read_csv('C:/User/anna/train', sep='\\t')\n\n"""
Rolling up data to higher organization level,"""Here's a pretty neat one-liner for you you trying to do without creating a function.\n(pd.concat([df.where((df == x).iloc[:,::-1].cumsum(1).astype(bool)).melt().dropna() \n           for x in dfr['Org ID']], keys=dfr['Report Name'])\n   .reset_index()\n   .drop('level_1', axis=1))\n\nOutput:\n  Report Name          variable  value\n0         ABC    Super Division  123.0\n1         ABC          Division  342.0\n2         DEF    Super Division  345.0\n3         DEF          Division  453.0\n4         DEF  Super Department  234.0\n5         GHI    Super Division  123.0\n\nDetails:\nUsing reversing element, ::-1, I going to keep all values left of True on each row with cumsum.  Then, use this boolean matrix to return only those True values using where.  Next we, use melt to convert from wide to long, then pd.concat the above results by row using list comprehension and last reset_index and drop the level_1 column.\n"""
Drop unchanged rows after concate operation in Pandas,"""Remove the rows where ""X"" and ""Y"" are the same:\ndf = pd.concat([df1.set_index('Id'), df2.set_index('Id')], axis=1, keys=['X','Y'])\n\n>>> df[df[""X""].ne(df[""Y""]).any(axis=1)]\n          X              Y      \n   Quantity Price Quantity Price\nId                              \n2        20    80     25.0  75.0\n3        30    90     20.0  90.0\n4        40   150      NaN   NaN\n\n"""
What is the difference between NaN and None?,"'NaN is used as a placeholder for missing data consistently in pandas, consistency is good. I usually read/translate NaN as ""missing"". Also see the \'working with missing data\' section in the docs.\nWes writes in the docs \'choice of NA-representation\':\n\nAfter years of production use [NaN] has proven, at least in my opinion, to be the best decision given the state of affairs in NumPy and Python in general. The special value NaN (Not-A-Number) is used everywhere as the NA value, and there are API functions isnull and notnull which can be used across the dtypes to detect NA values.\n...\nThus, I have chosen the Pythonic “practicality beats purity” approach and traded integer NA capability for a much simpler approach of using a special value in float and object arrays to denote NA, and promoting integer arrays to floating when NAs must be introduced.\n\nNote: the ""gotcha"" that integer Series containing missing data are upcast to floats.\nIn my opinion the main reason to use NaN (over None) is that it can be stored with numpy\'s float64 dtype, rather than the less efficient object dtype, see NA type promotions.\n#  without forcing dtype it changes None to NaN!\ns_bad = pd.Series([1, None], dtype=object)\ns_good = pd.Series([1, np.nan])\n\nIn [13]: s_bad.dtype\nOut[13]: dtype(\'O\')\n\nIn [14]: s_good.dtype\nOut[14]: dtype(\'float64\')\n\nJeff comments (below) on this:\n\nnp.nan allows for vectorized operations; its a float value, while None, by definition, forces object type, which basically disables all efficiency in numpy.\n\nSo repeat 3 times fast: object==bad, float==good\n\n\nSaying that, many operations may still work just as well with None vs NaN (but perhaps are not supported i.e. they may sometimes give surprising results):\nIn [15]: s_bad.sum()\nOut[15]: 1\n\nIn [16]: s_good.sum()\nOut[16]: 1.0\n\n\nTo answer the second question:\nYou should be using isnull and notnull to test for missing data (NaN).\n', 'NaN can be used as a numerical value on mathematical operations, while None cannot (or at least shouldn\'t).\n\nNaN is a numeric value, as defined in IEEE 754 floating-point standard.\nNone is an internal Python type (NoneType) and would be more like ""inexistent"" or ""empty"" than ""numerically invalid"" in this context.\n\nThe main ""symptom"" of that is that, if you perform, say, an average or a sum on an array containing NaN, even a single one, you get NaN as a result...\n\nIn the other hand, you cannot perform mathematical operations using None as operand.\n\nSo, depending on the case, you could use None as a way to tell your algorithm not to consider invalid or inexistent values on computations. That would mean the algorithm should test each value to see if it is None.\n\nNumpy has some functions to avoid NaN values to contaminate your results, such as nansum and nan_to_num for example.\n', 'Below are the differences:\n\n\nnan belongs to the class float\nNone belongs to the class NoneType\n\n\nI found the below article very helpful:\nhttps://medium.com/analytics-vidhya/dealing-with-missing-values-nan-and-none-in-python-6fc9b8fb4f31\n', 'The function isnan() checks to see if something is ""Not A Number"" and will return whether or not a variable is a number, for example isnan(2) would return false\n\nThe conditional myVar is not None returns whether or not the variable is defined\n\nYour numpy array uses isnan() because it is intended to be an array of numbers and it initializes all elements of the array to NaN these elements are considered ""empty""\n'"
How can I create a matrix with the categories and values from several excel files in a folder,"'I think something like this should do  what you need.\nimport pandas as pd\nfrom pathlib import Path\n\ndef reset_columns(df):\n    df = df.copy()\n    df.columns = pd.RangeIndex(df.columns.size)\n    return df\n\ndef flatten_person_df(df):\n    return pd.concat(\n        reset_columns(df[df.columns[i:i+2]])\n        for i in range(0, len(df.columns), 2)\n    ).dropna(how=""all"").reset_index(drop=True)\n\ndef combine_files(directory):\n    return pd.concat(\n        flatten_person_df(pd.read_excel(f, header=None)).set_index(0).transpose().assign(filename=f)\n        for f in Path(directory).glob(""*.xlsx"")\n    )\n\nYou can call combine_files on the directory containing the files. For example,\nresult = combine_files(""./data/"")\n\n'"
How to use np.where with a column name as one of the values?,"'Funnily enough copying and pasting your code gave me the expected result. But in your code, the time I believe is correct but it is using unix time. You can enforce the datetime format and that should give you the expected answer.\nc1 = df[""END DATE""].isnull()\nc2 = df[""TYPE""] == ""Hire""\nc3 = df[""TYPE""] == ""Retire""\n\ndf = df.copy(deep = True)\n\nx = datetime.datetime.strptime(""2023"" + ""-"" + ""12"" + ""-"" + ""31"", \'%Y-%m-%d\').date()\ny = pd.to_datetime(df[\'END DATE\']).dt.strftime(\'%Y-%m-%d\')\n# y = df[\'END DATE\']\ndf[""END DATE""] = np.where(((c2 | c3) & c1) , x, y)\n\nNow looking at the df gives:\n\n\n\n\n\nID\nState\nTYPE\nSTART DATE\nEND DATE\n\n\n\n\n0\n863\nMI\nHire\n9/01/23\n2023-12-31\n\n\n1\n224\nWI\nRetire\n9/01/23\n2023-12-31\n\n\n2\n567\nFL\nTransfer\n10/01/23\n2023-10-31\n\n\n3\n345\nNC\nNaN\nNaN\nNaN\n\n\n4\n432\nNY\nNaN\nNaN\nNaN\n\n\n\n\nYou can change the format of the END DATE by passing different formats to dt.strftime.\n'"
Attribute Error due to filtering a dataset based on a certain period that does not exist in the dataset,"""The errors says that you are trying to run a non-existent method, not that you don't have the appropriate data in the dataframe.\nThe correct syntax:\ndf = df[df['quarter'].apply(str).str.contains(ss)] \n\nIf you don't have a certain period in the dataframe, the result will be just empty.\n"""
Convert a dataframe column to timestamp format,"'To convert the values in the \'time\' column from \'DD/MM/YYYY HH:MM\' format to \'YYYY-MM-DD HH:MM:SS+01:00\' format, you can use the to_datetime function from the pandas library.\nimport pandas as pd\n\n# Example dataframe with \'time\' column in \'DD/MM/YYYY HH:MM\' format\ndf = pd.DataFrame({\'time\': [\'01/01/2022 09:00\', \'02/01/2022 10:30\', \'03/01/2022 12:45\']})\n\n# Convert \'time\' column to datetime format\ndf[\'time\'] = pd.to_datetime(df[\'time\'], format=\'%d/%m/%Y %H:%M\')\n\n# Convert \'time\' column to \'YYYY-MM-DD HH:MM:SS+01:00\' format\ndf[\'time\'] = df[\'time\'].dt.strftime(\'%Y-%m-%d %H:%M:%S+01:00\')\n\nprint(df)\n\nOutput:\n                        time\n0  2022-01-01 09:00:00+01:00\n1  2022-01-02 10:30:00+01:00\n2  2022-01-03 12:45:00+01:00\n\n', ""df['time'] = pd.to_datetime(df['time'], format='%d/%m/%Y %H:%M').dt.strftime('%Y-%m-%d %H:%M:%S+01:00')\n\nResult:\n0    2023-07-01 10:30:00+01:00\n\n"""
Plot correlation matrix using pandas,"'If your main goal is to visualize the correlation matrix, rather than creating a plot per se, the convenient pandas styling options is a viable built-in solution:\nimport pandas as pd\nimport numpy as np\n\nrs = np.random.RandomState(0)\ndf = pd.DataFrame(rs.rand(10, 10))\ncorr = df.corr()\ncorr.style.background_gradient(cmap=\'coolwarm\')\n# \'RdBu_r\', \'BrBG_r\', & PuOr_r are other good diverging colormaps\n\n\nNote that this needs to be in a backend that supports rendering HTML, such as the JupyterLab Notebook.\n\nStyling\nYou can easily limit the digit precision (this is now .format(precision=2) in pandas 2.*):\ncorr.style.background_gradient(cmap=\'coolwarm\').set_precision(2)\n\n\nOr get rid of the digits altogether if you prefer the matrix without annotations:\ncorr.style.background_gradient(cmap=\'coolwarm\').set_properties(**{\'font-size\': \'0pt\'})\n\n\nThe styling documentation also includes instructions of more advanced styles, such as how to change the display of the cell the mouse pointer is hovering over.\n\nTime comparison\nIn my testing, style.background_gradient() was 4x faster than plt.matshow() and 120x faster than sns.heatmap() with a 10x10 matrix. Unfortunately it doesn\'t scale as well as plt.matshow(): the two take about the same time for a 100x100 matrix, and plt.matshow() is 10x faster for a 1000x1000 matrix.\n\nSaving\nThere are a few possible ways to save the stylized dataframe:\n\nReturn the HTML by appending the render() method and then write the output to a file.\nSave as an .xslx file with conditional formatting by appending the to_excel() method.\nCombine with imgkit to save a bitmap\nTake a screenshot (like I have done here).\n\n\nNormalize colors across the entire matrix (pandas >= 0.24)\nBy setting axis=None, it is now possible to compute the colors based on the entire matrix rather than per column or per row:\ncorr.style.background_gradient(cmap=\'coolwarm\', axis=None)\n\n\n\nSingle corner heatmap\nSince many people are reading this answer I thought I would add a tip for how to only show one corner of the correlation matrix. I find this easier to read myself, since it removes the redundant information.\n# Fill diagonal and upper half with NaNs\nmask = np.zeros_like(corr, dtype=bool)\nmask[np.triu_indices_from(mask)] = True\ncorr[mask] = np.nan\n(corr\n .style\n .background_gradient(cmap=\'coolwarm\', axis=None, vmin=-1, vmax=1)\n .highlight_null(color=\'#f1f1f1\')  # Color NaNs grey\n .format(precision=2))\n\n\n', 'There are a lot of useful answers. I just want to add a way of visualizing the correlation matrix. Because sometimes the colors do not clear for you, heatmap library can plot a correlation matrix that displays square sizes for each correlation measurement.\nimport matplotlib.pyplot as plt\nfrom heatmap import corrplot\n\nplt.figure(figsize=(15, 15))\ncorrplot(df.corr())\n\n\n', 'When working with correlations between a large number of features I find it useful to cluster related features together.  This can be done with the seaborn clustermap plot.\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ng = sns.clustermap(df.corr(), \n                   method = \'complete\', \n                   cmap   = \'RdBu\', \n                   annot  = True, \n                   annot_kws = {\'size\': 8})\nplt.setp(g.ax_heatmap.get_xticklabels(), rotation=60);\n\n\nThe clustermap function uses hierarchical clustering to arrange relevant features together and produce the tree-like dendrograms.\nThere are two notable clusters in this plot:\n\ny_des and dew.point_des\nirradiance, y_seasonal and dew.point_seasonal\n\n\nFWIW the meteorological data to generate this figure can be accessed with this Jupyter notebook.\n', 'You can observe the relation between features either by drawing a heat map from seaborn or scatter matrix from pandas.\nScatter Matrix:\npd.scatter_matrix(dataframe, alpha = 0.3, figsize = (14,8), diagonal = \'kde\');\n\nIf you want to visualize each feature\'s skewness as well - use seaborn pairplots.\nsns.pairplot(dataframe)\n\nSns Heatmap:\nimport seaborn as sns\n\nf, ax = pl.subplots(figsize=(10, 8))\ncorr = dataframe.corr()\nsns.heatmap(corr,\n    cmap=sns.diverging_palette(220, 10, as_cmap=True),\n    vmin=-1.0, vmax=1.0,\n    square=True, ax=ax)\n\nThe output will be a correlation map of the features. i.e. see the below example.\n\nThe correlation between grocery and detergents is high. Similarly:\nPdoducts With High Correlation:\n\nGrocery and Detergents.\n\nProducts With Medium Correlation:\n\nMilk and Grocery\nMilk and Detergents_Paper\n\nProducts With Low Correlation:\n\nMilk and Deli\nFrozen and Fresh.\nFrozen and Deli.\n\nFrom Pairplots: You can observe same set of relations from pairplots or scatter matrix. But from these we can say that whether the data is normally distributed or not.\n\nNote: The above is same graph taken from the data, which is used to draw heatmap.\n', ""I would prefer to do it with Plotly because it's more interactive charts and it would be easier to understand. You can use the following snippet.\nimport plotly.express as px\n\ndef plotly_corr_plot(df,w,h):\n    fig = px.imshow(df.corr())\n    fig.update_layout(\n        autosize=False,\n        width=w,\n        height=h,)\n    fig.show()\n\n"", 'I think there are many good answers but I added this answer to those who need to deal with specific columns and to show a different plot.\nimport numpy as np\nimport seaborn as sns\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\nrs = np.random.RandomState(0)\ndf = pd.DataFrame(rs.rand(18, 18))\ndf= df.iloc[: , [3,4,5,6,7,8,9,10,11,12,13,14,17]].copy()\ncorr = df.corr()\nplt.figure(figsize=(11,8))\nsns.heatmap(corr, cmap=""Greens"",annot=True)\nplt.show()\n\n\n', ""corrmatrix = df.corr()\ncorrmatrix *= np.tri(*corrmatrix.values.shape, k=-1).T\ncorrmatrix = corrmatrix.stack().sort_values(ascending = False).reset_index()\ncorrmatrix.columns = ['Признак 1', 'Признак 2', 'Корреляция']\ncorrmatrix[(corrmatrix['Корреляция'] >= 0.7) + (corrmatrix['Корреляция'] <= -0.7)]\ndrop_columns = corrmatrix[(corrmatrix['Корреляция'] >= 0.82) + (corrmatrix['Корреляция'] <= -0.7)]['Признак 2']\ndf.drop(drop_columns, axis=1, inplace=True)\ncorrmatrix[(corrmatrix['Корреляция'] >= 0.7) + (corrmatrix['Корреляция'] <= -0.7)]\n\n"", 'Try this function, which also displays variable names for the correlation matrix:\ndef plot_corr(df,size=10):\n    """"""Function plots a graphical correlation matrix for each pair of columns in the dataframe.\n\n    Input:\n        df: pandas DataFrame\n        size: vertical and horizontal size of the plot\n    """"""\n\n    corr = df.corr()\n    fig, ax = plt.subplots(figsize=(size, size))\n    ax.matshow(corr)\n    plt.xticks(range(len(corr.columns)), corr.columns)\n    plt.yticks(range(len(corr.columns)), corr.columns)\n\n', ""Please check below readable code\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(36, 26))\nheatmap = sns.heatmap(df.corr(), vmin=-1, vmax=1, annot=True)\nheatmap.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12)```\n\n  [1]: https://i.stack.imgur.com/I5SeR.png\n\n"", 'You can use heatmap() from seaborn to see the correlation b/w different features:\nimport matplot.pyplot as plt\nimport seaborn as sns\n\nco_matrics=dataframe.corr()\nplot.figure(figsize=(15,20))\nsns.heatmap(co_matrix, square=True, cbar_kws={""shrink"": .5})\n\n', 'You can use pyplot.matshow()  from matplotlib:\nimport matplotlib.pyplot as plt\n\nplt.matshow(dataframe.corr())\nplt.show()\n\n\nEdit:\nIn the comments was a request for how to change the axis tick labels. Here\'s a deluxe version that is drawn on a bigger figure size, has axis labels to match the dataframe, and a colorbar legend to interpret the color scale.\nI\'m including how to adjust the size and rotation of the labels, and I\'m using a figure ratio that makes the colorbar and the main figure come out the same height.\n\nEDIT 2:\nAs the df.corr() method ignores non-numerical columns, .select_dtypes([\'number\']) should be used when defining the x and y labels to avoid an unwanted shift of the labels (included in the code below).\nf = plt.figure(figsize=(19, 15))\nplt.matshow(df.corr(), fignum=f.number)\nplt.xticks(range(df.select_dtypes([\'number\']).shape[1]), df.select_dtypes([\'number\']).columns, fontsize=14, rotation=45)\nplt.yticks(range(df.select_dtypes([\'number\']).shape[1]), df.select_dtypes([\'number\']).columns, fontsize=14)\ncb = plt.colorbar()\ncb.ax.tick_params(labelsize=14)\nplt.title(\'Correlation Matrix\', fontsize=16);\n\n\n', 'Surprised to see no one mentioned more capable, interactive and easier to use alternatives.\nA) You can use plotly:\n\nJust two lines and you get:\n\ninteractivity,\n\nsmooth scale,\n\ncolors based on whole dataframe instead of individual columns,\n\ncolumn names & row indices on axes,\n\nzooming in,\n\npanning,\n\nbuilt-in one-click ability to save it as a PNG format,\n\nauto-scaling,\n\ncomparison on hovering,\n\nbubbles showing values so heatmap still looks good and you can see\nvalues wherever you want:\n\n\nimport plotly.express as px\nfig = px.imshow(df.corr())\nfig.show()\n\n\nB) You can also use Bokeh:\nAll the same functionality with a tad much hassle. But still worth it if you do not want to opt-in for plotly and still want all these things:\nfrom bokeh.plotting import figure, show, output_notebook\nfrom bokeh.models import ColumnDataSource, LinearColorMapper\nfrom bokeh.transform import transform\noutput_notebook()\ncolors = [\'#d7191c\', \'#fdae61\', \'#ffffbf\', \'#a6d96a\', \'#1a9641\']\nTOOLS = ""hover,save,pan,box_zoom,reset,wheel_zoom""\ndata = df.corr().stack().rename(""value"").reset_index()\np = figure(x_range=list(df.columns), y_range=list(df.index), tools=TOOLS, toolbar_location=\'below\',\n           tooltips=[(\'Row, Column\', \'@level_0 x @level_1\'), (\'value\', \'@value\')], height = 500, width = 500)\n\np.rect(x=""level_1"", y=""level_0"", width=1, height=1,\n       source=data,\n       fill_color={\'field\': \'value\', \'transform\': LinearColorMapper(palette=colors, low=data.value.min(), high=data.value.max())},\n       line_color=None)\ncolor_bar = ColorBar(color_mapper=LinearColorMapper(palette=colors, low=data.value.min(), high=data.value.max()), major_label_text_font_size=""7px"",\n                     ticker=BasicTicker(desired_num_ticks=len(colors)),\n                     formatter=PrintfTickFormatter(format=""%f""),\n                     label_standoff=6, border_line_color=None, location=(0, 0))\np.add_layout(color_bar, \'right\')\n\nshow(p)\n\n\n', 'Form correlation matrix, in my case zdf is the dataframe which i need perform correlation matrix.\n\ncorrMatrix =zdf.corr()\ncorrMatrix.to_csv(\'sm_zscaled_correlation_matrix.csv\');\nhtml = corrMatrix.style.background_gradient(cmap=\'RdBu\').set_precision(2).render()\n\n# Writing the output to a html file.\nwith open(\'test.html\', \'w\') as f:\n   print(\'<!DOCTYPE html><html lang=""en""><head><meta charset=""UTF-8""><meta name=""viewport"" content=""width=device-widthinitial-scale=1.0""><title>Document</title></head><style>table{word-break: break-all;}</style><body>\' + html+\'</body></html>\', file=f)\n\n\nThen we can take screenshot. or convert html to an image file.\n', 'Along with other methods it is also good to have pairplot which will give scatter plot for all the cases-\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nrs = np.random.RandomState(0)\ndf = pd.DataFrame(rs.rand(10, 10))\nsns.pairplot(df)\n\n', 'For completeness, the simplest solution i know with seaborn as of late 2019, if one is using Jupyter:\n\nimport seaborn as sns\nsns.heatmap(dataframe.corr())\n\n', 'statmodels graphics also gives a nice view of correlation matrix\n\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\ncorr = dataframe.corr()\nsm.graphics.plot_corr(corr, xnames=list(corr.columns))\nplt.show()\n\n', 'If you dataframe is df you can simply use:\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(15, 10))\nsns.heatmap(df.corr(), annot=True)\n\n', ""You can use imshow() method from matplotlib\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\nplt.imshow(X.corr(), cmap=plt.cm.Reds, interpolation='nearest')\nplt.colorbar()\ntick_marks = [i for i in range(len(X.columns))]\nplt.xticks(tick_marks, X.columns, rotation='vertical')\nplt.yticks(tick_marks, X.columns)\nplt.show()\n\n"", ""Seaborn's heatmap version:\n\nimport seaborn as sns\ncorr = dataframe.corr()\nsns.heatmap(corr, \n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values)\n\n"""
Python NetworkX two dimensional list of all neighbor neighbors,"""Solving my own question after further research.  The below code does it.  Welcome suggestions on a more efficient way to do this.\ndfNodes = pd.DataFrame()\ni=0\nfor node in G.nodes():\n    if (i==0):\n        dfNode = pd.DataFrame(nx.descendants(G,node))\n        dfNode['first']=node\n        dfNode.columns.values[0] = ""next""\n        dfNodes =dfNode\n    else:\n        dfNode = pd.DataFrame(nx.descendants(G,node))\n        dfNode['first']=node\n        dfNode.columns.values[0] = ""next""\n        dfNodes = pd.concat([dfNodes,dfNode])\n    new_row = {'first': node, 'next': node}\n    dfNodes.loc[len(dfNodes.index)] = new_row\n    i=i+1\ndfNodes.sort_values(['first','next'])\n\n"""
How to replace all occurrences in a column with a given value based on a condition applied to a different column,"'This question shares some similarities with another I found, so I\'d definitely recommend searching the archives.\nOrder-Agnostic Approach\nI say order-agnostic because the other answer is ideal if you\'re looking to just use the \'first\' value.\nIt seems like you\'re going to need a dict/hashmap that serves as your source of truth for re-defining the Code based on the Name. Given what you wrote in the question it would look something like this:\nname_to_code_mapping = {""Ibuprofen"": ""ADVL"", ""Sildenafil"": ""..."", ...}\n\nYou can then use the dict inside of a function that you can pass to something like df.apply:\ndef change_code_based_on_drug_name(df_row: pd.Series, column_to_find: str, column_to_change: str, map: dict):\n    val_to_find = df_row[column_to_find]\n    ## You\'ll have to split the numbers off of the name here\n    trimmed_df_value = val_to_find.split("" "")[0]\n    \n    if trimmed_df_value in map:\n        df_row[column_to_change] = map[trimmed_df_value]\n    return df_row\n\nAnd feed it to that df.apply:\ndf = df.apply(lambda row: change_code_based_on_drug_name(row, ""Name"", ""Code"", name_to_code_mapping), axis=1)\n\nAlternative, Less Ideal Approach\nYou could also iterate over the map and use .loc[] (like in the above S/O post) to change the values but you would need to create a column of the name minus numbers:\n## Notice that since I\'m using apply over a single column (i.e. Series) here I don\'t need to axis paramter\ndf[""Name_without_numbers""] = df[""Name""].apply(lambda name: name.split("" "")[0])\n\nYou\'d then set that to the index so you can use the .loc method.\ndf.set_index(""Name_without_numbers"", inplace=True)\n\nfor key, val in name_to_code_mapping.items():\n    df.loc[key, ""Code""] = val\n\nI don\'t like or recommend that second approach because:\n\nIt adds an unnecessary column, and\nCreates a non-unique index\n\nHope that helps!\n', 'If I understand you correctly, you want to set the Code to the first observed Code value for each medicine:\ndf[\'Code\'] = df.groupby(df[\'Name\'].str.split().str[0])[\'Code\'].transform(\'first\')\nprint(df)\n\nPrints:\n               Name   Code  Amount     Seller\n0   Sildenafil 1045   VGRA      32   Rite Aid\n1     Ibuprofen 378   ADVL     209        CVS\n2       Paracetamol  PCTML      87   Keystone\n3      Aspirin 9852   DISP     372   Rite Aid\n4    Ibuprofen 1992   ADVL      87  Walgreens\n5          Benadryl   BDRL     120        CVS\n6      Aspirin 0541   DISP     197     H Mart\n7    Sildenafil 002   VGRA      12   Omnicare\n8     Ibuprofen 378   ADVL     301   Keystone\n9       Paracetamol  PCTML     673  Walgreens\n10   Ibuprofen 1992   ADVL      87   Omnicare\n11  Sildenafil 1045   VGRA      45     H Mart\n12         Benadryl   BDRL     111   Keystone\n13     Aspirin 9852   DISP     285        CVS\n14   Sildenafil 002   VGRA      79   Rite Aid\n15     Aspirin 0541   DISP     431   Omnicare\n\n'"
How to iterate with a for loop through a dictionary and save each output in a DataFrame,"""You can create an empty list before the loop and append the results of each iteration to that list. Then you can convert the list of tuples to a DataFrame using pd.DataFrame().\nimport pandas as pd\n\nn = 5\nresults = []\n\nfor i in range(1, n + 1):\n    result = 10 * 10 * i\n\n    # Append the result to the list\n    results.append(result)\n\n\ndf = pd.DataFrame(results, columns=['Result'])\n\n\nprint(df)\n\n"", ""It seems like there might be a misunderstanding about dataframes and loops. In your code snippet, df is being overwritten in every iteration of the x loop with a new, empty DataFrame, and the p loop does not actually modify df because pandas DataFrames are not mutable (changeable) in the way you're trying to do.\nHowever, based on your description, it sounds like you want to create a new DataFrame with a single column where each row contains the value of 10 * 10 * n for n in range(1, 6). Here is a way you could do that:\nimport pandas as pd\n\n# Define n\nn = 5\n\n# create an empty list to store results\nresults = []\n\n# Loop over the range\nfor i in range(1, n+1): #start at 1 and end at n. Range ends before the specified end value\n    # Compute the result\n    result = 10 * 10 * i\n    # Append to the list of results\n    results.append(result)\n\n# Create a Dataframe from the results\ndf = pd.DataFrame(results, columns=['Result'])\n\n# Print the Dataframe\nprint(df)\n\nThis should output:\n   Result\n0     100\n1     200\n2     300\n3     400\n4     500\n\n"""
I am having issues using Regex to parse a chat and turn it into a dataframe. - it is just skipping info,"'Try (regex101):\nimport re\nimport pandas as pd\n\n\nchat_str2 = """"""\n12thJ. JacobsLV - RB\n\n13thS. DiggsBuf - WR\n\n14thD. AdamsLV - WR\n\n19thA. St. BrownDet - WR\n\n97thJ. Smith-NjigbaSea - WR\n\n120thMiamiMia - DEF""""""\n\npat = re.compile(r""^(\\d+).*?(?:([A-Z])\\.\\s*)?([A-Z].*)([A-Z]\\S+) - (.*)"", flags=re.M)\n\ndf = pd.DataFrame(\n    pat.findall(chat_str2),\n    columns=[""draft_num"", ""first_initial"", ""last_name"", ""team"", ""pos""],\n)\nprint(df)\n\nPrints:\n  draft_num first_initial     last_name team  pos\n0        12             J        Jacobs   LV   RB\n1        13             S         Diggs  Buf   WR\n2        14             D         Adams   LV   WR\n3        19             A     St. Brown  Det   WR\n4        97             J  Smith-Njigba  Sea   WR\n5       120                       Miami  Mia  DEF\n\n'"
Handling MemoryError when Training a Model on Large Dataset,"'\n1.5 billion cells would take 12GB if each cell is a float64 (8 bytes).\n\nPython, if it is like many other languages, has a lot of overhead for each cell.\n\nC and C++ would allocate only 8 bytes.\n\nSome other languages allocate about 40 bytes.\n\n\n'"
How to convert Python Pandas function to Python PySpark without using UDF function,"'You can use Spark\'s window functions like this:\nfrom pyspark.sql import Window\nfrom pyspark.sql.functions import col, lag, sum as spark_sum, when\n\nwindow_spec = Window.partitionBy(\'Service\', \'Phone Number\').orderBy(\'date\')\n\ndf = df.withColumn(\'last_ref\', lag(col(\'date\')).over(window_spec))\ndf = df.withColumn(\'n\', when(col(\'date\') > (col(\'last_ref\') + expr(""INTERVAL 3 DAYS"")), 1).otherwise(0))\ndf = df.withColumn(\'seq\', spark_sum(\'n\').over(window_spec))\ndf = df.drop(\'last_ref\', \'n\')\n\n'"
IndexError: At least one sheet must be visible,"'The error message also acts as a catch-all error. With this in mind, check to see that there are no issues with any of the dataframes you are trying to write into a sheet. I noticed my dataframe itself returned an error. Once I debugged my dataframe, my excel file was able to be written.\n', ""You should be able to do wb.save(""filename.xlsx"") rather than use excelwriter at all.\nwriter.save is an excelwriter method while openpyxl has its own save method that takes a filename after you opened a workbook.\nYou can also use the wb.sheetnames and wb.create_sheet to navigate sheets.\nfrom openpyxl import Workbook, load_workbook\nfrom openpyxl.utils.dataframe import dataframe_to_rows\n\nwb = load_workbook(PATH + ""yourfile.xlsx"")\n#writer = pd.ExcelWriter(PATH + ""yourfile.xlsx"", engine='openpyxl')\n\nwb2 = []\n\nif ""MySheetName"" in wb.sheetnames:\n     wb2 = wb[""MySheetName""]\nelse:\n     wb2 = wb.create_sheet(""MySheetName"",-1)  #Auto add at the end\n\n#wb2 is now the active sheet for wb\n\nfor r in dataframe_to_rows(df, index=True, header=True):\n     wb2.append(r)\n\nwb.save(PATH + ""yourfile.xlsx"")\n\nHope it helps\n"", ""Same error, IndexError: At least one sheet must be visible, consistently reproduced when a writer is created, then saved, without having written anything. Example:\nwriter = pd.ExcelWriter(file_name, engine='openpyxl')\nwriter.save()\n\nStepping through the code, the workbook doesn't contain any sheets at time of writing, visible or otherwise, despite having sheets before calling save.\nGranted, hard to match that to your code, but a clue to you or next dev coming across same error.\n"", 'In Pandas 1.3.5 I needed to assign the engine as xlsxwriter.\nwith pd.ExcelWriter(\'path-to-.xlsx-file\',engine=\'xlsxwriter\') as writer:     \n    df.to_excel(writer, \'spreadsheet-name\')\n\nhttps://pandas.pydata.org/docs/dev/reference/api/pandas.ExcelWriter.html\n', 'working perfectly fine on pandas 1.1.5 and openpyxl 3.0.5.\nChange your pandas and openpyxl library on above mentioned version.\n', 'Alternatively, if you don\'t need to load a workbook, you can merely use xlsxwriter instead of openpyxl; it hasn\'t this problem. You can also create a workbook with the regular \n\nfrom openpyxl import Workbook\n#...\nwb= Workbook()\nws=wb.active\nwith pd.ExcelWriter(output_filepath, engine=""openpyxl"") as writer:\n    writer.book=wb\n    writer.sheets = dict((ws.title, ws) for ws in wb.worksheets)\n    #useful code\n    df.to_excel(writer, sheet, ...)\n    writer.save()\n\n\nnow you can work with both pandas.to_excel and the interesting methods of openpyxl that are not translated into it (that\'s not my hack; found it here but can\'t find where)\n', ""It seems like what you want to do is just write each DataFrame to the same sheet (appending it below the last), so I think you can write this as:\n\nstart_row = 1\nfor df in frames:  # assuming they're already DataFrames\n    df.to_excel(writer, sheet, startrow=start_row, index=False)\n    start_row += len(df) + 1  # add a row for the column header?\nwriter.save()  # we only need to save to disk at the very end!\n\n"""
Dataframe &gt; column &gt; incorrect correlative ID,"'What come up on my mind is there anyway that you can add a field with the same ids and when you sort that will sort id values if you have no access to sort them, I wish I can add a visual code to show you but hope the idea is there needs just testing.\n', ""If you are only trying to rename the Ids given by the excel sheet (without sorting), then I think this is what you're trying to do\ndf = df.reset_index()\ndf = df.rename(columns={""index"":""books_id""})\ndf['books_id'] = df.index + 1\n\n"""
Most efficient way to combine information from each row of groups in a pandas groupby,"'import numpy as np\nimport pandas as pd\n\n# create a reproducible dataframe\n_rng = np.random.default_rng(123)\nx = [""a"", ""a"", ""b"", ""b"", ""a""] * 10_000_000\n_rng.shuffle(x)\ny = [""x"", ""y"", ""z"", ""w"", ""t""] * 10_000_000\n_rng.shuffle(y)\ndf = pd.DataFrame({""A"" : x, ""B"" : y})\n\ngroups = df.groupby(""A"") \n\ndef strings2_old(dfg):\n    return dfg[\'B\'].apply(lambda x : str(len(np.unique(x))) + ""hello"")\n\ndef strings2_new(dfg):\n    return dfg[\'B\'].apply(lambda x : f""{len(pd.unique(x))}hello"")\n\n\n%timeit strings2_old(groups)\n>>> 20.7 s ± 82.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n%timeit strings2_new(groups)\n>>> 1.9 s ± 10.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\nEvery other implementation of your first method I tried including what Andrej suggested is worse than yours. Sorting is prohibitively expensive regardless of the scale unless it\'s a sorting network.\n\nUPD\nReading Sebastian\'s amazing answer got me down the rabbit hole of optimisations. Can we actually do better? Apparently, we can: Python lists: why is .sort() much faster than sorted()?\nimport pandas as pd\nimport numpy as np\n\n_rng = np.random.default_rng(123)\n\ndef setup(N):\n    x = [""a"", ""a"", ""b"", ""b"", ""a""] * N\n    _rng.shuffle(x)\n    y = [""x"", ""y"", ""z"", ""w"", ""t""] * N\n    _rng.shuffle(y)\n    df = pd.DataFrame({""A"": x, ""B"": y})\n    return [df]\n\ndef pandas_str_accessor_approach(df):\n    return df.groupby(""A"")[""B""].apply(sorted).str.join("" "")\n\ndef pandas_str_accessor_approach2(df):\n    def transform(_x):\n        _x = _x.tolist()\n        _x.sort()\n        return _x\n    return df.groupby(""A"")[""B""].apply(lambda x: transform(x)).str.join(\' \')\n\napproaches = [\n    pandas_str_accessor_approach,\n    pandas_str_accessor_approach2\n]\n\nrun_performance_comparison(\n    approaches,\n    [\n        1_000,\n        3_000,\n        5_000,\n        10_000,\n        30_000,\n        100_000,\n        300_000,\n        500_000,\n        1_000_000,\n        3_000_000,\n        5_000_000,\n        10_000_000],\n    setup=setup,\n    title=""Performance Comparison"",\n    number_of_repetitions=5,\n)\n\n\n\n', 'The fastest solution for strings2 I found was:\ndef cast_multiple_apply_approach(dfg):\n    return dfg[""B""].apply(pd.unique).map(len).map(repr) + ""hello""\n\nFor strings2 the OP\'s method is quite slow:\nimport numpy as np\nimport pandas as pd\n\n_rng = np.random.default_rng(123)\n\ndef setup(N):\n    x = [""a"", ""a"", ""b"", ""b"", ""a""] * N\n    _rng.shuffle(x)\n    y = [""x"", ""y"", ""z"", ""w"", ""t""] * N\n    _rng.shuffle(y)\n    df = pd.DataFrame({""A"": x, ""B"": y})\n\n    groups = df.groupby(""A"")\n    return [groups]\n\n\ndef original_poster_approach(dfg):\n    return dfg[""B""].apply(lambda x: str(len(np.unique(x))) + ""hello"")\n\n\ndef bracula_approach(dfg):\n    return dfg[""B""].apply(lambda x: f""{len(pd.unique(x))}hello"")\n\n\ndef len_multiple_apply_approach(dfg):\n    return dfg[""B""].apply(pd.unique).map(len).astype(""string"") + ""hello""\n\n\ndef npsize_multiple_apply_approach(dfg):\n    return dfg[""B""].apply(pd.unique).map(np.size).astype(""string"") + ""hello""\n\n\ndef set_multiple_apply_approach(dfg):\n    return dfg[""B""].apply(set).map(len).astype(""string"") + ""hello""\n\n\ndef cast_multiple_apply_approach(dfg):\n    return dfg[""B""].apply(pd.unique).map(len).map(repr) + ""hello""\n\n\napproaches = [\n    original_poster_approach,\n    bracula_approach,\n    len_multiple_apply_approach,\n    npsize_multiple_apply_approach,\n    set_multiple_apply_approach,\n    cast_multiple_apply_approach,\n]\nfor approach in approaches[1:]:\n    data = setup(100)\n    assert (approach(*data) == approaches[0](*data)).all()\n\n\nrun_performance_comparison(\n    approaches,\n    [\n        1000,\n        3000,\n        5000,\n        10000,\n        30000,\n        100_000,\n    ],  # ,300_000,500_000,1_000_000],#3_000_000,5_000_000,10_000_000],\n    setup=setup,\n    title=""Performance Comparison"",\n    number_of_repetitions=1,\n)\n\n\nTherefore I excluded it from running on a large sample:\n\nFor strings, I was surprised to find that:\n\nOP\'s method is actually really hard to beat\nSorting beforehand is terrible\n\nI could improve a little on OP\'s solution by using native pd.Series.str.join over the joining in pure Python:\ndef pandas_str_accessor_approach(df):\n    return df.groupby(""A"")[""B""].apply(sorted).str.join("" "")\n\ndef setup(N):\n    x = [""a"", ""a"", ""b"", ""b"", ""a""] * N\n    _rng.shuffle(x)\n    y = [""x"", ""y"", ""z"", ""w"", ""t""] * N\n    _rng.shuffle(y)\n    df = pd.DataFrame({""A"": x, ""B"": y})\n    return [df]\n\n\ndef original_poster_approach(df):\n    return df.groupby(""A"")[""B""].apply(lambda x: """".join(sorted(x.values + "" "")))\n\n\ndef pandas_str_accessor_approach(df):\n    return df.groupby(""A"")[""B""].apply(sorted).str.join("" "")\n\n\ndef sort_first_approach(df):\n    df.sort_values(by=[""B""], inplace=True)\n    strings = df.groupby(""A"")[""B""].apply("" "".join)\n    return strings\n\n\napproaches = [\n    original_poster_approach,\n    pandas_str_accessor_approach,\n    sort_first_approach,\n]\n\nrun_performance_comparison(\n    approaches,\n    [\n        1000,\n        3000,\n        5000,\n        10000,\n        30000,\n        100_000,\n        300_000,\n        500_000,\n        1_000_000,\n        3_000_000,\n    ],  # 5_000_000,10_000_000],\n    setup=setup,\n    title=""Performance Comparison"",\n    number_of_repetitions=1,\n)\n\n\n\nProfiling code used:\nimport timeit\nimport matplotlib.pyplot as plt\nfrom typing import List, Dict, Callable\n\nfrom contextlib import contextmanager\n\n\n@contextmanager\ndef data_provider(data_size, setup=lambda N: N, teardown=lambda: None):\n    data = setup(data_size)\n    yield data\n    teardown()\n\n\ndef run_performance_comparison(approaches: List[Callable],\n                               data_size: List[int],\n                               setup=lambda N: N,\n                               teardown=lambda: None,\n                               number_of_repetitions=5, title=\'Performance Comparison\',data_name=\'N\'):\n    approach_times: Dict[Callable, List[float]] = {approach: [] for approach in approaches}\n\n    for N in data_size:\n        with data_provider(N, setup, teardown) as data:\n            for approach in approaches:\n                approach_time = timeit.timeit(lambda: approach(*data), number=number_of_repetitions)\n                approach_times[approach].append(approach_time)\n\n    for approach in approaches:\n        plt.plot(data_size, approach_times[approach], label=approach.__name__)\n    plt.yscale(\'log\')\n    plt.xscale(\'log\')\n\n    plt.xlabel(data_name)\n    plt.ylabel(\'Execution Time (seconds)\')\n    plt.title(title)\n    plt.legend()\n    plt.show()\n\n\n', 'IIUC, you can sort the dataframe by column B before grouping and then just apply str.join:\ndf = df.sort_values(by=[\'B\'])\nstrings = df.groupby(""A"")[\'B\'].apply(\' \'.join)\n\nprint(strings)\n\nPrints:\nA\na    t x y\nb      w z\nName: B, dtype: object\n\n'"
Pandas: Setting no. of max rows,"""I would use a context manager to set these options so that I can control which df should be affected.\nwith pd.option_context('display.min_rows', 50, 'display.max_columns', None):\n    display(df)\n\nAlso, instead of display.max_rows use display.min_rows instead. This should work without setting display.max_rows.\n"", 'pd.options.display.max_rows = None\nThis would display all the rows\n', ""I don't know why nobody mentioned this.\nYou should also set 'display.min_rows'.\npd.set_option('display.min_rows', 500)  # <-add this!\npd.set_option('display.max_rows', 500)\n\n\nIf total number of rows > display.max_rows,\nthen, setting only display.max_rows would not work.\n(Yeah, it's confusing. This should be modified.)\n"", ""to set unlimited number of rows use\n\nNone\n\ni.e.,\npd.set_option('display.max_columns', None)\n\nnow the notebook will display all the rows in all datasets within the notebook ;)\nSimilarly you can set to show all columns as\npd.set_option('display.max_rows', None)\n\nnow if you use run the cell with only dataframe with out any head or tail tags\nas\ndf\n\nthen it will show all the rows and columns in the dataframe df\n"", 'pd.set_option(\'display.max_rows\', 500)\ndf\n\n\nDoes not work in Jupyter!\nInstead use:\n\npd.set_option(\'display.max_rows\', 500)\ndf.head(500)\n\n', 'Set display.max_rows:\n\npd.set_option(\'display.max_rows\', 500)\n\n\nFor older versions of pandas (<=0.11.0) you need to change both display.height and display.max_rows.\n\npd.set_option(\'display.height\', 500)\npd.set_option(\'display.max_rows\', 500)\n\n\nSee also pd.describe_option(\'display\').\n\nYou can set an option only temporarily for this one time like this:\n\nfrom IPython.display import display\nwith pd.option_context(\'display.max_rows\', 100, \'display.max_columns\', 10):\n    display(df) #need display to show the dataframe when using with in jupyter\n    #some pandas stuff\n\n\nYou can also reset an option back to its default value like this:\n\npd.reset_option(\'display.max_rows\')\n\nAnd reset all of them back:\n\npd.reset_option(\'all\')\n', 'It was already pointed in this comment and in this answer, but I\'ll try to give a more direct answer to the question:\n\nfrom IPython.display import display\nimport numpy as np\nimport pandas as pd\n\nn = 100\nfoo = pd.DataFrame(index=range(n))\nfoo[\'floats\'] = np.random.randn(n)\n\nwith pd.option_context(""display.max_rows"", foo.shape[0]):\n    display(foo)\n\n\npandas.option_context is available since pandas 0.13.1 (pandas 0.13.1 release notes).\nAccording to this,\n\n\n  [it] allow[s] you to execute a codeblock with a set of options that revert to prior settings when you exit the with block.\n\n', 'Personally, I like setting the options directly with an assignment statement as it is easy to find via tab completion thanks to iPython. I find it hard to remember what the exact option names are, so this method works for me.\n\nFor instance, all I have to remember is that it begins with pd.options\n\npd.options.<TAB>\n\n\n\n\nMost of the options are available under display\n\npd.options.display.<TAB>\n\n\n\n\nFrom here, I usually output what the current value is like this:\n\npd.options.display.max_rows\n60\n\n\nI then set it to what I want it to be:\n\npd.options.display.max_rows = 100\n\n\nAlso, you should be aware of the context manager for options, which temporarily sets the options inside of a block of code. Pass in the option name as a string followed by the value you want it to be. You may pass in any number of options in the same line:\n\nwith pd.option_context(\'display.max_rows\', 100, \'display.max_columns\', 10):\n    some pandas stuff\n\n\nYou can also reset an option back to its default value like this:\n\npd.reset_option(\'display.max_rows\')\n\n\nAnd reset all of them back:\n\npd.reset_option(\'all\')\n\n\nIt is still perfectly good to set options via pd.set_option. I just find using the attributes directly is easier and there is less need for get_option and set_option.\n', 'As @hanleyhansen noted in a comment, as of version 0.18.1, the display.height option is deprecated, and says ""use display.max_rows instead"". So you just have to configure it like this:\n\npd.set_option(\'display.max_rows\', 500)\n\n\nSee the Release Notes — pandas 0.18.1 documentation:\n\n\n  Deprecated display.height, display.width is now only a formatting option does not control triggering of summary, similar to < 0.11.0.\n\n', 'As in this answer to a similar question, there is no need to hack settings. It is much simpler to write:\n\nprint(foo.to_string())\n\n'"
How to compare two elements of a series with different indexes,"'Something like this is what you mean?\nlow = [1,2,3,4]\nhigh = [5,6,7,8]\n\nfor i in range(1, len(low)):\n    if low[i] > high[i-1]:\n        # whatever you need to do\n        pass\n    else:\n        # whatever you need to do\n        pass\n\n'"
sort the dataframe based on the time and keep the ids,"""Covert to datetime and then sort_values\ndf['date'] = pd.to_datetime(df['date'])\ndf.sort_values(['id', 'date'])\n\n   id  val                date\n1   1   15 2023-02-23 22:00:00\n0   1   10 2023-02-23 22:15:00\n2   1   12 2023-02-24 22:15:00\n4   2   -1 2023-02-23 21:15:00\n3   2   13 2023-02-23 22:15:00\n\n"""
Perform Excel MAXIFS in Pandas with multiple conditions,"'You can try:\n# https://stackoverflow.com/a/74359384/10035985\nindexer = pd.api.indexers.FixedForwardWindowIndexer(window_size=3)\n\n\ndef fn(x):\n    return (\n        x.set_index(""date"")\n        .asfreq(""1D"")\n        .rolling(indexer, min_periods=1)[""value""]\n        .max()\n        .shift(-1)\n    )\n\n\nout = pd.merge(\n    df, df.groupby(""id"").apply(fn), left_on=[""id"", ""date""], right_index=True\n).rename(columns={""value_x"": ""value"", ""value_y"": ""next_2d_max""})\nprint(out)\n\nPrints:\n   id       date  value  next_2d_max\n0   a 2023-01-01      3         10.0\n1   a 2023-01-02     10          NaN\n2   b 2023-01-03      2         24.0\n3   b 2023-01-04     20         24.0\n4   b 2023-01-05     24          9.0\n5   b 2023-01-06      9          7.0\n6   a 2023-01-07     21         25.0\n7   b 2023-01-08      7         12.0\n8   a 2023-01-09     25          NaN\n9   b 2023-01-10     12          7.0\n10  b 2023-01-11      7          NaN\n\n'"
Double measurement frequency,"'Here is a way using np.linspace()\nm = df.index.max()\n\ndf = df.reindex(np.linspace(0,m,(m*2)+1)).interpolate().reset_index(drop=True)\n\nOutput:\n   time  data\n0   1.0  10.0\n1   1.5  15.0\n2   2.0  20.0\n3   2.5  25.0\n4   3.0  30.0\n\n', 'You can try to .reindex + .interpolate:\ndf = pd.DataFrame({\'time\':[1,2.1,3], \'data\':[10,20,30]})\n\ndf.index = df.index * 2\ndf = df.reindex(index=range(0, df.index.max()+1)).interpolate()\n\nprint(df)\n\nPrints:\n   time  data\n0  1.00  10.0\n1  1.55  15.0\n2  2.10  20.0\n3  2.55  25.0\n4  3.00  30.0\n\n'"
how to extract the data from page in Json form?,"'BeautifulSoup allows you to traverse the rendered html of the page as a tree, which is what soup.select_one(""#chartEncours8a input"")[""value""].\nAs the author of your previous answer stated, inspect element does not give you the info you are looking for. Examining the raw html (which BeautifulSoup) is reading can show you a link.\n'"
pandas.errors.parsererror error tokenizing data. c error expected 2 fields?,"""usually, when this error occurs, the problem is the delimiter, try checking out the file that you are opening with pd.read_csv() and make sure they are separated by one space or tabs or commas and so on, then change the delimiter=' ' to delimiter='\\t' or ',', or spaces, etc accordingly if needed.\n"""
How to group data from a multiindex column dataframe for split violin- or boxplots,"'I have come up with something similar to already posted answer, but it seems a bit more compact. I use pandas melt:\nsns.violinplot(data=pd.melt(df, var_name=[""type"", ""subtype""]), \n               x=""value"", y=""subtype"", hue=""type"", split=True,\n               orient=""h"")\n\nOutput:\nThis is based on a mock dataset created from your input example - I used\nrest V1, V2, V3 as is and task VMA1, VMA2, VMA3 as task V1, V2, V3, hope that makes sense.\n', ""So funnily enough I had to do this yesterday. I admit my solution is not the cleanest but it works.\nFirst you need to change your data frame so that it has the structure that seaborn uses (vals in one column, category in another). To do that you do:\n## Concatenates different variables\ndf=pd.concat([data[[""rest""]], data[[""task""]]]).reset_index(drop=True)\n## Creates one column of values\ndf[""value""]=df[""rest""].fillna(df[""task""])\n## Created second column with category name for the corresponding value\ndf[""rest""]=(df[""rest""]/df[""rest""]).replace(1, ""rest"")\ndf[""task""]=(df[""task""]/df[""task""]).replace(1, ""task"")\ndf[""variable""]=df[""rest""].fillna(df[""task""])\n\nNow with that out of the way its time to plot\nsns.violinplot(data=df, x=var_in_xaxis, y=""value"", hue=""variable"", split=True, ax=ax, inner=None)\n\nIt wasn't clear to me what variable you are using for the x axis so I left it for you to fill out.\nEdit: I guess since your data is a multicolumn dataframe, then you'd need to use groupby to apply this to each one of the subcolumns and then aggregate it and/or plot it.\n"""
Perform Excel MAXIFS in Pandas,"""Try this:\n(df.assign(\n    next_local_max = df.iloc[::-1].groupby('id')['value'].transform(lambda x: x.cummax().shift()))\n    )\n\nOutput:\n   id       date  value  next_local_max\n0   a 2023-01-01      3            25.0\n1   a 2023-01-02     10            25.0\n2   b 2023-01-03      2            24.0\n3   b 2023-01-04     20            24.0\n4   b 2023-01-05     24            12.0\n5   b 2023-01-06      9            12.0\n6   a 2023-01-07     21            25.0\n7   b 2023-01-08      7            12.0\n8   a 2023-01-09     25             NaN\n9   b 2023-01-10     12             7.0\n10  b 2023-01-11      7             NaN\n\n"", 'You can use pandas cummax() to calculate the cumulative maximum of a series.\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\n\ndf = pd.DataFrame({\n    ""id"": [""a""] * 2 + [""b""] * 4 + [""a"", ""b""] * 2 + [""b""],\n    ""date"": pd.date_range(datetime(2023, 1, 1), periods=11).tolist(),\n    ""value"": [3, 10, 2, 20, 24, 9, 21, 7, 25, 12, 7]})\n\ndef get_next_max_local(group):\n    """""" Calculate the max for the given group and add a new ""next_local_max"" column, \n    containing the max local value from the remaining rows in the same group.\n    __Details\n        - group[\'next_local_max\'] => Assigns the final result to \'next_local_max\' column\n        - group[\'value\'] => Get the \'value\' column of the group df.\n        - iloc[::-1] => Reverse the order of the rows, \n            since cummax() need to operate from the end of the group to the beginning.\n        - cummax() => Calculate the cumulative maximum of the (reversed) \'value\' column, \n            to obtain the largest value seen so far, from the beginning to the current row.\n        - shift() => Shift the the cumulative maximum one row forward, to change the max value   \n            for each row, that becames the max value from the next row onwards.\n    """"""\n    group[\'next_local_max\'] = group[\'value\'].iloc[::-1].cummax().shift()\n    \n    return group\n\n\n# Apply the \'get_next_max_local\' function to each group selected by \'id\'...\n# group_keys=False option states that the resulting df only contains   \n# the columns that were modified, without \'id\'.\ndf = df.groupby(\'id\', group_keys=False).apply(get_next_max_local)\n\n# Replace last value of each group with NaN\ndf.loc[df.groupby(\'id\').tail(1).index, \'next_local_max\'] = np.nan\n\n', 'You can try to utilize np.tril to compute the local maximum:\ndef fn(x):\n    a = np.tril(x[::-1]).max(axis=1)[::-1]\n    return pd.Series(a, index=x.index).shift(-1)\n\ndf[\'next_local_max\'] = df.groupby(\'id\', group_keys=False)[\'value\'].apply(fn)\nprint(df)\n\nPrints:\n   id       date  value  next_local_max\n0   a 2023-01-01      3            25.0\n1   a 2023-01-02     10            25.0\n2   b 2023-01-03      2            24.0\n3   b 2023-01-04     20            24.0\n4   b 2023-01-05     24            12.0\n5   b 2023-01-06      9            12.0\n6   a 2023-01-07     21            25.0\n7   b 2023-01-08      7            12.0\n8   a 2023-01-09     25             NaN\n9   b 2023-01-10     12             7.0\n10  b 2023-01-11      7             NaN\n\n\nOr: Shorter version with np.triu (so that you skip the array-reversing):\ndef fn(x):\n    return pd.Series(np.triu(x).max(axis=1), index=x.index).shift(-1)\n\ndf[\'next_local_max\'] = df.groupby(\'id\', group_keys=False)[\'value\'].apply(fn)\nprint(df)\n\n'"
Merge two dataframes where the keys on the right dataframe are split across two columns,"""    ## your dataframes\n    df1 = pd.DataFrame.from_dict({""A"" : [""x"", ""y"", ""z"", ""w""], ""B"" : [""i"", ""j"", ""k"", ""l""]})\n    df2 = pd.DataFrame.from_dict({""C"" : [""w"", ""t"", ""s"", ""x""], ""D"" : [""n"", ""y"", ""z"", ""m""], ""E"" : [1, 2, 3, 4]})\n\n## \n    df2_a = df2.loc[df2['C'].isin(df1['A'].unique().tolist()), ['C', 'E']]\n    df2_b = df2.loc[df2['D'].isin(df1['A'].unique().tolist()), ['D', 'E']]\n    df2_b = df2_b.rename(columns={'D': 'C'})\n    df2_refined = pd.concat([df2_a, df2_b])\n    \n    \n    final_df = pd.merge(df1, df2_refined, left_on='A', right_on='C')\n    del final_df['C']\n\n"", 'Sounds like you can perform two separate inner merges and concatinate the results:\npd.concat([\n    df1.merge(df2, left_on=""A"", right_on=""C"")[[""A"", ""B"", ""E""]],\n    df1.merge(df2, left_on=""A"", right_on=""D"")[[""A"", ""B"", ""E""]]\n])\n\nOutput:\n\n   A  B  E\n0  x  i  4\n1  w  l  1\n0  y  j  2\n1  z  k  3\n\n', 'melt and merge, you can handle an arbitrary number of columns:\ndf3 = df1.merge(df2.melt(\'E\', value_name=\'A\')[[\'E\', \'A\']], on=\'A\', how=\'left\')\n\n# or\ndf3 = df1.merge(df2.melt(\'E\', value_name=\'A\').drop(columns=\'variable\'),\n                on=\'A\', how=\'left\')\n\nOutput:\n   A  B  E\n0  x  i  4\n1  y  j  2\n2  z  k  3\n3  w  l  1\n\n', ""Here is a way using merge and stack():\ndf = pd.merge(df1,df2.set_index('E').stack().reset_index(level=0,name = 'A'),how = 'left')\n\nOutput:\n   A  B  E\n0  x  i  4\n1  y  j  2\n2  z  k  3\n3  w  l  1\n\n"", ""You can concatenate the two copies of df2 then merge:\ndf1.merge(pd.concat([df2.drop(columns='D').rename(columns={'C':'A'}),\n                     df2.drop(columns='C').rename(columns={'D':'A'})]),\n          on='A')\n\nOutput:\n   A  B  E\n0  x  i  4\n1  y  j  2\n2  z  k  3\n3  w  l  1\n\n"""
how to web scrape the data from this site?,"'The data is stored in the page in Json form. To extract it to pandas dataframe you can do:\nimport json\nimport requests\nimport pandas as pd\nfrom bs4 import BeautifulSoup\n\nurl = ""https://www.quantalys.com/espace/518""\n\nsoup = BeautifulSoup(requests.get(url).content, ""html.parser"")\ndata = soup.select_one(""#chartEncours8a input"")[""value""]\ndata = json.loads(data)\n\ndf = pd.DataFrame(data[\'dataProvider\'])\ndf[\'unit\'] = data[\'valueAxes\'][0][\'unit\']\nprint(df)\n\nPrints:\n   category  column-1   unit\n0      2015     41.02   Mrd€\n1      2016     44.92   Mrd€\n2      2017     43.44   Mrd€\n3      2018     31.58   Mrd€\n4      2019     25.30   Mrd€\n5      2020     25.26   Mrd€\n6      2021     25.55   Mrd€\n7      2022     19.57   Mrd€\n\n'"
Pandas: add a column to a multiindex column dataframe,"""If we want to add a multi-level column:\nSource DF:\nIn [221]: df\nOut[221]:\nfirst        bar                 baz\nsecond       one       two       one       two\nA      -1.089798  2.053026  0.470218  1.440740\nB       0.488875  0.428836  1.413451 -0.683677\nC      -0.243064 -0.069446 -0.911166  0.478370\n\nOption 1: adding result of division: bar / baz as a new foo column\nIn [222]: df = df.join(\n     ...:     df[['bar']].div(df['baz']).rename(columns={'bar':'foo'}))\n\nIn [223]: df\nOut[223]:\nfirst        bar                 baz                 foo\nsecond       one       two       one       two       one       two\nA      -1.089798  2.053026  0.470218  1.440740 -2.317647  1.424980\nB       0.488875  0.428836  1.413451 -0.683677  0.345873 -0.627250\nC      -0.243064 -0.069446 -0.911166  0.478370  0.266761 -0.145172\n\nOption 2: adding multi-level column with three ""sub-columns"":\nIn [235]: df = df.join(pd.DataFrame(\n     ...:     np.random.rand(3,3),\n     ...:     columns=pd.MultiIndex.from_product([['new'], ['one','two','three']]),\n     ...:     index=df.index))\n\nIn [236]: df\nOut[236]:\nfirst        bar                 baz                 new\nsecond       one       two       one       two       one       two     three\nA      -1.089798  2.053026  0.470218  1.440740  0.274291  0.636257  0.091048\nB       0.488875  0.428836  1.413451 -0.683677  0.668157  0.456931  0.227568\nC      -0.243064 -0.069446 -0.911166  0.478370  0.333824  0.363060  0.949672\n\n"", 'If you want to insert (instead of append at the end of the DF) do this:\ndf.insert(0, (\'bar\', \'three\'), [0, 1, 2])\n\nThe second item has to be hashable, so a list will not work.\n', 'If you want to add multiple columns to a multiindex column dataframe, you can try\n\nAll same value for columns\n\ndf[[(""foo"", ""bar1""), (""foo"", ""bar2"")]] = 2\n\n        bar                 baz            foo\n        one       two       one       two bar1 bar2\n0  0.487880 -0.487661 -1.030176  0.100813    2    2\n1  0.267913  1.918923  0.132791  0.178503    2    2\n2  1.550526 -0.312235 -1.177689 -0.081596    2    2\n\n\nSame value for each column\n\ndf[[(""foo"", ""bar1""), (""foo"", ""bar2"")]] = [2, 3]\n\n        bar                 baz            foo\n        one       two       one       two bar1 bar2\n0  0.487880 -0.487661 -1.030176  0.100813    2    3\n1  0.267913  1.918923  0.132791  0.178503    2    3\n2  1.550526 -0.312235 -1.177689 -0.081596    2    3\n\n\nDifferent value for each cell\n\ndf[[(""foo"", ""bar1""), (""foo"", ""bar2"")]] = [[1,2], [3,4], [5,6]] # shape is (3, 2) where 3 is index length and 2 is new added column length\n\n        bar                 baz            foo\n        one       two       one       two bar1 bar2\n0  0.487880 -0.487661 -1.030176  0.100813    1    2\n1  0.267913  1.918923  0.132791  0.178503    3    4\n2  1.550526 -0.312235 -1.177689 -0.081596    5    6\n\n\nAnother usecase is that we have a single index dataframe, and we want to concat it to the multi index dataframe\n        bar                 baz\n       one       two       one       two     concat to      bar1  bar2\n0  0.487880 -0.487661 -1.030176  0.100813   <---------  0     1     2\n1  0.267913  1.918923  0.132791  0.178503               1     3     4\n2  1.550526 -0.312235 -1.177689 -0.081596               2     5     6\n\n\nGenerate a list of tuples for columns\n\ndf[[(""foo"", col) for col in single_index_df.columns]] = single_index_df\n\n        bar                 baz            foo\n        one       two       one       two bar1 bar2\n0  0.487880 -0.487661 -1.030176  0.100813    1    2\n1  0.267913  1.918923  0.132791  0.178503    3    4\n2  1.550526 -0.312235 -1.177689 -0.081596    5    6\n\n\nCreate a new multi index columns dataframe from the single index dataframe as Option 2 of MaxU - stop genocide of UA\n\ndf = df.join(pd.DataFrame(single_index_df.values,\n                          columns=pd.MultiIndex.from_product([[\'foo\'], single_index_df.columns]),\n                          index=single_index_df.index))\n\n\nCreate a multi index dataframe from single index dataframe with pd.concat({\'foo\': single_index_df}, axis=1)\n\ndf = pd.concat([df, pd.concat({\'foo\': single_index_df}, axis=1)], axis=1)\n\n', ""It's actually pretty simple (FWIW, I originally thought to do it your way):\n\ndf['bar', 'three'] = [0, 1, 2]\ndf = df.sort_index(axis=1)\nprint(df)\n\n        bar                        baz          \n        one       two  three       one       two\nA -0.212901  0.503615      0 -1.660945  0.446778\nB -0.803926 -0.417570      1 -0.336827  0.989343\nC  3.400885 -0.214245      2  0.895745  1.011671\n\n"""
numpy dataframe get maximum difference in a single column,"'Looks like passing a custom max-min function to DataFrameGroupBy.agg is fastest. The alternatives are slower:\n\nusing np.ptp inside agg;\nusing apply (see solution by @MariaKozlova)\n\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed(0) # for reproducibility\n\n# sample df: 10 countries with 5 temperatures\ndata = {\'country_code\': np.repeat(range(10),5), \'temperature\': np.random.randint(-10,50,50)}\ndf = pd.DataFrame(data)\n\n# method1 (pass lambda function to `agg`)\nout = df.groupby(\'country_code\', sort=False)[\'temperature\'].agg(lambda x: max(x) - min(x))\n\n# method2 (pass `np.ptp` to `agg`)\nout2 = df.groupby(\'country_code\', sort=False).agg({\'temperature\': np.ptp})\n\nout.equals(out2[\'temperature\'])\n# True\n\nout\n\ncountry_code\n0    53\n1    56\n2    44\n3    57\n4    23\n5    29\n6    42\n7    47\n8    27\n9    25\nName: temperature, dtype: int64\n\nPerformance comparison\n# intriguingly, `np.ptp` is actually quite a bit slower\n%timeit df.groupby(\'country_code\', sort=False)[\'temperature\'].agg(lambda x: max(x) - min(x))\n# 238 µs ± 4.35 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n\n%timeit df.groupby(\'country_code\', sort=False).agg({\'temperature\': np.ptp})\n# 1.26 ms ± 22 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n\n# adding comparison for `apply` (solution by @MariaKolzova)\ndef temp_range(group):\n    return group.max() - group.min()\n\n%timeit df.groupby(\'country_code\')[\'temperature\'].apply(temp_range)\n# 434 µs ± 9.26 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n\n', ""Here's a slightly different approach, dealing with only one set of groups\ndef temp_range(group):\n    return group.max() - group.min()\n\ndf.groupby('country_code')['temperature'].apply(temp_range)\n\nNot sure if it's better though\n"""
Pandas aggregating and applying a condition on column names,"""Another solution:\nd1 = dict()\nfor language in df['language'].unique():\n    df_filtered = df[df['language'] == language]\n    d2 = dict()\n    for col in ['is_bruiser', 'is_tank', 'is_support']:\n        values = list(df_filtered[df_filtered[col]]['id'])\n        if values:\n            d2[col] = values\n    d1[language] = d2\n\n"", ""You can apply a function to each group. Dictionary with result can be calculated for each group. And them grouped dataframe can be formatted as a final dictionary.\nfrom collections import defaultdict\n\ndef get_dict(group):\n    dct = defaultdict(list)\n    bool_cols = filter(lambda x: x.startswith(""is""), group.columns)\n    \n    for column in bool_cols:\n        for _, row in group.iterrows():\n            if row[column]:\n                dct[column].append(row[""id""])\n    return dict(dct)\n        \naggregated_df = df \\\n    .groupby(""language"") \\\n    .apply(get_dict)\n\ndict(zip(aggregated_df.index, aggregated_df))\n\nOutput:\n{'de': {'is_bruiser': ['121', '1542'], 'is_tank': ['121', '542']},\n 'fr': {'is_bruiser': ['4578', '1216']}}\n\n"", ""you can iterate in your languages and your boolean columns :\nresult = {} for lang in df[""language""].unique():\n    result[lang] = {}\n    for col in [""is_bruiser"", ""is_tank"", ""is_support""]:\n        ids = df.loc[(df[""language""] == lang) & (df[col]), ""id""].tolist()\n        if ids:\n            result[lang][col] = ids\n\nThen you'll get what you want :\nprint(result)\n> {'fr': {'is_bruiser': [4578, 1216]}, 'de': {'is_bruiser': [121, 1542], 'is_tank': [121, 542]}}\n\n"""
How do I insert data into a desired column of a csv file?,"""I believe you could simplify this by creating two series, then merging them into your dataframe and writing them out to CSV.\nSomething like the below should work, but might have missed the essence of what you're trying to accomplish:\n# Load Original Images\n# Dates list\ndates = []\norg_images_path = fr""/wormbot/{lifespan_number}""\n# Establish Prefix for Files\nfile_prefix = ""frame""\n# Sort Images\nimg_list = sorted(os.listdir(org_images_path))\n# Filter Sorted Images by Prefix\nimg_list = [img_name for img_name in img_list if img_name if  img_name.startswith(file_prefix)]\n# Loop through every other file in the folder\nfor index, img_name in enumerate(img_list):\n    if index % 2 ==0:\n        # Open Image\n        image_path = os.path.join(org_images_path, img_name)\n        # Get Date and Time\n        get_image = os.path.getmtime(image_path)\n        # Filter to Just Date Number\n        date_number = datetime.datetime.fromtimestamp(get_image).day\n        # Output Date Number\n        number_day = (date_number)\n        # Append number_day to dates list\n        dates.append(number_day)\n\nThen we would process your counts next:\n# Loop through all files in the folder\nrotifier_counts = []\nfor image in sorted(os.listdir(output_crop)):\n    if image.endswith('.jpg'):\n        image_path = os.path.join(output_crop, image)\n        img = Image.open(image_path)\n        # Run Model with however much confidence on image.\n        results = model(img, conf=.19,verbose=False, max_det=5)\n        counts = {}\n        for result in results:\n            boxes = result.boxes.cpu().numpy()\n            for box in boxes:\n                cls = int(box.cls[0])\n                if not cls in counts.keys():\n                    counts[cls] = 1\n                else:\n                    counts[cls] += 1\n            try:\n                alive = (str(counts[key]))\n                rotifier_counts.append(alive)\n            except KeyError:\n                alive = ('0')\n                rotifier_counts.append(alive)\n        # Adds 1 to current_frame to count.\n        current_frame +=1\n        # Close current image\n        img.close()\n\nFinally, we could create a series from our dates and rotifier_counts list and stand up the dataframe and write to CSV.\n# Create series of dates to create data frame\ns1 = pd.Series(dates, name=""Dates"")\ns2 = pd.Series(rotifier_counts, name=""Rotifer Counts"")\n# Create dataframe with column names\ndf = pd.concat([s1, s2], axis=1)\n# Write df out to CSV\ndf.to_csv(fr""/home/lm/anaconda3/envs/yolov8_rotiferdetect/CSV_files/exp_{lifespan_number}.csv"", index=False, header=True)\n\n"""
Assign new values for columns in data frame from a random row in another data frame,"'Based on your example, if you need to copy col2 and col4 to df1(after shuffled), just do\ndf1.loc[: [""col2"", ""col4""]] = df2[[""col2"", ""col4""]].sample(n=4)\n\nbut make sure their length are same\n', ""I hope this will help you:\nFirst, randomly sample the indices of the df2 rows\nrandom_sampled = random.sample(range(len(df2)), len(df1))\n\nThen, create new columns in df1 and assign values from df2\ndf1[['col2', 'col4']] = df2.iloc[random_sampled][['col2', 'col4']].reset_index(drop=True)\n\nprint(df1) \n\n"""
Pivot a dataframe in pandas while creating additional new columns for values,"'You can try to .set_index() + .stack() + .reset_index(). Then just rename columns:\ndf = df.set_index([\'ID\', \'First Name\', \'Last Name\', \'Date of Birth\', \'Start Date\', \'End Date\']).stack().reset_index(name=\'Status\').rename(columns={\'level_6\': \'Effective Date\'})\nprint(df)\n\nPrints:\n     ID First Name Last Name Date of Birth Start Date   End Date Effective Date Status\n0  MO12      Wanda    Sample      1/1/2020   1/1/2022  1/31/2022       1/1/2022   Good\n1  MO12      Wanda    Sample      1/1/2020   1/1/2022  1/31/2022       2/1/2022   Good\n2  MO12      Wanda    Sample      1/1/2020   1/1/2022  1/31/2022       3/1/2022   Good\n3  MO12      Wanda    Sample      1/1/2020   1/1/2022  1/31/2022       4/1/2022   Good\n\n', ""Try melt:\ndf.melt(['ID', 'First Name', 'Last Name', 'Date of Birth', 'Start Date', 'End Date'], \n        var_name='Effective Date', value_name='Status')\n\nOutput:\n     ID First Name Last Name Date of Birth Start Date   End Date Effective Date Status\n0  MO12      Wanda    Sample      1/1/2020   1/1/2022  1/31/2022       1/1/2022   Good\n1  MO12      Wanda    Sample      1/1/2020   1/1/2022  1/31/2022       2/1/2022   Good\n2  MO12      Wanda    Sample      1/1/2020   1/1/2022  1/31/2022       3/1/2022   Good\n3  MO12      Wanda    Sample      1/1/2020   1/1/2022  1/31/2022       4/1/2022   Good\n\n"""
pandas frequency table with missing values,"""This works but time consuming:\nimport pandas as pd\nimport numpy as np\n\n\ndata = {\n    'key1': [1, 1, 2, np.nan],\n    'key2': [1, 1, 1, 1],\n    'key3': [3, np.nan, 3, np.nan]\n}\ndf = pd.DataFrame(data)\n\nfk_lst = []\nfor index, row in df.iterrows():\n    non_nan_columns = row[row.notna()].index.tolist()\n    df = df[non_nan_columns]\n    for col in df.columns:\n        df[col] = df[col].fillna(row[col])\n    count = df.value_counts(dropna=False).reset_index()\n    count = int(count[['count']].iloc[0])\n    fk_lst.append(count)\n    df = pd.DataFrame(data)\n\ndf['f_k'] = fk_lst\n\n"""
Export pandas dataframe to xlsx: dealing with the openpyxl issue on python 3.9,"'OK, I was able to replicate the problem. It is pandas related. Everything works just fine up to pandas 1.1.5\nIn pandas 1.2.0 they did some changes\nAt the time when you instantiate pd.ExcelWriter with\nwriter = pd.ExcelWriter(filename, engine=\'openpyxl\')`\n\nit creates empty file with size 0 bytes and overwrites the existing file  and then you get error when try to load it. It is not openpyxl related, because with latest version of openpyxl it works fine with pandas 1.1.5.\nThe solution - specify mode=\'a\', change the above line to\nwriter = pd.ExcelWriter(filename, engine=\'openpyxl\', mode=\'a\')\n\nAlternatively - look at this or  this solution where it loads the file before instantiating the pd.ExcelWriter.\nEDIT: I\'ve been advised in the comments that with mode=\'a\' it will raise FileNotFoundError  in case the file does not exists. Although it\'s unexpected that it will not create the file in this case, the solution is to move creating the writer inside the existing try block and create a writer with mode w in the except part:\ndef append_df_to_excel(filename, df, sheet_name=\'Sheet1\', startrow=None,\n                           truncate_sheet=False, \n                           **to_excel_kwargs):\n        """"""\n        Append a DataFrame [df] to existing Excel file [filename]\n        into [sheet_name] Sheet.\n        If [filename] doesn\'t exist, then this function will create it.\n     \n        Parameters:\n          filename : File path or existing ExcelWriter\n                     (Example: \'/path/to/file.xlsx\')\n          df : dataframe to save to workbook\n          sheet_name : Name of sheet which will contain DataFrame.\n                       (default: \'Sheet1\')\n          startrow : upper left cell row to dump data frame.\n                     Per default (startrow=None) calculate the last row\n                     in the existing DF and write to the next row...\n          truncate_sheet : truncate (remove and recreate) [sheet_name]\n                           before writing DataFrame to Excel file\n          to_excel_kwargs : arguments which will be passed to `DataFrame.to_excel()`\n                            [can be dictionary]\n     \n        Returns: None\n     \n        (c) [MaxU](https://stackoverflow.com/users/5741205/maxu?tab=profile)\n        """"""\n        from openpyxl import load_workbook\n     \n        # ignore [engine] parameter if it was passed\n        if \'engine\' in to_excel_kwargs:\n            to_excel_kwargs.pop(\'engine\')\n     \n        \n     \n        # Python 2.x: define [FileNotFoundError] exception if it doesn\'t exist \n        try:\n            FileNotFoundError\n        except NameError:\n            FileNotFoundError = IOError\n     \n     \n        try:\n            writer = pd.ExcelWriter(filename, engine=\'openpyxl\', mode=\'a\')\n            # try to open an existing workbook\n            writer.book = load_workbook(filename)\n             \n            # get the last row in the existing Excel sheet\n            # if it was not specified explicitly\n            if startrow is None and sheet_name in writer.book.sheetnames:\n                startrow = writer.book[sheet_name].max_row\n     \n            # truncate sheet\n            if truncate_sheet and sheet_name in writer.book.sheetnames:\n                # index of [sheet_name] sheet\n                idx = writer.book.sheetnames.index(sheet_name)\n                # remove [sheet_name]\n                writer.book.remove(writer.book.worksheets[idx])\n                # create an empty sheet [sheet_name] using old index\n                writer.book.create_sheet(sheet_name, idx)\n             \n            # copy existing sheets\n            writer.sheets = {ws.title:ws for ws in writer.book.worksheets}\n        except FileNotFoundError:\n            # file does not exist yet, we will create it\n            writer = pd.ExcelWriter(filename, engine=\'openpyxl\')\n     \n        if startrow is None:\n            startrow = 0\n     \n        # write out the new sheet\n        df.to_excel(writer, sheet_name, startrow=startrow, **to_excel_kwargs)\n     \n        # save the workbook\n        writer.save()\n\n', ""The solution is the following:\nimport pandas as pd\n\ndef append_df_to_excel(filename, df, sheet_name='Sheet1', startrow=None, startcol=None,\n    truncate_sheet=False, resizeColumns=True, na_rep = 'NA', **to_excel_kwargs):\n    """"""\n    Append a DataFrame [df] to existing Excel file [filename]\n    into [sheet_name] Sheet.\n    If [filename] doesn't exist, then this function will create it.\n\n    Parameters:\n      filename : File path or existing ExcelWriter\n                 (Example: '/path/to/file.xlsx')\n      df : dataframe to save to workbook\n      sheet_name : Name of sheet which will contain DataFrame.\n                   (default: 'Sheet1')\n      startrow : upper left cell row to dump data frame.\n                 Per default (startrow=None) calculate the last row\n                 in the existing DF and write to the next row...\n      truncate_sheet : truncate (remove and recreate) [sheet_name]\n                       before writing DataFrame to Excel file\n\n      resizeColumns: default = True . It resize all columns based on cell content width\n      to_excel_kwargs : arguments which will be passed to `DataFrame.to_excel()`\n                        [can be dictionary]\n      na_rep: default = 'NA'. If, instead of NaN, you want blank cells, just edit as follows: na_rep=''\n\n\n    Returns: None\n\n    *******************\n\n    CONTRIBUTION:\n    Current helper function generated by [Baggio]: https://stackoverflow.com/users/14302009/baggio?tab=profile\n    Contributions to the current helper function: https://stackoverflow.com/users/4046632/buran?tab=profile\n    Original helper function: (c) [MaxU](https://stackoverflow.com/users/5741205/maxu?tab=profile)\n\n\n    Features of the new helper function:\n    1) Now it works with python 3.9 and latest versions of pandas and openpxl\n    ---> Fixed the error: ""zipfile.BadZipFile: File is not a zip file"".\n    2) Now It resize all columns based on cell content width AND all variables will be visible (SEE ""resizeColumns"")\n    3) You can handle NaN,  if you want that NaN are displayed as NaN or as empty cells (SEE ""na_rep"")\n    4) Added ""startcol"", you can decide to start to write from specific column, oterwise will start from col = 0\n\n    *******************\n\n\n\n    """"""\n    from openpyxl import load_workbook\n    from string import ascii_uppercase\n    from openpyxl.utils import get_column_letter\n    from openpyxl import Workbook\n\n    # ignore [engine] parameter if it was passed\n    if 'engine' in to_excel_kwargs:\n        to_excel_kwargs.pop('engine')\n\n    try:\n        f = open(filename)\n        # Do something with the file\n    except IOError:\n        # print(""File not accessible"")\n        wb = Workbook()\n        ws = wb.active\n        ws.title = sheet_name\n        wb.save(filename)\n\n    writer = pd.ExcelWriter(filename, engine='openpyxl', mode='a')\n\n\n    # Python 2.x: define [FileNotFoundError] exception if it doesn't exist\n    try:\n        FileNotFoundError\n    except NameError:\n        FileNotFoundError = IOError\n\n\n    try:\n        # try to open an existing workbook\n        writer.book = load_workbook(filename)\n\n        # get the last row in the existing Excel sheet\n        # if it was not specified explicitly\n        if startrow is None and sheet_name in writer.book.sheetnames:\n            startrow = writer.book[sheet_name].max_row\n\n        # truncate sheet\n        if truncate_sheet and sheet_name in writer.book.sheetnames:\n            # index of [sheet_name] sheet\n            idx = writer.book.sheetnames.index(sheet_name)\n            # remove [sheet_name]\n            writer.book.remove(writer.book.worksheets[idx])\n            # create an empty sheet [sheet_name] using old index\n            writer.book.create_sheet(sheet_name, idx)\n\n        # copy existing sheets\n        writer.sheets = {ws.title:ws for ws in writer.book.worksheets}\n    except FileNotFoundError:\n        # file does not exist yet, we will create it\n        pass\n\n    if startrow is None:\n        # startrow = -1\n        startrow = 0\n\n    if startcol is None:\n        startcol = 0\n\n    # write out the new sheet\n    df.to_excel(writer, sheet_name, startrow=startrow, startcol=startcol, na_rep=na_rep, **to_excel_kwargs)\n\n\n    if resizeColumns:\n\n        ws = writer.book[sheet_name]\n\n        def auto_format_cell_width(ws):\n            for letter in range(1,ws.max_column):\n                maximum_value = 0\n                for cell in ws[get_column_letter(letter)]:\n                    val_to_check = len(str(cell.value))\n                    if val_to_check > maximum_value:\n                        maximum_value = val_to_check\n                ws.column_dimensions[get_column_letter(letter)].width = maximum_value + 2\n\n        auto_format_cell_width(ws)\n\n    # save the workbook\n    writer.save()\n\nExample Usage:\n# Create a sample dataframe\ndf = pd.DataFrame({'numbers': [1, 2, 3],\n                    'colors': ['red', 'white', 'blue'],\n                    'colorsTwo': ['yellow', 'white', 'blue'],\n                    'NaNcheck': [float('NaN'), 1, float('NaN')],\n                    })\n\n# EDIT YOUR PATH FOR THE EXPORT \nfilename = r""C:\\DataScience\\df.xlsx"" \n\n# RUN ONE BY ONE IN ROW THE FOLLOWING LINES, TO SEE THE DIFFERENT UPDATES TO THE EXCEL FILE\nappend_df_to_excel(filename, df, index=False, startrow=0) # Basic Export of df in default sheet (Sheet1)\nappend_df_to_excel(filename, df, sheet_name=""Cool"", index=False, startrow=0) # Append the sheet ""Cool"" where ""df"" is written\nappend_df_to_excel(filename, df, sheet_name=""Cool"", index=False) # Append another ""df"" to the sheet ""Cool"", just below the other ""df"" instance\nappend_df_to_excel(filename, df, sheet_name=""Cool"", index=False, startrow=0, startcol=5) # Append another ""df"" to the sheet ""Cool"" starting from col 5\nappend_df_to_excel(filename, df, index=False, truncate_sheet=True, startrow=10, na_rep = '') # Override (truncate) the ""Sheet1"", writing the df from row 10, and showing blank cells instead of NaN\n\n"""
Adding multiple rows to newly created columns in a pandas dataframe,"""You can unpack the values from the results list and assigns them to the columns col3 and col4:\nimport pandas as pd\n\ndf = pd.DataFrame({'col1':[1,2,3,4,5], 'col2':[1,2,3,4,5]})\n\nresults = [[1,2,3,4,5],[1,2,3,4,5]]\n\ndf['col3'], df['col4'] = results\n\n"", ""You could write the outputs of col3 and col4 to another dataframe and then join them. I'm not sure if this is still separating your results out from the model?\ndf = pd.DataFrame({'col1':[1,2,3,4,5], 'col2':[1,2,3,4,5]})\ndf2 = pd.DataFrame({'col3':[1,2,3,4,5],'col4':[1,2,3,4,5]})\ndf3 = df.join(df2)\nprint(df3)\n   col1  col2  col3  col4\n0     1     1     1     1\n1     2     2     2     2\n2     3     3     3     3\n3     4     4     4     4\n4     5     5     5     5\n\n"", ""In your specific example you could use the loc property. Make sure you insert the array in the right shape though:\ndf.loc[:,[""col3"", ""col4""]] = [[1,1],[2,2],[3,3],[4,4],[5,5]]\n\nAlternatively you can use numpy's transpose to create the right shape from the array that you had in your example.\ndf.loc[:,[""col3"", ""col4""]] = np.transpose([[1,2,3,4,5],[1,2,3,4,5]])\n\n"", 'Assuming your results are stored in a list of lists, you could convert that to  pandas DataFrame and join to the original:\nresults = [[1,2,3,4,5],[1,2,3,4,5]]\n\n>>> df.join(pd.DataFrame(results, index=[""col_3"",""col_4""]).T)\n\n   col1  col2  col_3  col_4\n0     1     1      1      1\n1     2     2      2      2\n2     3     3      3      3\n3     4     4      4      4\n4     5     5      5      5\n\n'"
Extract values based on timestamps diff conditions in pandas,"""This answer should work assuming you start from the first value of an ID, so the first timestamp.\nFirst, I added the 'Date_diff_cumsum' column, which stores the difference in days between the first date for the ID and the row's date:\ndf['Date_diff_cumsum'] = df.groupby('ID').Date.diff().dt.days\ndf['Date_diff_cumsum'] = df.groupby('ID').Date_diff_cumsum.cumsum().fillna(0)\n\nThen, I add the 'Value_diff' column, which is the difference between the first value for an ID and the row's value:\ndf['Val_diff'] = df.groupby('ID')['Val'].transform(lambda x:x-x.iloc[0])\n\nHere is what I get after adding the columns for your sample DataFrame:\n    Val     ID      Date                    Date_diff_cumsum    Val_diff\n0   0.90    0.0     2017-12-26 11:00:01     0.0                 0.00\n1   1.35    1.0     2017-12-26 11:00:01     0.0                 0.00\n2   0.72    1.0     2018-08-10 11:53:01     227.0               -0.63\n3   0.86    1.0     2018-10-14 08:33:01     291.0               -0.49\n4   0.99    2.0     2018-02-01 09:25:53     0.0                 0.00\n5   1.05    2.0     2018-04-01 08:00:04     58.0                0.06\n6   1.19    2.0     2018-07-01 08:50:00     149.0               0.20\n7   1.01    4.0     2017-01-01 08:12:02     0.0                 0.00\n\nAnd finally, return the rows which satisfy the conditions in your question:\ndf[((df['Val_diff']>=0.3) & (df['Date_diff_cumsum']<=2)) |\n   ((df['Val'] >= 1.5*(df['Val']-df['Val_diff'])) & (df['Date_diff_cumsum']<=7))]\n\nIn this case, it will return no rows.\nyesDf = df[((df['Val_diff']>=0.3) & (df['Date_diff_cumsum']<=2)) |\n           ((df['Val'] >= 1.5*(df['Val']-df['Val_diff'])) & (df['Date_diff_cumsum']<=7))].ID.drop_duplicates().to_frame()\n\nnoDf = df[~((df['Val_diff']>=0.3) & (df['Date_diff_cumsum']<=2)) |\n           ((df['Val'] >= 1.5*(df['Val']-df['Val_diff'])) & (df['Date_diff_cumsum']<=7))].ID.drop_duplicates().to_frame()\n\nyesDf contains the IDs that satisfy the condition, and noDf the ones that don't\n"""
Python large size array and high time cost,"'Your original code gives me an error because of a dimension mismatch for the first few entries in the array, which are too short because the window isn\'t full yet, so I changed it to discard the first values:\ndef rolling_approach(df, window_size=3):\n    return np.array(\n        [w.values for w in df[[""close"", ""volume""]].rolling(window=window_size)][\n            window_size - 1 :\n        ]\n    )\n\npd.DataFrame.rolling can be extremely slow for these kinds of operations. shift is very efficient in pandas  Here\'s an example for window_size=3 written out:\npd.concat(\n            [\n                df[[""close"", ""volume""]].shift().shift(),\n                df[[""close"", ""volume""]].shift(),\n                df[[""close"", ""volume""]],\n            ],\n            axis=1,\n        )\n        .values[2:, :]\n        .reshape(-1, 3, 2)\n    )\n\nWe stack the shifts and then reshape the values.\nGeneralized to a variable window_size we get:\ndef shift_approach(df, window_size=3):\n    shifted_df = pd.concat(\n        [df[[""close"", ""volume""]].shift(i) for i in range(window_size - 1, -1, -1)],\n        axis=1,\n    )\n    reshaped_array = shifted_df.values[window_size - 1 :, :].reshape(-1, window_size, 2)\n    return reshaped_array\n\n\nshift outperforms the rolling operation by almost two orders of magnitude:\n\nAnd it scales somewhat well into hundreds of millions of rows on my MacBook:\n\ndef setup(N):\n    np.random.seed(42)\n    close_values = np.random.randint(1, 100, size=N)\n    volume_values = np.random.randint(100, 1000, size=N)\n    df = pd.DataFrame({""close"": close_values, ""volume"": volume_values})\n    return [df, 10]\n\n\napproaches = [rolling_approach, shift_approach]\n# Show correctness\nfor approach in approaches[1:]:\n    data = setup(10)\n    assert np.isclose(approach(*data), approaches[0](*data)).all()\n\nrun_performance_comparison(\n    approaches,\n    [\n        1000,\n        3000,\n        5000,\n        10000,\n        30000,\n        100_000,\n        300_000,\n        500_000,\n        1_000_000,\n        3_000_000,\n        5_000_000,\n        10_000_000,\n    ],\n    setup=setup,\n    title=""Performance Comparison"",\n    number_of_repetitions=2,\n)\n\nProfiling code:\nimport timeit\nfrom functools import partial\n\nimport matplotlib.pyplot as plt\nfrom typing import List, Dict, Callable\n\nfrom contextlib import contextmanager\n\n\n@contextmanager\ndef data_provider(data_size, setup=lambda N: N, teardown=lambda: None):\n    data = setup(data_size)\n    yield data\n    teardown()\n\n\ndef run_performance_comparison(approaches: List[Callable],\n                               data_size: List[int],\n                               setup=lambda N: N,\n                               teardown=lambda: None,\n                               number_of_repetitions=5, title=\'Performance Comparison\', data_name=\'N\'):\n    approach_times: Dict[Callable, List[float]] = {approach: [] for approach in approaches}\n    for N in data_size:\n        with data_provider(N, setup, teardown) as data:\n            for approach in approaches:\n                function = partial(approach, *data)\n                approach_time = min(timeit.Timer(function).repeat(repeat=number_of_repetitions, number=2))\n                approach_times[approach].append(approach_time)\n\n    for approach in approaches:\n        plt.plot(data_size, approach_times[approach], label=approach.__name__)\n    plt.yscale(\'log\')\n    plt.xscale(\'log\')\n\n    plt.xlabel(data_name)\n    plt.ylabel(\'Execution Time (seconds)\')\n    plt.title(title)\n    plt.legend()\n    plt.show()\n\n'"
Using Pandas to write file creates blank lines,"'Have a look at the official documentation for pandas.DataFrame.to_csv\nFor convenience, I have posted some items of interest here:\n\nlineterminator : string, optional\nThe newline character or character sequence to use in the output file. Defaults to os.linesep, which depends on the OS in which this method is called (’\\n’ for linux, ‘\\r\\n’ for Windows, i.e.).\n⚠️ In older version of pandas, this parameter is called line_terminator.\n\n\nindex : bool, default True\nWrite row names (index).\n\nAre probably what you\'re looking for. As for the empty lines, try explicitly specifying a single newline:\ndf.to_csv(f, header=fieldnames, index=False, lineterminator=\'\\n\')\n\n', ""I came here just for the title and not removal of index numbers. That is why, for completeness sake, I want to add to the accepted answer, that removing the double linebreaks is done just by line_terminator='\\n'.\nIn this example this would be\nf = open(destination, 'w')\ndf.to_csv(f, line_terminator='\\n')\nf.close()\n\nor when using 'with open(..)'\nwith open(destination, 'w') as f\n    f.write(df.to_csv(line_terminator='\\n'))\n\nOther options such as headers can be added to df.to_csv() as needed.\n"""
Plot Different Data Frame in Single Line Plot,"'Since you just have numbers, I don\'t see the reason to use a pandas DataFrame, so I converted them to numpy arrays. Then it\'s just a matter of plotting each of the arrays in a single subplot.\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nL1 = np.array([20,19,18,17,16,15])\nK1 = np.array([15,14,13,11,18,21])\nk2 = np.array([10,15,16,21,22,25])\n\nfig, ax = plt.subplots()\nax.plot(L1, label=""L1"")\nax.plot(K1, label=""K1"")\nax.plot(k2, label=""k2"")\nax.legend()\nax.set_xlabel(""index"")\n\n\n', ""when you create a subplots set number of rows and columns plt.sublots(nbrow, nbcol)\nthis will return two objects: figure and axis of main frame each axis being a subplot\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\nL1 = pd.DataFrame([20,19,18,17,16,15])\nK1 = pd.DataFrame([15,14,13,11,18,21])\nk2 = pd.DataFrame([10,15,16,21,22,25])\nfig, axs = plt.subplots(3, 1)\naxs[0].plot(L1.values)\naxs[1].plot(K1.values)\naxs[2].plot(k2.values)\naxs[0].set_ylabel('YLabel 0')\naxs[1].set_ylabel('YLabel 1')\naxs[2].set_ylabel('YLabel 2')\nfig.align_ylabels()\nplt.show()\n\n"""
Shifting a Pandas time series without frequency,"""This works if you load it into a pd.Series object first, and then shift -\n\npd.Series(i).shift(-1).head()\n\n0   2001-02-20\n1   2001-03-26\n2   2001-04-12\n3   2001-04-24\n4   2001-07-05\nName: date, dtype: datetime64[ns]\n\n\nThe actual result contains NaNs, which you can remove using dropna.\n\npd.DatetimeIndex(pd.Series(i).shift(-1).dropna())\n\nDatetimeIndex(['2001-02-20', '2001-03-26', '2001-04-12', '2001-04-24',\n               '2001-07-05', '2001-08-15', '2001-09-10', '2001-09-18',\n               '2001-10-02', '2001-10-11', '2001-10-30', '2001-12-13',\n               '2002-03-07', '2002-06-13', '2002-09-12', '2002-12-12',\n               '2003-03-13', '2003-06-12', '2013-02-19', '2013-05-28',\n               '2013-09-03', '2014-01-21', '2014-02-18', '2014-05-27',\n               '2014-07-07', '2014-09-02', '2015-01-20', '2015-02-17',\n               '2015-05-26', '2015-07-06', '2016-05-31', '2016-07-05',\n               '2016-09-06', '2016-10-04', '2017-01-17', '2017-02-21',\n               '2017-05-30', '2017-09-05'],\n              dtype='datetime64[ns]', name='date', freq=None)\n\n"""
Python : How to split the given start date and end date in a dataframe into number of days falling in each month creating new row for every date split,"""I haven't been able to find a good way to vectorize this operation, so I'm not sure how this will perform across 30 million rows.\ndef split_dates(row):\n    row = row.iloc[0]\n    months = int(np.datetime64(row.endDate, ""M"") - np.datetime64(row.startDate, ""M""))\n\n    start = np.datetime64(row.startDate, ""D"")\n    end = np.datetime64(row.endDate, ""D"")\n\n    ends = np.array(\n        [start + i * pd.offsets.MonthEnd() for i in range(1, months + 1)],\n        dtype=""datetime64[D]"",\n    )\n    starts = ends + np.timedelta64(1, ""D"")\n\n    starts = np.insert(starts, 0, start)\n    ends = np.append(ends, end)\n\n    new_df = pd.DataFrame({""startDate"": starts, ""endDate"": ends})\n\n    return new_df\n\n\ndf.groupby(""ID"").apply(split_dates).reset_index().drop(columns=""level_1"")\n\n"""
ValueError: Cannot shift with no freq,"""To resolve the error, instead of df.index.shift(), I had to use pd.Series(df.index).shift().\nOverall, I used it as below:\ndf['index_diff'] = (df.index - pd.Series(df.index).shift()).values\n\n"", ""The problem was that I had date as index. Since only weekdays were shown delta became incorrect. When I changed to period.\n\nfor column_names in list(df.columns.values):\n        df[column_names+'%-daily'] =df[column_names].pct_change(periods=1).fillna(0)\n\n"""
Read Teradata query into Pandas,"'You could nicely use sqlalchemy\n#remember to first pip install teradatasql\n\nfrom sqlalchemy import create_engine\nfrom urllib import parse\n\nhost = \'localhost\'\nuser = \'your username\'\npwd = \'your password\'\n\n#Create your sqlalchemy engine object\nengine = create_engine(f""teradatasql://{host}/?user={user}&password={parse.quote(pwd)}"")\n\n#Read the table from the database into a pandas dataframe named df\ndf = pd.read_sql(""SELECT * FROM my_table"", engine)\n\nprint(df.info())\n\n#you are done\n\n', ""You can use slqalchemy but you will need to install sqlalchemy-teradata too. You can do that via PIP\n\npip install sqlachemy-teradata\n\n\nThe rest of the code remains the same :)\n\nfrom sqlalchemy import create_engine\nimport pandas as pd\n\nuser, pasw, host = 'username','userpass', 'hostname'\n\n# connect\ntd_engine = create_engine('teradata://{}:{}@{}:22/'.format(user,pasw,hostname))\n\n# execute sql\nquery = 'select * from dbc.usersV'\nresult = td_engine.execute(query)\n\n#To read your query to Pandas\ndf = pd.read_sql(query,td_engine)  \n\n"", 'I did it using read_sql . Below id the code snip :\n\ndef dqm() :\n    conn_rw = create_connection()\n    dataframes = []\n    srcfile = open(\'srcqueries.sql\', \'rU\').read()\n    querylist = srcfile.split(\';\')\n    querylist.pop()\n    for query in querylist :\n        dataframes.append(pd.read_sql(query, conn_rw))\n    close_connection(conn_rw)\n    return dataframes,querylist\n\n\nYou can create connection as below :\n\n    def create_connection():\n        conn = pyodbc.connect(""DRIVER=Teradata;DBCNAME=tddb;UID=uid;PWD=pwd;QUIETMODE=YES"", autocommit=True,unicode_results=True)\n        return conn\n\n\nYou can check complete code here : GitHub Link\nLet me know if this answers your query .\n'"
Fillna row wise as per change in another column,"'You can use cumsum to cumulative differences in col_2 and use those to calculate col_1 based on the last available value:\ndf[""col_1""] = df[""col_1""].fillna(df[""col_2""].diff().where(df[""col_1""].isna()).cumsum()+df[""col_1""].ffill())\n\n>>> df\n              col_1    col_2\n2022-10-31   99.094  102.498\n2022-11-30   99.001  101.880\n2022-12-31  105.619  108.498\n2023-01-31   97.621  100.500\n\n'"
How can I obtain the same result as pandas.autocorr() by numpy?,"'If we check the doc of the pandas.Series.autocorr, if you call the function with default arguments, the lag is 1, which means you need to shift one element for calculating the correlation.\nFor example:\na = np.array([0.25, 0.5, 0.2, -0.05])\ns = pd.Series(a)\n\ngives you :\n0.1035526330902407\n\nWith np.corrcoef you need to slice the array to two arrays shifted :\nnp.corrcoef(a[:-1], a[1:])[0, 1]\n\nWhich gives you same result:\n0.1035526330902407\n\nSo in your case the codes should be like :\nW = 5 # Window size\nnrows = len(df) - W + 1 # number of elemnets after rolling\nlag=1\ny = []\nfor i in range(nrows):\n    y.append(np.corrcoef(df[\'A\'][i:i+W-lag],df[\'A\'][i+lag:i+W])[0,1])\n\nYou will get same result as x.\n', 'The pd.autocorr() computes the autocorrelation with a default lag=1.  This means that it computes the correlation between an array and an array shifted by 1.\nHere are a couple of ways to compute:\n1. Looping through and appending for each np.corrcoef\nYou almost had the correct answer, but you just needed to offset by the lag=1 parameter:\ny = []\nfor i in range(16):\n    y.append(np.corrcoef(df.A[i:i+5][:-1], df.A[i:i+5][1:])[0, 1])\n\n# 3.66 ms ± 98.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\nThe solution is a mix of your own working and this answer - which might give a bit more explanation for you.\n2. Without loop, using np.lib.stride_tricks.sliding_window_view\nnp.lib.stride_tricks.sliding_window_view allows you to create an array of the sliding window.  You can then slice this by the lag parameter and calculate the np.corrcoef, then take the diagonal output of the second half of the output array:\nwindow = 5\nlag = 1\n\ny = np.diagonal(\n    np.corrcoef(\n        np.lib.stride_tricks.sliding_window_view(df.A, window)[:, :-lag],\n        np.lib.stride_tricks.sliding_window_view(df.A, window)[:, lag:])\n    [len(df.A) - window + 1:])\n\n# 174 µs ± 2.32 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n\nAs can be seen from the timings, the second option is far faster - although the code and concepts are perhaps a little more abstract.\n'"
Find First and Second Occurrence of Value Across Two Columns by Group,"""I managed to get the result by applying a separate function to groups of the dataframe, turning the result into a new dataframe and summarizing it.\nIn the function the main trick is to use np.where. I used it on a sum of columns PP1 and PP2 to find just any occurences, and then checked which column provided the occurence just by checking if the value in PP1 column is 1 (if yes - then the occurence is in PP1, if not - in PP2, as you said that the occurences can not happen simultaneously).\nAlthough, I am not sure why your output doesn't have 1 for P1 Second Occ, because the first group (Ref1 == ""aaaa"" and EvnNo == 0) shows exactly that, if I understood the question correctly.\nimport numpy as np\n\ndef count_occurences(group):\n    result = [0] * 4\n\n    occurences = np.where(group.sum(axis=1) == 1)[0]\n    \n    # track first occurence\n    if len(occurences) > 0 and group.iloc[occurences[0]][""PP1""] == 1:\n        result[0] += 1\n    elif len(occurences) > 0:\n        result[1] += 1\n    \n    # track second occurence\n    if len(occurences) > 1 and group.iloc[occurences[1]][""PP1""] == 1:\n        if result[0] != 1:\n            result[2] += 1\n    elif len(occurences) > 1:\n        if result[1] != 1:\n            result[3] += 1\n        \n    return result\n\n\noccurences_df = pd.DataFrame(\n    df \\\n        .groupby([""Ref1"", ""EvnNo""]) \\\n        [[""PP1"", ""PP2""]] \\\n        .apply(count_occurences) \\\n        .to_list(),\n    columns = [""P1 First Occ"", ""P2 First Occ"", ""P1 Second Occ"", ""P2 Second Occ""]\n)\n\nprint(occurences_df.sum())\n\nOutput:\nP1 First Occ     2\nP2 First Occ     1\nP1 Second Occ    1\nP2 Second Occ    1\n\n"""
How can I filter a dataframe for rows between two given timestamps?,"'Convert the timestamp column to a datetime data type if it is not already in that format. Assuming your timestamp column is already in the correct format, we can skip this step.\nSet the timestamp column as the index of the dataframe. This step is necessary to use the .loc accessor to filter by timestamps.\nUse the .loc accessor to select the rows that fall within the desired range of timestamps.\ndf[\'timestamp\'] = pd.to_datetime(df[\'timestamp\'])\ndf.set_index(\'timestamp\', inplace=True)\nstart_timestamp = pd.Timestamp(1640000000000000000)\nend_timestamp = pd.Timestamp(1680000000000000000)\nfiltered_df = df.loc[(df.index >= start_timestamp) & (df.index <= end_timestamp)]\nfiltered_df.reset_index(inplace=True)\n\n\n\n', ""df = df.loc[(df['timestamp'] > 1640000000000000000) & (df['timestamp'] < 1680000000000000000)]\n\nIf you want to include the numbers themselves, you can use >= and <= respectively.\n"""
How to convert a json object to pandas json type column,"""You simply need to rewrite your line with JSON to this one:\ndata = [[110636, '[{""Name"": ""cd0"", ""id"": ""1""}, {""Name"": ""cd1"", ""id"": ""2""}, {""Name"": ""cd2"", ""id"": ""3""}]']]\nFirst, JSON object must be str, bytes, or bytearray, not list. Thus, add a single quote to make this list a string. \nSecond, JSON format is expected to have double quoting for your name properties.\nIf you run your code with mine correction, the output would be like this:\n  Name id\n0  cd0  1\n1  cd1  2\n2  cd2  3\n\nOn the other hand, if that [[110636, [{'Name': 'cd0', 'id': '1'}, {'Name': 'cd1', 'id': '2'}, {'Name': 'cd2', 'id': '3'}]]] is the exact format of your data, try this:\ndata = [[str(item).replace(""'"", ""\\"""") for v1 in data for item in v1]]\nbefore applying json.loads to your df. \nThe reason you actually getting an error message is that both id, which is 110636, and the following list have to be in quotes, as those are JSON properties\n"""
Python parse then put in a dataframe,"'Another regex approach with pivot:\nimport re\n\n                                                         # or file.read()\nout = (pd.DataFrame(re.findall(r\'^\\s+(\\w+)(\\d+) = (\\d+)\', text, flags=re.M))\n         .pivot(index=1, columns=0, values=2)\n         .rename_axis(index=None, columns=None)\n      )\n\nprint(out)\n\nOutput:\n     DATA ERROR\n1  123456   500\n2   56789   505\n\nUsed input:\ntext = \'\'\'------------------------------\n------------------------------\n<TIME:2020-01-01 01:25:10> \n<TIME:2020-01-01 01:25:10> \n<TIME:2020-01-01 01:25:10> \n<TIME:2020-01-01 01:25:10>\n\n------\n++++++\n%%RequestHandler\n    DATA1 = 123456\n    ERROR1 = 500\n    DATA2 = 56789\n    ERROR2 = 505\n\nCount = 4\'\'\'\n\nregex demo\n', 'Here is the code that you want, you can regular expressions to extract desired data from raw structured text file:\nimport re\nimport pandas as pd\n\n# Read the file\nwith open(""file.txt"", ""r"") as file:\n    content = file.read()\n\n# Use regular expressions to extract the values\ndata = re.findall(r""DATA\\d+\\s*=\\s*(\\d+)"", content)\nerror = re.findall(r""ERROR\\d+\\s*=\\s*(\\d+)"", content)\n\n# Create a dataframe\ndf = pd.DataFrame({""DATA"": data, ""ERROR"": error})\nprint(df)\n\nExample:\nimport re\nimport pandas as pd\n\ncontent = \'\'\'\n------------------------------\n------------------------------\n<TIME:2020-01-01 01:25:10> \n<TIME:2020-01-01 01:25:10> \n<TIME:2020-01-01 01:25:10> \n<TIME:2020-01-01 01:25:10>\n\n------\n++++++\n%%RequestHandler\n    DATA1 = 123456\n    ERROR1 = 500\n    DATA2 = 56789\n    ERROR2 = 505\n\nCount = 4\n---\n\'\'\'\n\ndata = re.findall(r""DATA\\d+\\s*=\\s*(\\d+)"", content)\nerror = re.findall(r""ERROR\\d+\\s*=\\s*(\\d+)"", content)\n\ndf = pd.DataFrame({""DATA"": data, ""ERROR"": error})\nprint(df)\n\n\nOutput:\n     DATA ERROR\n0  123456   500\n1   56789   505\n\n'"
Replacing Pandas or Numpy Nan with a None to use with MysqlDB,"""replace np.nan with None is accomplished differently across different version of pandas:\nif version.parse(pd.__version__) >= version.parse('1.3.0'):\n    df = df.replace({np.nan: None})\nelse:\n    df = df.where(pd.notnull(df), None)\n\nthis solves the issue that for pandas versions <1.3.0, if the values in df are already None then df.replace({np.nan: None}) will toggle them back to np.nan and vice versa.\n"", 'This should work:\ndf[""column""]=df[""column""].apply(lambda x: None if pd.isnull(x) else x)\n', 'Sometimes it is better to use this code. Note that np refers to the numpy:\ndf = df.fillna(np.nan).replace([np.nan], [None])\n\n', 'df = df.replace({np.nan: None})\n\nNote: For pandas versions <1.4, this changes the dtype of all affected columns to object.\nTo avoid that, use this syntax instead:\ndf = df.replace(np.nan, None)\n\nCredit goes to this guy here on this Github issue and Killian Huyghe\'s comment.\n', 'Doing it by hand is the only way that is working for me right now.\nThis answare from @rodney cox worked for me in almost every case.\nThe following code set all columns to object data type and then replace any null value to None. Setting the column data type to object is crucial because it prevents pandas to change the type further.\nfor col in df.columns:\n    df[col] = df[col].astype(object)\n    df.loc[df[col].isnull(), col] = None\n\nWarning: This solution is not eficient, because it process columns that might not have np.nan values.\n', 'Astoundingly, None of the previous answers worked for me, so I had to do it for each column.\nfor column in df.columns:\n            df[column] = df[column].where(pd.notnull(df[column]), None)\n\n', 'Yet another option, that actually did the trick for me:\ndf = df.astype(object).replace(np.nan, None)\n\n', ""After finding that neither the recommended answer, nor the alternate suggested worked for my application after a Pandas update to 1.3.2 I settled for safety with a brute force approach:\nbuf = df.to_json(orient='records')\nrecs = json.loads(buf)\n\n"", 'Convert numpy NaN to pandas NA before replacing with the where statement:\ndf = df.replace(np.NaN, pd.NA).where(df.notnull(), None)\n\n', 'This worked for me:\ndf = df.fillna(0)\n\n', 'Do you have a code block to review by chance?\nUsing .loc, pandas can access records based on logic conditions (filtering) and do action with them (when using =). Setting a .loc mask equal to some value will change the return array inplace (so be a touch careful here; I suggest test on a df copy prior to using in code block).\ndf.loc[df[\'SomeColumn\'].isna(), \'SomeColumn\'] = None\n\nThe outer function is df.loc[row_label, column_label] = None. We\'re going to use a boolean mask for row_label by using the .isna() method to find \'NoneType\' values in our column SomeColumn.\nWe\'ll use the .isna() method to return a boolean array of rows/records in column SomeColumn as our row_label: df[\'SomeColumn\'].isna(). It will isolate all rows where SomeColumn has any of the \'NoneType\' items pandas checks for with the .isna() method.\nWe\'ll use the column_label both when masking the dataframe for the row_label, and to identify the column we want to act on for the .loc mask.\nFinally, we set the .loc mask equal to None, so the rows/records returned are changed to None based on the masked index.\nBelow are links to pandas documentation regarding .loc & .isna().\nReferences:\nhttps://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html\nhttps://pandas.pydata.org/docs/reference/api/pandas.DataFrame.isna.html\n', 'I believe the cleanest way would be to make use of the na_value argument in the pandas.DataFrame.to_numpy() method (docs):\n\nna_value : Any, optional\nThe value to use for missing values. The default value depends on dtype  and the dtypes of the DataFrame columns.\nNew in version 1.1.0.\n\nYou could e.g. convert to dictionaries with NaN\'s replaced by None using\ncolumns = df.columns.tolist()\ndicts_with_nan_replaced = [\n    dict(zip(columns, x))\n    for x in df.to_numpy(na_value=None)\n]\n\n', '@bogatron has it right, you can use where, it\'s worth noting that you can do this natively in pandas:\n\ndf1 = df.where(pd.notnull(df), None)\n\n\nNote: this changes the dtype of all columns to object.\n\nExample:\n\nIn [1]: df = pd.DataFrame([1, np.nan])\n\nIn [2]: df\nOut[2]: \n    0\n0   1\n1 NaN\n\nIn [3]: df1 = df.where(pd.notnull(df), None)\n\nIn [4]: df1\nOut[4]: \n      0\n0     1\n1  None\n\n\n\n\nNote: what you cannot do recast the DataFrames dtype to allow all datatypes types, using astype, and then the DataFrame fillna method:\n\ndf1 = df.astype(object).replace(np.nan, \'None\')\n\n\nUnfortunately neither this, nor using replace, works with None see this (closed) issue.\n\n\n\nAs an aside, it\'s worth noting that for most use cases you don\'t need to replace NaN with None, see this question about the difference between NaN and None in pandas.\n\nHowever, in this specific case it seems you do (at least at the time of this answer).\n', 'Another addition: be careful when replacing multiples and converting the type of the column back from object to float. If you want to be certain that your None\'s won\'t flip back to np.NaN\'s apply @andy-hayden\'s suggestion with using pd.where.\nIllustration of how replace can still go \'wrong\':\n\nIn [1]: import pandas as pd\n\nIn [2]: import numpy as np\n\nIn [3]: df = pd.DataFrame({""a"": [1, np.NAN, np.inf]})\n\nIn [4]: df\nOut[4]:\n     a\n0  1.0\n1  NaN\n2  inf\n\nIn [5]: df.replace({np.NAN: None})\nOut[5]:\n      a\n0     1\n1  None\n2   inf\n\nIn [6]: df.replace({np.NAN: None, np.inf: None})\nOut[6]:\n     a\n0  1.0\n1  NaN\n2  NaN\n\nIn [7]: df.where((pd.notnull(df)), None).replace({np.inf: None})\nOut[7]:\n     a\n0  1.0\n1  NaN\n2  NaN\n\n', 'Just an addition to @Andy Hayden\'s answer:\n\nSince DataFrame.mask is the opposite twin of DataFrame.where, they have the exactly same signature but with opposite meaning:\n\n\nDataFrame.where is useful for Replacing values where the condition is False. \nDataFrame.mask is used for Replacing values where the condition is True.\n\n\nSo in this question, using df.mask(df.isna(), other=None, inplace=True) might be more intuitive.\n', 'After stumbling around, this worked for me:\n\ndf = df.astype(object).where(pd.notnull(df),None)\n\n', ""Quite old, yet I stumbled upon the very same issue.\nTry doing this:\n\ndf['col_replaced'] = df['col_with_npnans'].apply(lambda x: None if np.isnan(x) else x)\n\n"", ""You can replace nan with None in your numpy array:\n\n>>> x = np.array([1, np.nan, 3])\n>>> y = np.where(np.isnan(x), None, x)\n>>> print y\n[1.0 None 3.0]\n>>> print type(y[1])\n<type 'NoneType'>\n\n"""
Group by and aggregate the columns in pandas dataframe,"""It could be because col4 contains integers, therefore the join doesn't work. You could try with an if/else like this:\ndata = {'col1': {0: 'THREE M SYNDROME 1  {3-M syndrome 1, 273750 3)}',\n  1: 'THREE M SYNDROME 1  {3-M syndrome 1, 273750 (3)}'},\n 'col2': {0: '3-m syndrome 1', 1: '3-m syndrome 2'},\n 'col3': {0: '{3-M syndrome 1}', 1: '{3-M syndrome 2}'},\n 'col4': {0: 273750, 1: 273750}}\n\ndf = pd.DataFrame(data)\n\n>>> df.groupby('col1').agg(lambda x: ' | '.join(x.unique()) if x.nunique()>1 else x.unique()[0]   )\n\nOut:\n\n                                                col2    col3    col4\ncol1            \nTHREE M SYNDROME 1 {3-M syndrome 1, 273750 (3)} 3-m syndrome 1 | 3-m syndrome 2 {3-M syndrome 1} | {3-M syndrome 2} 273750\n\n"""
Read dat. file with pandas,"'Assuming that the lines starting with a # character are comments, you can use:\ndf = pd.read_csv(<file_path>, delim_whitespace = True, comment=\'#\')\n\nOther useful options for dealing with lines in your CSV that do not contain data are on_bad_lines and skiprows / skipfooter.\nAlso refer to the documentation of pandas.read_csv().\n'"
Merging pandas dataframes with 2 keys that are not the same,"'This looks like a merge_asof.\nHere on the nearest previous value:\nout = (pd.merge_asof(tab1.reset_index().sort_values(by=\'col2\'),\n                     tab2.sort_values(by=\'col2\'),\n                     on=\'col2\', by=\'col1\', direction=\'backward\')\n         .set_index(\'index\').reindex(tab1.index)\n      )\n\nOutput:\n  col1  col2 col3\n0    A  2017  foo\n1    A  2018  foo  # 2018 is absent, let\'s take 2017\n2    A  2019  fii\n3    B  2017  boo\n4    B  2019  boo\n5    C  2017  coo\n6    C  2018  cii\n\nOne the nearest following value:\nout = (pd.merge_asof(tab1.reset_index().sort_values(by=\'col2\'),\n                     tab2.sort_values(by=\'col2\'),\n                     on=\'col2\', by=\'col1\', direction=\'forward\')\n         .set_index(\'index\').reindex(tab1.index)\n      )\n\nOutput:\n  col1  col2 col3\n0    A  2017  foo\n1    A  2018  fii  # 2018 is absent, let\'s take 2019\n2    A  2019  fii\n3    B  2017  boo\n4    B  2019  bii\n5    C  2017  coo\n6    C  2018  cii\n\nHere on the nearest overall value:\nout = (pd.merge_asof(tab1.reset_index().sort_values(by=\'col2\'),\n                     tab2.sort_values(by=\'col2\'),\n                     on=\'col2\', by=\'col1\', direction=\'nearest\')\n         .set_index(\'index\').reindex(tab1.index)\n      )\n\nOutput:\n  col1  col2 col3\n0    A  2017  foo\n1    A  2018  foo  # 2017/2019 are equidistant, let\'s take 2017\n2    A  2019  fii\n3    B  2017  boo\n4    B  2019  bii  # 2020 is closer than 2017\n5    C  2017  coo\n6    C  2018  cii\n\nAnd if you want to merge on the nearest value, with the forward one in case of equality:\nout = (pd.merge_asof(tab1.reset_index().eval(\'col2=-col2\').sort_values(by=\'col2\'),\n                     tab2.eval(\'col2=-col2\').sort_values(by=\'col2\'),\n                     on=\'col2\', by=\'col1\', direction=\'nearest\')\n         .set_index(\'index\').reindex(tab1.index)\n         .eval(\'col2=-col2\')\n      )\n\nOutput:\n  col1  col2 col3\n0    A  2017  foo\n1    A  2018  fii  # 2017 and 2019 are equality distant, give 2019 priority \n2    A  2019  fii\n3    B  2017  boo\n4    B  2019  bii  # 2020 is closer than 2017\n5    C  2017  coo\n6    C  2018  cii\n\n'"
Newline character for pandas .to_html?,"'You can apply the changes that you want in the dataframe like this using render and df.style.format:\nimport pandas as pd\n\ndata = {\'Text\': [""Hello <br> World"", ""Hello \\n World""]}\ndf = pd.DataFrame(data)\n\ndef add_line_breaks(text):\n    return text.replace(\'\\n\', \'<br>\')\n\ndf_styled = df.style.format({\'Text\': add_line_breaks})\n\nhtml_table = df_styled.render()\n\nwith open(\'output.html\', \'w\') as file:\n    file.write(html_table)\n\nOutput:\n\n'"
How to Subtract Values from Matching Columns in Two Different Pandas DataFrames?,"""You can specify columns for corresponding match between both DataFrames, convert to index and subtract:\ncols = ['BusinessUtc','Horizon','BZ']\ndf = df_t.set_index(cols)['Value'].sub(df2.set_index(cols)['Value']).reset_index()\nprint (df)\n   BusinessUtc Horizon   BZ     Value\n0   2023-01-01     D-1  NO1     0.000\n1   2023-01-01     D-1  NO1   199.394\n2   2023-01-01     D-1  NO1   630.551\n3   2023-01-01     D-1  NO1   344.372\n4   2023-01-01     D-1  NO4     0.000\n5   2023-01-01     D-1  NO4   -30.202\n6   2023-01-01     D-1  NO4   158.362\n7   2023-01-01     D-1  NO4     0.676\n8   2023-01-01     D-1  NO5     0.000\n9   2023-01-01     D-1  NO5  2580.621\n10  2023-01-01     D-1  NO5  2294.442\n11  2023-01-01     D-1  NO5     0.000\n\nIf need match by one column (and add new subtract column) use mapping:\ndf = df_t.assign(sub = df_t['Value'].sub(df_t['BZ'].map(df2.set_index('BZ')['Value'])))\nprint (df)\n  BusinessUtc Horizon EvaluationUtc   BZ     Value       sub\n0   2023-01-01     D-1    2023-01-01  NO4  1301.681     0.000\n1   2023-01-01     D-1    2022-12-31  NO4  1271.479   -30.202\n2   2023-01-01     D-1    2022-12-30  NO4  1460.043   158.362\n3   2023-01-01     D-1    2022-12-29  NO4  1302.357     0.676\n4   2023-01-01     D-1    2023-01-01  NO1  3412.155     0.000\n5   2023-01-01     D-1    2022-12-31  NO1  3611.549   199.394\n6   2023-01-01     D-1    2022-12-30  NO1  4042.706   630.551\n7   2023-01-01     D-1    2022-12-29  NO1  3756.527   344.372\n8   2023-01-01     D-1    2023-01-01  NO5  1462.085     0.000\n9   2023-01-01     D-1    2022-12-31  NO5  4042.706  2580.621\n10  2023-01-01     D-1    2022-12-30  NO5  3756.527  2294.442\n11  2023-01-01     D-1    2022-12-29  NO5  1462.085     0.000\n\n"", 'As noted in the comments, the expected output is unclear.\nI really you need to broadcast the value for a set of columns, then a left-merge seems most appropriate (it will maintain the original structure):\ncols = [\'BusinessUtc\',\'Horizon\',\'BZ\']\nout = df_t.assign(Value=df_t[\'Value\'].sub(df_t.drop(columns=\'Value\')\n                                              .merge(df2, on=cols, how=\'left\')[\'Value\'],\n                                          fill_value=0))\n\nOutput:\n   BusinessUtc Horizon EvaluationUtc   BZ     Value\n0   2023-01-01     D-1    2023-01-01  NO4     0.000\n1   2023-01-01     D-1    2022-12-31  NO4   -30.202\n2   2023-01-01     D-1    2022-12-30  NO4   158.362\n3   2023-01-01     D-1    2022-12-29  NO4     0.676\n4   2023-01-01     D-1    2023-01-01  NO1     0.000\n5   2023-01-01     D-1    2022-12-31  NO1   199.394\n6   2023-01-01     D-1    2022-12-30  NO1   630.551\n7   2023-01-01     D-1    2022-12-29  NO1   344.372\n8   2023-01-01     D-1    2023-01-01  NO5     0.000\n9   2023-01-01     D-1    2022-12-31  NO5  2580.621\n10  2023-01-01     D-1    2022-12-30  NO5  2294.442\n11  2023-01-01     D-1    2022-12-29  NO5     0.000\n\n'"
Adding rows for missing months and Fill na values with last value in a partition Python,"'One option using reshaping as a rectangular intermediate with pivot/stack:\ndf[\'month\'] = pd.to_datetime(df[\'month\'])\n\nout = (df\n    .pivot(index=\'id\', columns=\'month\', values=\'value\')\n    .reindex(columns=pd.date_range(\'2023-01-01\', \'2023-04-01\', freq=\'MS\').rename(\'month\'))\n    .ffill(axis=1)\n    .stack().reset_index(name=\'value\')\n)\n\nOr with a MultiIndex and groupby.ffill:\nout = (df\n   .set_index([\'id\', \'month\'])\n   .reindex(pd.MultiIndex.from_product([df[\'id\'].unique(),\n                                        pd.date_range(\'2023-01-01\', \'2023-04-01\', freq=\'MS\')],\n                                      names=[\'id\', \'month\']\n                                      ))\n   .groupby(level=0).ffill().dropna().reset_index()\n)\n\nOutput:\n    id      month     value\n0    1 2023-01-01    London\n1    1 2023-02-01     Paris\n2    1 2023-03-01     Paris\n3    1 2023-04-01     Paris\n4    2 2023-01-01  New York\n5    2 2023-02-01  New York\n6    2 2023-03-01  New York\n7    2 2023-04-01  New York\n8    3 2023-02-01     Paris\n9    3 2023-03-01     Paris\n10   3 2023-04-01     Paris\n\nkeeping/filling original NaNs:\nYou can add a helper column to identify the original NaNs:\nout = (df\n   .set_index([\'id\', \'month\']).assign(flag=True)\n   .reindex(pd.MultiIndex.from_product([df[\'id\'].unique(),\n                                        pd.date_range(\'2023-01-01\', \'2023-04-01\', freq=\'MS\')],\n                                      names=[\'id\', \'month\']\n                                      ))\n   .groupby(level=0).ffill().loc[lambda d: d.pop(\'flag\').notna()].reset_index()\n)\n\nOutput:\n    id      month     value\n0    1 2023-01-01    London\n1    1 2023-02-01     Paris\n2    1 2023-03-01     Paris\n3    1 2023-04-01     Paris\n4    2 2023-01-01  New York\n5    2 2023-02-01  New York\n6    2 2023-03-01  New York\n7    2 2023-04-01  New York\n8    3 2023-02-01     Paris\n9    3 2023-03-01     Paris\n10   3 2023-04-01     Paris\n11   4 2023-03-01       NaN\n12   4 2023-04-01       NaN\n\n'"
Complex grouping and ordering with pandas,"'I would use value_counts, then reindex/slice with the desired order:\norder = [\'world\', \'bar\', \'foo\']\nout = df[[\'A\', \'B\']].value_counts().loc[order]\n\nOutput:\nA      B    \nworld  text2    1\n       text3    1\nbar    text1    1\n       text3    1\nfoo    text1    2\n       text2    1\ndtype: int64\n\nVariant using groupby.count:\nout = df.groupby([\'A\', \'B\']).count().loc[order]\n\nOr if you really want to use pivot_table:\nout = df.pivot_table(index=[\'A\', \'B\'], values=\'C\', aggfunc=\'count\').loc[order]\n\nOutput:\n             C\nA     B       \nworld text2  1\n      text3  1\nbar   text1  1\n      text3  1\nfoo   text1  2\n      text2  1\n\nOther variant with an ordered Categorical:\ndf[\'A\'] = pd.Categorical(df[\'A\'], categories=[\'world\', \'bar\', \'foo\'])\n\nout = df[[\'A\', \'B\']].value_counts().sort_index(level=0, kind=\'stable\')\n\n'"
python backtest (bt) error - TypeError: unhashable type: &#39;Series&#39;,"'the issue is related to the signal variable inside your code that is a pandas series and it is unhashable but WeightTarget requires a hashable input so hope it helps:\nimport pandas as pd\nimport numpy as np\nimport bt\n\n# Sample trading data\ndata = {\n    \'Date\': [\'2023-01-01\', \'2023-01-02\', \'2023-01-03\', \'2023-01-04\', \'2023-01-05\'],\n    \'Open\': [100, 105, 110, 108, 115],\n    \'Close\': [105, 110, 108, 115, 112],\n    \'High\': [115, 112, 115, 116, 118],\n    \'Low\': [98, 103, 107, 105, 108]\n}\n\n# Create DataFrame\ndf = pd.DataFrame(data)\ndf[\'Date\'] = pd.to_datetime(df[\'Date\'])\ndf.set_index(\'Date\', inplace=True)\n\n# Calculate moving averages\ndf[\'SMA_long\'] = df[\'Close\'].rolling(window=50).mean()\ndf[\'SMA_short\'] = df[\'Close\'].rolling(window=20).mean()\n\n# Generate signal as a DataFrame\nsignal_df = pd.DataFrame(index=df.index)\nsignal_df[\'Close\'] = 0 # Default to 0\nsignal_df.loc[df[\'SMA_long\'] > df[\'SMA_short\'], \'Close\'] = 1\nsignal_df.loc[df[\'SMA_long\'] < df[\'SMA_short\'], \'Close\'] = -1\n\n# Create strategy and backtest\nbt_strategy = bt.Strategy(\'EMA_crossover\', [bt.algos.WeighTarget(signal_df), bt.algos.Rebalance()])\nbt_backtest = bt.Backtest(bt_strategy, df)\nbt_result = bt.run(bt_backtest)\n\n# Print results\nprint(bt_result)\n\n'"
college football analysis - TypeError when obtaining star player counts,"""import pandas as pd\n\ndf_recruits = pd.read_csv(r'C:\\Users\\sjohn\\OneDrive\\Documents\\D195 Capstone\\DataFrames\\df_recruits_typeerror.csv')\ndf_teams = pd.read_csv(r'C:\\Users\\sjohn\\OneDrive\\Documents\\D195 Capstone\\DataFrames\\df_teams_typeerror.csv')\n\n# Assuming here to use Year_Played and not Year_Recruited\n# Change it in line below if needed\nyear_col = 'Year_Played'\nstars_counts = df_recruits.groupby(['Team', year_col])['Stars'].value_counts().unstack().fillna(0)\n\n# Add '2' and '3' columns to form '2_3', '4' and '5' columns to form '4_5'\nstars_counts['2_3'] = stars_counts.get(2, 0) + stars_counts.get(3, 0)\nstars_counts['4_5'] = stars_counts.get(4, 0) + stars_counts.get(5, 0)\n\n# Rename the columns to have .Stars\nstars_counts.columns = [f'{col}.Stars' for col in stars_counts.columns]\n\n# Reset index and rename column to do merge later with df_teams\nstars_counts = stars_counts.reset_index(drop=False)\nstars_counts = stars_counts.rename(columns={year_col: 'Year'})\n\n# Merge to df_teams\ndf_teams = pd.merge(df_teams, stars_counts, how='left', on=['Team', 'Year'], suffixes=['_DROP','']).fillna(0)\n\ndrop_cols = [col for col in df_teams.columns if col.endswith('_DROP')]\ndf_teams = df_teams.drop(columns=drop_cols)\n\nSample Output: First 20 rows of 970\nprint(df_teams.head(20).to_string())\n                Team       Conference  Year  Games  Win  Loss  Off.Yards.per.Game  Yards.Per.Game.Allowed  Touchdowns  Touchdowns.Allowed  1.Stars  2.Stars  3.Stars  4.Stars  5.Stars  2_3.Stars  4_5.Stars\n0             Akron               MAC  2013     12    5     7               342.0                   397.0          31                  44      0.0      0.0      0.0      0.0      0.0        0.0        0.0\n1           Alabama               SEC  2013     13   11     2               454.1                   286.5          65                  23      0.0      0.0      0.0      0.0      0.0        0.0        0.0\n2           Arizona            Pac-12  2013     13    8     5               458.5                   401.1          57                  39      0.0      0.0      0.0      0.0      0.0        0.0        0.0\n3      Arizona State           Pac-12  2013     14   10     4               457.3                   372.4          69                  48      0.0      2.0      2.0      0.0      0.0        4.0        0.0\n4          Arkansas               SEC  2013     12    3     9               357.2                   413.4          30                  49      0.0      0.0      0.0      0.0      0.0        0.0        0.0\n5    Arkansas State          Sun Belt  2013     13    8     5               407.8                   413.1          48                  43      0.0      0.0      0.0      0.0      0.0        0.0        0.0\n6   Army West Point   FBS Independent  2013     12    3     9               387.3                   410.8          38                  49      0.0      0.0      0.0      0.0      0.0        0.0        0.0\n7            Auburn               SEC  2013     14   12     2               501.3                   420.7          73                  41      0.0      0.0      0.0      0.0      0.0        0.0        0.0\n8        Ball State               MAC  2013     13   10     3               477.1                   413.8          64                  39      0.0      0.0      0.0      0.0      0.0        0.0        0.0\n9            Baylor            Big 12  2013     13   11     2               618.8                   360.3          91                  41      0.0      0.0      0.0      0.0      0.0        0.0        0.0\n10   Boston College               ACC  2013     13    7     6               367.2                   429.0          43                  48      0.0      0.0      0.0      0.0      0.0        0.0        0.0\n11    Bowling Green               MAC  2013     14   10     4               459.4                   321.4          62                  29      0.0      0.0      0.0      0.0      0.0        0.0        0.0\n12          Buffalo               MAC  2013     13    8     5               394.7                   382.8          51                  40      0.0      0.0      0.0      0.0      0.0        0.0        0.0\n13              BYU   FBS Independent  2013     13    8     5               493.6                   378.8          47                  33      0.0      0.0      0.0      0.0      0.0        0.0        0.0\n14       California            Pac-12  2013     12    1    11               453.6                   529.6          32                  72      0.0      0.0      0.0      0.0      0.0        0.0        0.0\n15  Central Michigan              MAC  2013     12    6     6               341.1                   405.2          35                  43      0.0      0.0      3.0      1.0      0.0        3.0        1.0\n16       Cincinnati               AAC  2013     13    9     4               472.1                   315.6          57                  34      0.0      0.0      0.0      0.0      0.0        0.0        0.0\n17          Clemson               ACC  2013     13   11     2               507.7                   356.7          69                  37      0.0      0.0      0.0      0.0      0.0        0.0        0.0\n18         Colorado            Pac-12  2013     12    4     8               369.9                   468.0          36                  58      0.0      0.0      0.0      0.0      0.0        0.0        0.0\n19             Duke               ACC  2013     14   10     4               426.1                   418.0          60                  48      0.0      0.0      0.0      0.0      0.0        0.0        0.0\n\n"""
save images to folder from DeepFace.find(),"'You can do it as shown below:\nfrom deepface import DeepFace\nimport shutil\n\nimage1 = ""test.jpeg""\ndfs = DeepFace.find(img_path = image1,   \n              db_path = ""faces"",    \n              enforce_detection = False)[0]\nfor df in dfs:\n   img_paths = df[""identity""].values.tolist()\n   for img_path in img_paths:\n      shutil.copyfile(img_path, f""SOME_TARGET_FOLDER/{img_path}"")\n      \n\n', 'I\'m assuming the code has a sample image and a set of images (inside the \'faces\' directory) to compare with using deep face. You need to find and save similar images to the sample image in a new folder.\nThis is how we can modify your solution:\nI\'m using shutil library to copy files from the image path.\nimport os\nfrom shutil import copyfile\nfrom deepface import DeepFace\n\nimage1 = ""test.jpeg""\nresults = DeepFace.find(img_path = image1,   \n              db_path = ""faces"",    \n              enforce_detection = False)[0]\n\n# creating output folder\nsimilar_folder_name = \'similar_folder\'\nos.makedirs(similar_folder_name, exist_ok=True)\n\n#  Iterating through matches to store in folder \nfor index, row in results.iterrows():\n    file_path = row[""identity""]\n    file_name = os.path.basename(file_path)\n    print(f""match found at: filename: {file_path}, cosine_similarity: {row[\'VGG-Face_cosine\']}"")\n    new_file_path = os.path.join(similar_folder_name, file_name)\n    # using shutil to copy image from path to new path\n    copyfile(file_path, new_file_path)\n\n'"
Copy all rows of a pandas Dataframe which have certain bits in column set,"'You can replace your loop with (partially) vectorial code + boolean indexing:\ndf2 = df.loc[(df[\'Header\'].apply(int, args=(2,)) & 15).eq(15)]\n\nOutput:\n   S    Header                                   Data\n1  M  10101111  More & longer string with bit pattern\n4  M  11001111  More & longer string with bit pattern\n\n'"
_NoDefault.no_default in pandas library,"'As far as I know no_default is used as a placeholder when there is no single default value, but rather the default value is context specific.\nFor example, in pandas.DataFrame.where the default value for other depends on the dtype of the input data, it can either be np.nan (e.g. for floats) or pd.NA (e.g. for strings), and so its default value is set to no_default.\nIn the where source code the no_default is used like this:\nif other is lib.no_default:\n    other = self.fill_value\n\nYou can use no_default yourself by importing as follows:\nfrom pandas.api.extensions import no_default\n\nIt appears to be defined here as the sole member of a _NoDefault enum.\n'"
how to add both date and time picker in dash application,"'You can use custom RangeSider with custom mark, step, min and max for it like this:\nhtml.Div(\n    className=\'date-picker-container\',\n    children=[\n        dcc.DatePickerRange(\n            id=\'date-picker\',\n            start_date=df[\'date_time\'].min().date(),\n            end_date=df[\'date_time\'].max().date(),\n            min_date_allowed=df[\'date_time\'].min().date(),\n            max_date_allowed=df[\'date_time\'].max().date(),\n            initial_visible_month=df[\'date_time\'].max().date(),\n            display_format=\'YYYY-MM-DD\',\n        ),\n        dcc.RangeSlider(\n            id=\'time-range-picker\',\n            marks={i: {\'label\': str(i)} for i in range(24)},\n            min=0,\n            max=23,\n            step=1,\n            value=[0, 23]\n        )\n    ]\n)\n\nAnd then add Input(\'time-range-picker\', \'value\') to callback decorator and the query will be like this:\nstart_date = datetime.strptime(start_date, \'%Y-%m-%d\')\nend_date = datetime.strptime(end_date, \'%Y-%m-%d\') + timedelta(days=0)  # Add one day to include end date\n\nstart_time = datetime.combine(start_date, datetime.min.time()) + timedelta(hours=time_range[0])\nend_time = datetime.combine(start_date, datetime.min.time()) + timedelta(hours=time_range[1])\n\nquery = f""SELECT * FROM {table_name} WHERE date_time >= \'{start_time}\' AND date_time < \'{end_time}\'""\n\nSo the final update_card will be :\n@app.callback(\n    [Output(\'card\', \'children\'), Output(\'card1\', \'children\'), Output(\'card2\', \'children\')],\n    [Input(\'table-selection\', \'value\'), Input(\'date-picker\', \'start_date\'), Input(\'date-picker\', \'end_date\'),\n     Input(\'time-range-picker\', \'value\')]\n)\ndef update_card(table_name, start_date, end_date, time_range):\n    if not table_name or not start_date or not end_date:\n        return \'\', \'\', \'\'\n\n    start_date = datetime.strptime(start_date, \'%Y-%m-%d\')\n    end_date = datetime.strptime(end_date, \'%Y-%m-%d\') + timedelta(days=0)  # Add one day to include end date\n\n    start_time = datetime.combine(start_date, datetime.min.time()) + timedelta(hours=time_range[0])\n    end_time = datetime.combine(start_date, datetime.min.time()) + timedelta(hours=time_range[1])\n\n    query = f""SELECT * FROM {table_name} WHERE date_time >= \'{start_time}\' AND date_time < \'{end_time}\'""\n    cursor.execute(query)\n    # Rest of your code...\n\n\n'"
"Why is the solution to multiply rather than dividing by 10,000?","'Karl\'s answer is the correct one, but to add some meat onto the bones, with a dataframe you can play around with:\nimport pandas as pd\n\n# in the example dataframe, California has a population of 10,000 people, and 20 homeless individuals, so we know the rate per 10,000 people must be 20\nhomelessness = pd.DataFrame({\n    ""state"": [""CA""],\n    ""state_pop"" : [10000],\n    ""individuals"": [20]\n    })\n\n# this is the correct answer, because the numerator is multiplied by 10,000\n10000 * homelessness[\'individuals\'] / homelessness[\'state_pop\'] # 20\n\n# this is the wrong answer, because you are multiplying the denominator instead\nhomelessness[\'individuals\'] / (homelessness[\'state_pop\'] * 10000) # 2.000000e-07 (a.k.a 0.0000002)\n\nFor more information, here\'s a video on Khan Academy on rates.\n', 'homelessness[\'individuals\'] / homelessness[\'state_pop\'] is the proportion of homeless individuals in your state population.\nIt is a value between 0 and 1 (inclusive).\nYou can multiply this proportion by any number to find the proportion per that number.\nE.g. You could multiply by 100 to get percent.\nBut you\'ve been asked to get homeless individuals per 10k, so you need to multiply by 10,000.\n\nBecause of Order of Operations,\n10000 * homelessness[\'individuals\'] / homelessness[\'state_pop\']\nis the same as\n(homelessness[\'individuals\'] / homelessness[\'state_pop\']) * 10000\nbut is NOT the same as\nhomelessness[\'individuals\'] / (homelessness[\'state_pop\'] * 10000)\nIn this final sum, we\'re multiplying the population by 10000, rather than the proportion of homeless individuals in the state population.\nSo, we would end up with a tiny number between 0 and 1 representing the number of homeless individuals in a much larger population.\n'"
How to calculate sample weights to match population in python,"""Let's assume a, b, and c the weights:\n   Age Gender     Origin Weight\n0   20      M  Caucasian      a\n1   30      F      Asian      b\n2   40      F     Latino      c\n\nYou want M:F = 1:1, thus a = b+c, and 20:30:40 = 1:2:3, thus 30 * a = 20 * b and a * 2 = c.\nYou can solve the first 2 equations, giving:\nb = 2/3 * a\nc = a / 3\n\nWhich is incompatible with the third equation (a * 2 = c, unless a = b = c = 0) and I'm not even considering here the conditions on Origin.\nSo in summary, what your want is impossible.\n"", ""First, define the sample population ratios\ngender_ratio = 1 # the ration is 1:1\nage_ratio = {'20': 1, '30': 2, '40': 3} # 1:2:3\norigin_ratio = {'Caucasian': 1, 'Asian': 4, 'Latino': 2} #1:4:2\n\nThen, calculate the weight from the sample population and add it as a new column to the dt:\ndt['Weight'] = dt.apply(lambda row: gender_ratio * age_ratio[str(row['Age'])] * origin_ratio[row['Origin']], axis=1)\n\n"""
Minimum of a timestamp in a pandas dataframe at a row level,"""I am a bit confused by your approach. I would do the following\nimport pandas as pd\n\ndf = pd.DataFrame(\n    {\n           ""key1"" : ['2023-06-05 09:30:21', '2023-05-24 19:40:56', None, None],\n           ""key2"" : ['value1', 'value2',0,0]\n    }\n)\n\n#convert to datetime and get the min value\nr = pd.to_datetime(df.key1).min()\n\nprint(r)\n#2023-05-24 19:40:56\n\n"""
How to track changes of two non-similar dataframes?,"'You can mask the common values, then stack+groupby in a dictionary comprehension:\nout = (  {k: ""deleted"" for k in df1.index.difference(df2.index)}\n       | {k: g.droplevel(0).to_dict() for k, g in\n          df2.where(df1.ne(df2)).stack().groupby(level=0)}\n      )\n\nOutput:\n{\'b\': \'deleted\',\n \'c\': {\'col2\': 3.0},\n \'d\': {\'col2\': 5.0},\n \'e\': {\'col1\': \'xxx\', \'col2\': 6.0}}\n\nkeeping None\nout = (  {k: ""deleted"" for k in df1.index.difference(df2.index)}\n       | {k: {k2: v2 for k2, v2 in zip(g.droplevel(0).index, g)\n             if v2 is None or pd.notna(v2)} for k, g in\n          df2.mask(df1.notna()&df2.isna(), None)\n             .where(df1.ne(df2)).dropna(how=\'all\')\n             .stack(dropna=False).groupby(level=0)}\n      )\n\nOutput:\n{\'b\': \'deleted\',\n \'c\': {\'col1\': None, \'col2\': 3.0},\n \'d\': {\'col2\': 5.0},\n \'e\': {\'col1\': \'xxx\', \'col2\': 6.0}}\n\n', '\nFirst, you will need to allign your DataFrames:\n\n df1, df2 = df1.align(df2, fill_value=np.nan)\n\nCompare your Dataframes:\n\n comp = df1.compare(df2)\n\nProcess the comparison\n\nres = {}\nfor idx in set(df1.index).union(df2.index):\n    if idx not in df2.index:\n        res[idx] = ""deleted""\n    elif idx not in df1.index:\n        res[idx] = {col: df2.loc[idx, col] for col in df2.columns}\n    elif idx in comp.index:\n        diff = comp.loc[idx]\n        res[idx] = {col: df2.loc[idx, col] for col in diff.index.get_level_values(0)}\n\nHere is entire code:\nimport pandas as pd\nimport numpy as np\n\ndf1 = pd.DataFrame({\'col1\': {\'a\': \'foo\', \'b\': \'qux\', \'c\': \'bar\', \'d\': \'baz\'},\n \'col2\': {\'a\': np.nan, \'b\': 2.0, \'c\': np.nan, \'d\': 4.0}})\n\ndf2 = pd.DataFrame({\'col1\': {\'a\': \'foo\', \'c\': np.nan, \'d\': \'baz\', \'e\': \'xxx\'},\n \'col2\': {\'a\': np.nan, \'c\': 3.0, \'d\': 5.0, \'e\': 6.0}})\n\n# Step 1\ndf1, df2 = df1.align(df2, fill_value=np.nan)\n\n# Step 2\ncomp = df1.compare(df2)\n\n# Step 3\nres = {}\nfor idx in set(df1.index).union(df2.index):\n    if idx not in df2.index:\n        res[idx] = ""deleted""\n    elif idx not in df1.index:\n        res[idx] = {col: df2.loc[idx, col] for col in df2.columns}\n    elif idx in comp.index:\n        diff = comp.loc[idx]\n        res[idx] = {col: df2.loc[idx, col] for col in diff.index.get_level_values(0)}\n\nprint(res)\n\n'"
Pandas update Dataframes while keep some columns as the old one,"""I figure out this method:\ndf_merge = df_old.merge(df_new, on=['name', 'datetime'], how='right')\n\ndf_merge['value'] = np.where(df_merge['value_x'].isna(),\n                             df_merge['value_y'],\n                             df_merge['value_x'])\n\ndf_merge = df_merge.drop(columns=['value_x', 'value_y'])\n\n"", ""This seems to produce the desired behaviour:\nmerged_df = df_old.merge(df_new, on='name', how='right')\n\nfilter1 = merged_df['datetime_y'].notnull()\nfilter2 = merged_df['value_x'].notnull()\nfilter3 = merged_df['datetime_x'] == merged_df['datetime_y']\n\nmerged_df['datetime'] = merged_df['datetime_y'].where(filter1, merged_df['datetime_x'])\n\nmerged_df['value'] = merged_df['value_x'].where(filter2 & filter3, merged_df['value_y']).astype('int')\n\nmerged_df = merged_df.drop(columns=['datetime_x', 'datetime_y', 'value_x', 'value_y'])\n\n"""
Fill NaN values in Polars using a custom-defined function for a specific column,"""The issue here is that in Polars .apply defaults to skip_nulls=True\ndf_polars.with_columns(\n   pl.col('A').apply(lambda me: print(f'{me=}'))\n)\n\nme=1\nme=2\nme=4\n\nAs your example specifically needs to target the nulls, you need to change this to False\ndf_polars.with_columns(\n   pl.col('A').apply(lambda me: print(f'{me=}'), skip_nulls=False)\n)\n\nme=1\nme=2\nme=None\nme=None\nme=None\nme=None\nme=4\nme=None\n\n"""
Make PandasAI retain context,"'\nyou can implement this use langchain https://github.com/hwchase17/langchain.\nsince pandasai is opensource, you can check and modify the code to implement this\n\n'"
Filtering dataframes in pandas : use a list of conditions,"'This is a pretty clean solution if you have identical join ops (& or | for all filters:\ncols = [\'col1\', \'col2\']\nconditions = [\'foo\', \'bar\']\n\nfiltered_rows = True\nfor col, condition in zip(cols, conditions):\n    # update filtered_rows with each filter condition\n    current_filter = (df[col] == condition)\n    filtered_rows &= current_filter\n\ndf = df[filtered_rows]\n\n', ""I would like to point out an alternative for the accepted answer as eval is not necessary for solving this problem.\nfrom functools import reduce\n\ndf = pd.DataFrame({'col1': ['foo', 'bar', 'baz'], 'col2': ['bar', 'spam', 'ham']})\ncols = ['col1', 'col2']\nvalues = ['foo', 'bar']\nconditions = zip(cols, values)\n\ndef apply_conditions(df, conditions):\n    assert len(conditions) > 0\n    comps = [df[c] == v for c, v in conditions]\n    result = comps[0]\n    for comp in comps[1:]:\n        result &= comp\n    return result\n\ndef apply_conditions(df, conditions):\n    assert len(conditions) > 0\n    comps = [df[c] == v for c, v in conditions]\n    return reduce(lambda c1, c2: c1 & c2, comps[1:], comps[0])\n\ndf[apply_conditions(df, conditions)]\n\n"", ""Posting because I ran into a similar issue and found a solution that gets it done in one line albeit a bit inefficiently\ncols, vals = [""col1"",""col2""],['foo','bar']\npd.concat([df.loc[df[cols[i]] == vals[i]] for i in range(len(cols))], join='inner')\n\nThis is effectively an & across the columns. To have an | across the columns you can ommit join='inner' and add a drop_duplicates() at the end\n"", 'I know I\'m late to the party on this one, but if you know that all of your values will use the same sign, then you could use functools.reduce. I have a CSV with something like 64 columns, and I have no desire whatsoever to copy and paste them. This is how I resolved:\n\nfrom functools import reduce\n\nplayers = pd.read_csv(\'players.csv\')\n\n# I only want players who have any of the outfield stats over 0.\n# That means they have to be an outfielder.\ncolumn_named_outfield = lambda x: x.startswith(\'outfield\')\n\n# If a column name starts with outfield, then it is an outfield stat. \n# So only include those columns\noutfield_columns = filter(column_named_outfield, players.columns)\n\n# Column must have a positive value\nhas_positive_value = lambda c:players[c] > 0\n# We\'re looking to create a series of filters, so use ""map""\nlist_of_positive_outfield_columns = map(has_positive_value, outfield_columns)\n\n# Given two DF filters, this returns a third representing the ""or"" condition.\nconcat_or = lambda x, y: x | y\n# Apply the filters through reduce to create a primary filter\nis_outfielder_filter = reduce(concat_or, list_of_positive_outfield_columns)\noutfielders = players[is_outfielder_filter]\n\n', 'To the best of my knowledge, there is no way in Pandas for you to do what you want.  However, although the following solution may not me the most pretty, you can zip a set of parallel lists as follows:\n\ncols = [\'col1\', \'col2\']\nconditions = [\'foo\', \'bar\']\n\ndf[eval("" & "".join([""(df[\'{0}\'] == \'{1}\')"".format(col, cond) \n   for col, cond in zip(cols, conditions)]))]\n\n\nThe string join results in the following:\n\n>>> "" & "".join([""(df[\'{0}\'] == \'{1}\')"".format(col, cond) \n    for col, cond in zip(cols, conditions)])\n\n""(df[\'col1\'] == \'foo\') & (df[\'col2\'] == \'bar\')""\n\n\nWhich you then use eval to evaluate, effectively:\n\ndf[eval(""(df[\'col1\'] == \'foo\') & (df[\'col2\'] == \'bar\')"")]\n\n\nFor example:\n\ndf = pd.DataFrame({\'col1\': [\'foo\', \'bar, \'baz\'], \'col2\': [\'bar\', \'spam\', \'ham\']})\n\n>>> df\n  col1  col2\n0  foo   bar\n1  bar  spam\n2  baz   ham\n\n>>> df[eval("" & "".join([""(df[\'{0}\'] == {1})"".format(col, repr(cond)) \n            for col, cond in zip(cols, conditions)]))]\n  col1 col2\n0  foo  bar\n\n'"
strip() on multiple column not working - pandas,"'One idea is use DataFrame.apply with Series.str.strip, but your solution should working well:\ndf[cols_to_strip] = df[cols_to_strip].apply(lambda x: x.str.strip())\n\nEDIT: Is possible test ouput of your function?\nnhl_df = cleanse_data(nhl_df,[\'team\'])\nprint(nhl_df.loc[nhl_df[\'team\'].str.contains(\'Jose Sharks\'), \'team\'].tolist()) \n\n'"
Pandas: Merge Dataframes Based on Condition but Keep NaN,"""Try this:\nidx = pd.IntervalIndex.from_arrays(df2['startdate'],df2['enddate'])\ndf['value'] = pd.Series(df2['value'].tolist(),index = idx).reindex(df['triggerdate']).tolist()\n\nOld answer:\nTry pd.merge_asof()\nmodify df2 to merge on date column\ndf2 = (pd.concat([df2,\ndf2[['id','enddate']]\n.rename({'enddate':'startdate'},axis=1)]))\n\nThen merge\n(pd.merge_asof(df.reset_index().sort_values('triggerdate'),\ndf2.sort_values('startdate'),\nleft_on = 'triggerdate',\nright_on = 'startdate',\nby = 'id')\n.sort_values('index')\n.drop('index',axis=1))\n\nOutput:\n  id triggerdate  startdate    enddate  value\n4  a  2022-09-01 2022-08-30 2022-09-03   30.0\n3  a  2022-08-15        NaT        NaT    NaN\n0  b  2022-06-25        NaT        NaT    NaN\n1  c  2022-06-30 2022-06-28 2022-07-05   10.0\n2  c  2022-07-01 2022-06-28 2022-07-05   10.0\n\n"""
Casting to unit-less dtype &#39;datetime64&#39; is not supported,"""I have the same problem. Solved it by downgrading to pandas 1.5.3 version or any version earlier than 2.0.0 . If you are on pandas version 1.5.3, you should see a depreciating warning on this but your code will work. Hope this helps.\npip install pandas==1.5.3\n\nIf you want to make it work on latest pandas version or any version from 2.0.0 onwards, try update the schema to something like this:\n{'timestamp': 'datetime64[ns]'}\n\n"", ""You have to specify the unit for internal storage ('ns', 'ms', 's', ...):\nimport pandas as pd\nimport numpy as np\n\n#                           HERE --v\nschema = {'timestamp': 'datetime64[ns]', 'instrument_token': int, 'last_price': float, 'volume': int}\ndata = pd.DataFrame(columns=schema.keys()).astype(schema)\n\nOutput:\n>>> data.dtypes\ntimestamp           datetime64[ns]\ninstrument_token             int64\nlast_price                 float64\nvolume                       int64\ndtype: object\n\n"""
"With Python, move values from a actual column to the end of a previous column under certain conditions","'Probably not the best code, but worked for my case with 500+ columns and 100 rows.\n# Assign column names to transposed DataFrame\ndf.columns = column_names\n\n# Get the total number of columns\nnum_columns = len(df.columns)\n\n# Iterate through each column\ncurrent_column = 0\nwhile current_column < num_columns:\n    # get the name of the current column\n    current_column_name = df.columns[current_column]\n    # get the name of the previous column\n    previous_column = df.columns[current_column - 1]\n    # Check if the first entry in the current column is NaN\n    if pd.isnull(df.iloc[0, current_column]):\n        # Get the value of the first cell in the actual column\n        first_cell = df[previous_column].iloc[0]\n        # Count the amount of vaules in the actual column\n        count = df[current_column_name].count()\n        # Shifts the previous column down 1 position\n        df[previous_column] = df[previous_column].shift(count-1)\n        # add the values of the current column to the previous column\n        df[previous_column] = df[current_column_name].combine_first(df[previous_column].shift())\n        # add the first value back to the previous column\n        df.at[0, previous_column] = first_cell\n        # Delete the current column\n        df.drop(df.columns[current_column], axis=1, inplace=True)\n        # Update the number of columns after deletion\n        num_columns -= 1 \n    else:\n        current_column += 1\n´´´\n\n', 'You can use lreshape with a dummy column for Column A (the number of columns must be identical) and dropna=False, then dropna to remove rows with all NaN:\nout = (pd.lreshape(df.assign(dummy=np.nan),\n                   {\'Column A\': [\'Column A\', \'Column B\', \'dummy\'],\n                    \'Column C\': [\'Column C\', \'Column D\', \'Column E\']},\n                   dropna=False\n                   )\n         .dropna(how=\'all\').reset_index(drop=True)\n      )\n\nNB. if you remove .dropna(how=\'all\') you will have the intermediate DataFrame with NaN rows.\nOr with stack, and concat:\ncols = [[\'Column A\', \'Column B\'], [\'Column C\', \'Column D\', \'Column E\']]\n\nout = pd.concat({c[0]: df[c].stack().reset_index(drop=True)\n                 for c in cols}, axis=1)\n\nOutput:\n  Column A Column C\n0      100      300\n1     R100     R300\n2     R200     R400\n3      NaN     R500\n\n'"
How to iterate functions over rows in a DataFrame in Python,"""Maybe you can use something like:\ndef download(x, y, z):\n    # download archive\n    ...\n    return result\n\nfor n, row in df.iterrows():\n    X, Y, Z = row[['X', 'Y', 'Z']]\n    result = download(X, Y, Z)\n    # process result\n    ...\n\n"""
How to compare rows of specific columns of two dataframes and in the event of equalness replace with them with specific values?,"'import pandas as pd\ndf1 = pd.DataFrame({\'ID\': [1, 2, 3], \'VALUES\': [4, 6, 7]})\ndf2 = pd.DataFrame({\'NAME\': [7, 8, 9, 10], \'VALUES\': [4, 6, 6, 7]})\ndf_out = df2.merge(df1,on=[\'VALUES\'],how=\'left\')\ndf_out[\'VALUES\'] = df_out[\'ID\']\ndf_out[[\'NAME\',\'VALUES\']]\n\nPlease try this ,\nit gives me below result\n\n'"
AttributeError: &#39;ParquetFile&#39; object has no attribute &#39;row_groups&#39;,"'Indeed, one of the causes of the issue appears to be dependent on incorrect file access path. Providing correct path solves it.\nThe other one seems to depend on mismatch between pyarrow and fastparquet load/save versions. This behavior however is not consistent (or I was not able to pin-point it across different versions) and depends on the version of the packages (very annoying). For example: at some point save had to occur with pyarrow and load with fastparquet. At some point, after package updates, both load and save started relying on the same version. Strange.\nAdditionally, I found there seems to be a GitHub issues that refers to the same issue, but different root-cause. Either way, if the path is correct, the update of the packages to ""matching"" versions should solve the issue.\n', 'I am suffering similar issue with you. It can be possible to have file path issue. In my case, when i tried to access S3 file, happened that error. and this is permission issue to access S3.\n'"
Finding duplicates across multiple sheets in an excel corresponding to the first column using python,"""Assuming df1/df2, you can align df1 on df2 and mask the cells that are different, except in Date/Place:\ntmp = df2.set_index('Date/Place')\n\nout = tmp.mask(tmp.eq(df1.set_index('Date/Place')), '@').reset_index()\n\nOr:\nout = df2.mask(\n df2[['Date/Place']].merge(df1, how='left')\n .set_axis(df2.index)\n .eq(df2).assign(**{'Date/Place': False}),\n '@'\n )\n\nOutput:\n   Date/Place PlaceE PlaceF PlaceG PlaceH PlaceI\n0  2019-03-05      @      @      U      A      B\n1  2019-03-06      X      P      @      N      H\n\n"""
How to simplify &quot;Percentage for each class&quot;,"'Very simple method, as you already have 0/1, just get the groupby.mean:\nout = (fr1.groupby(\'class\')[\'survived\']\n          .mean().mul(100).reset_index()\n       )\n\nVariant:\nout = (fr1.groupby(\'class\', as_index=False)[\'survived\']\n          .agg(lambda g: g.mean()*100)\n       )\n\nOutput:\n  class  survived\n0     1       0.0\n1     2     100.0\n\n'"
"How to interpolate monthly frequency sample data&#39;s missing values with interp1d(x, y) from scipy","'You can interpolate the values using the seconds from some reference time (below I used the first date) as shown in this answer. I can\'t guarantee the accuracy of these results since there is a lot of missing data to interpolate.\nimport pandas as pd\nimport numpy as np\nfrom scipy.interpolate import interp1d\n\ndata = pd.DataFrame({\n    ""value"": [1.0, 1.2, np.nan, 1.4, 1.6, np.nan, 1.8, 2.0, np.nan, 2.2, 2.4, np.nan]\n}, index=pd.date_range(""2000-01-01"", periods=12, freq=""M""))\n\ndata.index = pd.to_datetime(data.index)\nmask = ~np.isnan(data[""value""])     # mask out the missing values\n\ndref = data.index[0]\n\nx = (data.index-dref).total_seconds()[mask]\ny = data[""value""][mask].to_numpy()\n\nf = interp1d(x, y, fill_value=""extrapolate"")\ny_new = f((data.index - dref).total_seconds())\n\ndata[""value_interpolated""] = y_new\n\nOut:\n            value  value_interpolated\n2000-01-31    1.0            1.000000\n2000-02-29    1.2            1.200000\n2000-03-31    NaN            1.301639\n2000-04-30    1.4            1.400000\n2000-05-31    1.6            1.600000\n2000-06-30    NaN            1.698361\n2000-07-31    1.8            1.800000\n2000-08-31    2.0            2.000000\n2000-09-30    NaN            2.098361\n2000-10-31    2.2            2.200000\n2000-11-30    2.4            2.400000\n2000-12-31    NaN            2.606667\n\n'"
"When using np.where on a pandas datetime column, an &quot;object&quot; dtype value is returned. Is this expected behavior?","'The numpy datetime data type seems incompatible with the pandas one, so it will convert the data to an object. I recommend using the pandas where method instead.\nimport pandas as pd\n\nexample_df = pd.DataFrame({\'animal\': [\'Falcon\', \'Falcon\', \'Parrot\', \'Parrot\'],\n                           \'time\': [pd.to_datetime(\'2023-01-01 23:59:58\'), \n                                    pd.to_datetime(\'2023-01-01 23:59:58\'), \n                                    pd.to_datetime(\'2023-01-01 23:59:58\'), \n                                    pd.to_datetime(\'2023-01-01 23:59:58\')]})\nexample_df = example_df.assign(same_time=example_df[""time""].where(~example_df[""time""].isna(), \n                                                                  pd.to_datetime(\'1900-01-01 00:00:00\')))\n\nWith that, example_df becomes:\n   animal                time           same_time\n0  Falcon 2023-01-01 23:59:58 2023-01-01 23:59:58\n1  Falcon 2023-01-01 23:59:58 2023-01-01 23:59:58\n2  Parrot 2023-01-01 23:59:58 2023-01-01 23:59:58\n3  Parrot 2023-01-01 23:59:58 2023-01-01 23:59:58\n\n'"
"ERROR: Could not build wheels for hnswlib, which is required to install pyproject.toml-based projects","'if you using windows, try set HNSWLIB_NO_NATIVE=1\n', 'Downgraded Python to 3.10.10\nexport HNSWLIB_NO_NATIVE=1 \npip install HNSWLIB\n\n', 'Ok so I also encoutered this problem when installing lang chain package to my Windows devices. I have download microsoft visual studio before, and I figured that you can just run the microsoft visual studio before you install, it will then work.\n', 'In Mac, install GCC. It should work.\nbrew info gcc\nbrew install gcc\n\n', 'None of these solutions worked for me, what did work for me in the end was using Anaconda to do the build rather than VisualStudio.\n', 'In Ubuntu, these commands solved this problem, perhaps a Mac alternative could be useful too:\nsudo apt install python3-dev\nsudo apt-get install build-essential -y\n\n', 'I have faced similar issue on Windows OS, while doing pip install chromadb==0.3.22.\nTo Resolve this issue,\nYou need to download https://visualstudio.microsoft.com/visual-cpp-build-tools/ first.\nNext, navigate to ""Individual components"", find these two\n\nand Windows 10 SDK\n\nsource: https://github.com/chroma-core/chroma/issues/189#issuecomment-1454418844\n', 'This worked for me\nInstall Microsoft Visual C++ Build Tools on your system. You can download and install it from the official Microsoft website: https://visualstudio.microsoft.com/visual-cpp-build-tools/.\n', 'Clang compiler is not found.\ncheck c/c++ version clang -v. if not found\nsudo xcode-select --install\n\n', 'Try setting an environment variable:\nexport HNSWLIB_NO_NATIVE=1  \n\nAnd then retry...\n', 'clang: error: the clang compiler does not support \'-march=native\'\n\nerror: command \'/usr/bin/clang\' failed with exit code 1\n\nIt looks like the problem is caused by the clang compiler, which did not support the flag -march=native previously.\nThe following commit added support of the flag, and you may need to install the latest Xcode for it.\nhttps://github.com/llvm/llvm-project/commit/fcca10c69aaab539962d10fcc59a5f074b73b0de\n'"
TypeError: unsupported format string passed to Series.__format__,"'Another approach here would be to use .applymap()\nFor readability, we can create a function to format the numbers, and then use the function in the .applymap().\ndef format_ints(row):\n    return f""{row:,}""\n\ndf[[""A"", ""B"", ""C""]] = df[[""A"", ""B"", ""C""]].applymap(format_ints)\n\nDocumentation reference\n', 'Short abstracted example code that reproduces your problem:\nimport pandas as pd\n\ndf = pd.DataFrame(\n    {\n        ""A"": [1000000.111, 1000000.222, 1000000.333],\n        ""B"": [2000000.111, 2000000.222, 2000000.333],\n        ""C"": [3000000.111, 3000000.222, 3000000.333],\n    }\n)\n\ndf[[""A"", ""B"", ""C""]] = df[[""A"", ""B"", ""C""]].apply(lambda x: round(x, 2))\ndf[[""A"", ""B"", ""C""]] = df[[""A"", ""B"", ""C""]].apply(lambda x: f\'{x:,}\')\n\ngives:\n\nTypeError: unsupported format string passed to Series.format\n\nThe problem is with the last line where you try to format the numbers.\nThe problem is that x in the lambda function refers to a Pandas Series and not a number. A Series doesn\'t support the type of formatting string you\'re using:\n\nThe \',\' option signals the use of a comma for a thousands separator.\n\nhttps://docs.python.org/3/library/string.html\n\nInstead you could do something like this:\nimport pandas as pd\n\ndf = pd.DataFrame(\n    {\n        ""A"": [1000000.111, 1000000.222, 1000000.333],\n        ""B"": [2000000.111, 2000000.222, 2000000.333],\n        ""C"": [3000000.111, 3000000.222, 3000000.333],\n    }\n)\n\ndf[[""A"", ""B"", ""C""]] = df[[""A"", ""B"", ""C""]].apply(lambda x: round(x, 2))\ndf[[""A"", ""B"", ""C""]] = df[[""A"", ""B"", ""C""]].apply(\n    lambda series: series.apply(lambda value: f""{value:,}"")\n)\n\nSo the idea behind the nested apply here is: For each column in the DataFrame df[[""A"", ""B"", ""C""]] take each row value and apply the string formatter to it. The row values are just floats so the string formatter is able to handle that.\nResult\n>>> print(df)\n              A             B             C\n0  1,000,000.11  2,000,000.11  3,000,000.11\n1  1,000,000.22  2,000,000.22  3,000,000.22\n2  1,000,000.33  2,000,000.33  3,000,000.33\n\nAlternatively you can set the format using pd.options.display.float_format:\npd.options.display.float_format = ""{:,}"".format\n\nKeep in mind that this is applies to more than just df[[""A"", ""B"", ""C""]].\n'"
how to convert object dtype to inter dtype in pandas?,"""Use df.astype(float).astype(int)\nimport pandas as pd\n\ndata = [\n    ['123.45', '456'],\n    ['234.56', '789']\n]\ndf = pd.DataFrame(data, columns=['a', 'b'])\nassert (df.dtypes == object).all()\n\ndf2 = df.astype(float).astype(int)\nassert (df2.dtypes == int).all()\n\nprint(df2)\n\n     a    b\n0  123  456\n1  234  789\n\nThat truncates the floats to remove the fractional part.  If you want to round to the nearest integer, use round():\ndf3 = df.astype(float).round().astype(int)\nprint(df3)\n\n     a    b\n0  123  456\n1  235  789\n\n"""
Y-Axis format to HH:mm from seconds,"'I could not find a direct, equivalent solution for seaborn users, so I wanted to post what I piecemealed together after several SO searches:\nimport pandas as pd\nimport seaborn as sns\nimport datetime\nfrom matplotlib import ticker as tkr\nfrom matplotlib import pyplot as plt\n\n#Sample dataframe\ndata = {\n        \'category\': [1,2,3],\n        \'time\': [915,2710,2000]\n       }\ndf = pd.DataFrame.from_dict(data)\n\ng = sns.catplot(data=df, x=\'category\', y=\'time\', kind=""bar"")\n\n#format y-axis\nfmt = tkr.FuncFormatter(lambda x, pos: str(datetime.timedelta(seconds=x)))\nfor ax in g.axes.flat:\n    ax.yaxis.set_major_locator(tkr.MultipleLocator(900)) #15-minute (900s) step\n    ax.yaxis.set_major_formatter(fmt) #convert s to hh:mm:ss\n\nplt.show()\n\nseaborn chart\nThe key here being for ax in g.axes.flat, which allows the seaborn user to utilize pyplot\'s yaxis methods. See How do i increase seaborn Y axis step.\n', 'Problem is solved! Thank you very much!!! I dont know the difference between scatter and plot but i changed the line style to ""o"" and it is working for me.\n', 'Try this code:\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import dates\n\n# fake data\narr_data = range(0, 4)\narr_secs = [52000, 53000, 54000, 55000]\n\n# format seconds to date with this format \'%H:%M:%S\'\narr__secs_formatted = list(map(datetime.datetime.strptime, map(lambda s: time.strftime(\'%H:%M:%S\', time.gmtime(s)), arr_secs), len(arr_secs)*[\'%H:%M:%S\']))\n\nfig, ax = plt.subplots()\nax.plot(arr_data, arr_secs_formatted, \'ro-\')\n\n# set your date formatter whit this format \'%H:%M\'\nformatter = dates.DateFormatter(\'%H:%M\')\nax.yaxis.set_major_formatter(formatter)\n\nplt.show()\n\n\n\n', ""try this\n\n plt.yticks(ylim, [str(n).zfill(2) + ':00' for n in np.arange(0, 24, 1)])\n\n"""
Delete random columns that contain a specific value in Pandas,"""I think I found where the problem is, although it wasn't too easy.\nIn this snippet from your question look at when you're appending your new_df to the final list html_file.append(new_df.to_html()):\ndef create_html_files(event_dict):\n    html_file = []\n    count = 0\n    #Turn rows into columns\n    for items in list(event_dict):\n        df = pd.DataFrame(items)\n        new_df = df.transpose()\n        new_df.columns = ['Timestamp','Username','Host','FilePath','AccountName']\n    \n        #Delete columns whose row values are -\n        for column in new_df.columns:\n        # Check if the specific value is present in the column\n            if new_df[column].eq(' - ').any():\n                # Delete the column\n                new_df.drop(column, axis=1, inplace=True)\n\n                html_file.append(new_df.to_html())\n                count +=1\n                with open(f'event{count}.html','w') as wf:\n                    wf.write(html_file[-1])\n    return html_file\ncreate_html_files(event_dict)\n\nIt happens for each df each time the condition if new_df[column].eq(' - ').any() passes, i. e. each time you've found a column with "" - "". In some of your input examples there are multiple such columns - therefore the strange number of files.\nInstead, write the final version of new_df only once after checking all the columns\ndef create_html_files(event_dict):\n    html_file = []\n    count = 0\n    #Turn rows into columns\n    for items in list(event_dict):\n        df = pd.DataFrame(items)\n        new_df = df.transpose()\n        new_df.columns = ['Timestamp','Username','Host','FilePath','AccountName']\n    \n        #Delete columns whose row values are -\n        for column in new_df.columns:\n        # Check if the specific value is present in the column\n            if new_df[column].eq(' - ').any():\n                # Delete the column\n                new_df.drop(column, axis=1, inplace=True)\n\n        # CORRECTED INDENTATION STARTS HERE\n        html_file.append(new_df.to_html())\n        count +=1\n        with open(f'event{count}.html','w') as wf:\n            wf.write(html_file[-1])\n    return html_file\ncreate_html_files(event_dict)\n\n"""
Remove Azure Table Storage EntityProperty from TableClient.query_entities() Results Before Writing to Pandas DataFrame in Python,"'Please try by adding the following header to your query:\nheaders = {""Accept"" : ""application/json;odata=nometadata""}\n\nSo your code would be something like:\nheaders = {""Accept"" : ""application/json;odata=nometadata""}\nfor entity in table_client.query_entities(query_filter, headers=headers):\n\nYou can learn more about this here: https://learn.microsoft.com/en-us/rest/api/storageservices/payload-format-for-table-service-operations#json-format-applicationjson-versions-2013-08-15-and-later.\n'"
How to simplify &quot;Get mean ages for women of every class&quot;,"'You can filter the females, then pandas groupby method and apply aggregation function to each group (=for each class):\nfr1 = pd.DataFrame({\n    ""class"": [""1"", ""2"", ""2""],\n    ""gender"": [""female"", ""female"", ""female""],\n    ""age"": [30, 25, 18]\n})\n\nfr1[fr1[""gender""] == ""female""] \\\n    .groupby(""class"", as_index=False)[""age""] \\\n    .agg(lambda x: round(np.mean(x)))\n\nOutput:\n  class  age\n0     1   30\n1     2   22\n\nIt could be even simpler, if you skipped the rounding part.\nfr1[fr1[""gender""] == ""female""] \\\n    .groupby(""class"", as_index=False)[""age""] \\\n    .mean()\n\n'"
Pandas infer_objects() doesn&#39;t convert string columns to numeric,"'Inferring types has lots of edge cases. There is an open bug covering this issue.\nIn the meantime, a workaround could be to use the type inference in the CSV reader, which is quite good, and what most people arriving at this page were expecting. So, until someone refactors that code out from the reader into a more generic solution, this hack might get many people moving:\n# given a Pandas DataFrame df, createa a CSV in memory and read it back in:\n\nimport io\n\nstream = io.StringIO()\ndf.to_csv(stream, index=False)\nstream.seek(0)\ndf = pd.read_csv(stream)\nstream.close()\n\nWith all the usual caveats around speed and opening yourself up to other type conversion bugs.\n', 'A little late for the show, but sharing my findings.\nFor me also, infer_objects() did not give the results I was aiming for.\nWhat gave me a hint was @stackoverflowuser2010 \'s comment:\n\nThe documentation for infer_objects() says:\n""The inference rules are the same as during normal Series/DataFrame construction.""\nWhenever I run pd.read_csv() to build a new dataframe, that function correctly infers the data types.\n\nSo I figured, I\'ll build the data from csv:\nimport pandas as pd\nfrom io import StringIO\n\ndata = [ [\'Alice\', \'100\', \'1.1\'], [\'Bob\', \'200\', \'2.1\'], [\'Carl\', \'300\', \'3.1\']]\ncolumns=[\'name\', \'value1\', \'value2\']\nas_csv = \'\\n\'.join(\' \'.join(l) for l in data)\ndf = pd.read_csv(StringIO(as_csv), sep=\' \', header=None, names=columns)\n\nprint(df)  \n#    name  value1  value2\n# 0  Alice     100     1.1\n# 1    Bob     200     2.1\n# 2   Carl     300     3.1\n\n\nprint(df.info())  \n# <class \'pandas.core.frame.DataFrame\'>\n# RangeIndex: 3 entries, 0 to 2\n# Data columns (total 3 columns):\n#  #   Column  Non-Null Count  Dtype\n# ---  ------  --------------  -----\n#  0   name    3 non-null      object\n#  1   value1  3 non-null      int64\n#  2   value2  3 non-null      float64\n# dtypes: float64(1), int64(1), object(1)\n# memory usage: 200.0+ bytes\n\n\nI admit, it is not very elegant, but it is the best solution I could find for setting arbitrary columns to their correct data type.\n', ""This function works okay.  Add more string replaces for different currencies.\ndef convert_datatype(df):\n    for column in df.columns:\n        \n        try:\n            df[column] = df[column].str.replace('$','').str.replace('£','').str.replace(',','').astype(float)\n        except (ValueError, TypeError):\n            try:\n                df[column] = df[column].astype(int)\n            except (ValueError, TypeError):\n                df[column] = df[column].astype(str)\n    return df\n\n"", 'df_new = df.convert_dtypes() may help.\nhttps://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.convert_dtypes.html\n', 'Or further to @wwnde same solution slightly different, \n\ndf[""value1""] = pd.to_numeric(df[""value1""])\ndf[""value2""] = pd.to_numeric(df[""value2""])\n\n\nEDIT: \nThis is an interesting question and I\'m surprised that pandas doesn\'t convert obvious string floats and integers as you show. \n\nHowever, this small code can get you through the dataframe and convert your columns. \n\ndata = [[""Alice"", ""100"", ""1.1""], [""Bob"", ""200"", ""2.1""], [""Carl"", ""300"", ""3.1""]]\ndf = pd.DataFrame(data, columns=[""name"", ""value1"", ""value2""])\n\nprint(df.info(), ""\\n"")\n\nRangeIndex: 3 entries, 0 to 2\nData columns (total 3 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   name    3 non-null      object\n 1   value1  3 non-null      object\n 2   value2  3 non-null      object\ndtypes: object(3)\n\ncols = df.columns\nfor c in cols:\n    try:\n        df[c] = pd.to_numeric(df[c])\n    except:\n        pass\n\nprint(df.info())\n\nRangeIndex: 3 entries, 0 to 2\nData columns (total 3 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   name    3 non-null      object \n 1   value1  3 non-null      int64  \n 2   value2  3 non-null      float64\ndtypes: float64(1), int64(1), object(1)\n\n'"
"How to Increase Axis: Pandas: ValueError: Length mismatch: Expected axis has 1 elements, new values have 30 elements","""If you see in your df.head(), your DataFrame has just one column. Also you can see the separator is ',' instead of '|'. You can try the following:\n\ndf=pd.read_csv('verybig.csv', sep=',', header=None, chunksize=1000, low_memory=False)\ndf.columns = ['MyTime','Attribute','Court','Energy','State','30','29','28','27','26','25','24','23','22','21','20','19','18','17','16','15','14','13','12','11','10','9','8','7','6']\n\n\nDoes that work for you?\n"""
Saving pandas dataframe variable to csv from google compute engine to google storage bucket without saving to disk first,"""The above response created an error for me because of the encoding. Here's an alternative:\nfrom google.cloud import storage\nimport os\nfrom io import BytesIO\n\nf = BytesIO()  ## this is to avoid creating local file\ndf.to_csv(f, encoding='utf-8', index=False)  # ensure UTF-8 encoding\nf.seek(0)\n\ngcs = storage.Client()\ngcs.get_bucket('BUCKET_NAME').blob('FILENAME.csv').upload_from_file(f, content_type='text/csv')\n\n"", ""Like this:\n\nfrom google.cloud import storage\nimport os\nfrom io import StringIO\n\nf = StringIO()  ## this is to avoid creating local file\ndf1.to_csv(f)\nf.seek(0)\n\ngcs = storage.Client()\ngcs.get_bucket('google-storge-bucket-1').blob('dataframe1.csv').upload_from_file(f, content_type='text/csv')\n\n"""
ValueError: Mime type rendering requires nbformat&gt;=4.2.0 but it is not installed,"""if your jupyter notebook is being managed by conda, install nbformat with;\nconda install -c conda-forge nbformat\n\nthen restart your kernel.\nelse use the pip package manager to install or upgrade your nbformat\npip install --upgrade nbformat\n\n\ndon't forget to restart the kernel as well.\n"", 'run below command\nstep 1: write in the box\n!pip install nbformat\n\nstep 2: restart the kernel\nwooo, your issue is fixed!!!\n', 'Restarting ipykernel works for me.\n', 'Configurations\n\n\nMac OS\nVS code\npython 3.x\n\n\nMethod\n\n\nEnter the following code:\npip3 install --upgrade nbformat\nRestart the kernel in the VS code prompt\nRun your code again\n\n\nIt should work now.\n', 'Method 1\nreinstall  ipykernel via\npip install ipykernel\n\nMethod 2\npip install --upgrade nbformat\n\n', ""For those that use conda, this worked for me:\nVerify the name of your conda environment:\nconda info --envs\nSupposing it's ""myenv"", proceed:\nconda activate myenv\nconda install nbformat\nThen restart the kernel.\n"", 'I just ran into this problem too. In my case, it turned out I had only installed ipykernel, but it\'s best to install the jupyter metapackage.\nReference: https://code.visualstudio.com/docs/datascience/jupyter-notebooks\n', '!pip install nbformat \n\n\nInstall this.\nRestart your Kernel.\nDam sure it will work!\n\n'"
Add a list of string to each row in a Pandas DataFrame,"""You can use:\ndf['cities'] = np.tile(cities, len(df)).reshape(-1, len(cities)).tolist()\nprint(df)\n\n# Output\n   name            cities\n0  john  [NY, LN, PK, SF]\n1   ray  [NY, LN, PK, SF]\n2  tony  [NY, LN, PK, SF]\n\n"", ""Using apply with axis = 1 can work:\ndata = {'name':['john', 'ray', 'tony']}\ndf= pd.DataFrame(data)\n\ncities = ['NY', 'LN', 'PK', 'SF']\ndf['cities'] = df.apply(lambda x: cities, axis = 1)\n\nOutput:\n    name    cities\n0   john    [NY, LN, PK, SF]\n1   ray     [NY, LN, PK, SF]\n2   tony    [NY, LN, PK, SF]\n\n"", 'If you want every row to contain a list, then you have to assign a column (in your example, \'cities\') a list of lists:\nimport pandas as pd\n\ndata = {\'name\': [\'john\', \'ray\', \'tony\']}\ndf = pd.DataFrame(data)\n\n# add a list to each row\ncities = [\'NY\', \'LN\', \'PK\', \'SF\']\ndf[\'cities\'] = [cities for _ in range(df.shape[0])]\n\n'"
Pandas Dataframe sampling based on multiple custom column category distribution,"'I had to fix your numbers a bit because they didn\'t add up to 100%, but it should work for any data.\nA dataframe in this case is a distribution, when you pick a row from it (with replacement) you effectively sample from said distribution. When the sample size is high enough you start seeing something similar to the original shape. Now your main criterion is:\n\nensuring maximum possible data\n\nI understand it as having any possible combination of values in the final dataset, even if didn\'t exist in the original one.\nimport pandas as pd\n\n#ensure marginal distributions, this can be experimental or synthetic data\nd = {\'religion\': [\'c\']*32 +[\'m\']*53 + [\'h\']*15,\n     \'gender\': [\'m\']*88 + [\'f\']*12,\n     \'education\': [\'hs\']*14 + [\'g\']*33 + [\'pg\']*43 + [\'ps\']*10}\n\ndf = pd.DataFrame(data=d)\nprint(len(df.drop_duplicates().index)) # unique combinations, 7 out of 24 ever possible\n\ndf[\'religion\'] = df[\'religion\'].sample(frac=1, random_state=123).reset_index(drop=True)\ndf[\'gender\'] = df[\'gender\'].sample(frac=1, random_state=456).reset_index(drop=True)\ndf[\'education\'] = df[\'education\'].sample(frac=1, random_state=789).reset_index(drop=True)\nprint(len(df.drop_duplicates().index))\n#increase the number of combinations, 19/24, still could be better\n\ndf = pd.concat([df]*10).reset_index(drop=True)\ndf[\'religion\'] = df[\'religion\'].sample(frac=1, random_state=123).reset_index(drop=True)\ndf[\'gender\'] = df[\'gender\'].sample(frac=1, random_state=456).reset_index(drop=True)\ndf[\'education\'] = df[\'education\'].sample(frac=1, random_state=789).reset_index(drop=True)\nprint(len(df.drop_duplicates().index))\n# blow up the dataset and shuffle every one of them to make the number of combinations as high as possible (24 at most)\n\nprint(df[\'religion\'].value_counts() / len(df.index))\nprint(df[\'gender\'].value_counts() / len(df.index))\nprint(df[\'education\'].value_counts() / len(df.index))\n#original marginal distributions\n\ndf1 = df.sample(10000, replace=True, random_state=100).reset_index(drop=True)\nprint(df1[\'religion\'].value_counts() / len(df1.index))\nprint(df1[\'gender\'].value_counts() / len(df1.index))\nprint(df1[\'education\'].value_counts() / len(df1.index))\n#resulting marginal distributions\n\n'"
Add numpy array as column to Pandas data frame,"""Here is other example:\nimport numpy as np\nimport pandas as pd\n\n"""""" This just creates a list of tuples, and each element of the tuple is an array""""""\na = [ (np.random.randint(1,10,10), np.array([0,1,2,3,4,5,6,7,8,9]))  for i in \nrange(0,10) ]\n\n"""""" Panda DataFrame will allocate each of the arrays , contained as a tuple \nelement , as column""""""\ndf = pd.DataFrame(data =a,columns=['random_num','sequential_num'])\n\nThe secret in general is to allocate the data in the form a = [ (array_11, array_12,...,array_1n),...,(array_m1,array_m2,...,array_mn) ] and panda DataFrame will order the data in n columns of arrays. Of course , arrays of arrays could be used instead of tuples, in that case the form would be :\na = [ [array_11, array_12,...,array_1n],...,[array_m1,array_m2,...,array_mn] ]\nThis is the output if you print(df) from the code above:\n                       random_num                  sequential_num\n0  [7, 9, 2, 2, 5, 3, 5, 3, 1, 4]  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n1  [8, 7, 9, 8, 1, 2, 2, 6, 6, 3]  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n2  [3, 4, 1, 2, 2, 1, 4, 2, 6, 1]  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n3  [3, 1, 1, 1, 6, 2, 8, 6, 7, 9]  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n4  [4, 2, 8, 5, 4, 1, 2, 2, 3, 3]  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n5  [3, 2, 7, 4, 1, 5, 1, 4, 6, 3]  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n6  [5, 7, 3, 9, 7, 8, 4, 1, 3, 1]  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n7  [7, 4, 7, 6, 2, 6, 3, 2, 5, 6]  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n8  [3, 1, 6, 3, 2, 1, 5, 2, 2, 9]  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n9  [7, 2, 3, 9, 5, 5, 8, 6, 9, 8]  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\nOther variation of the example above:\nb = [ (i,""text"",[14, 5,], np.array([0,1,2,3,4,5,6,7,8,9]))  for i in \nrange(0,10) ]\ndf = pd.DataFrame(data=b,columns=['Number','Text','2Elemnt_array','10Element_array'])\n\nOutput of df:\n   Number  Text 2Elemnt_array                 10Element_array\n0       0  text       [14, 5]  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n1       1  text       [14, 5]  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n2       2  text       [14, 5]  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n3       3  text       [14, 5]  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n4       4  text       [14, 5]  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n5       5  text       [14, 5]  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n6       6  text       [14, 5]  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n7       7  text       [14, 5]  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n8       8  text       [14, 5]  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n9       9  text       [14, 5]  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\nIf you want to add other columns of arrays, then:\ndf['3Element_array']=[([1,2,3]),([1,2,3]),([1,2,3]),([1,2,3]),([1,2,3]),([1,2,3]),([1,2,3]),([1,2,3]),([1,2,3]),([1,2,3])]\n\nThe final output of df will be:\n   Number  Text 2Elemnt_array                 10Element_array 3Element_array\n0       0  text       [14, 5]  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]      [1, 2, 3]\n1       1  text       [14, 5]  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]      [1, 2, 3]\n2       2  text       [14, 5]  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]      [1, 2, 3]\n3       3  text       [14, 5]  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]      [1, 2, 3]\n4       4  text       [14, 5]  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]      [1, 2, 3]\n5       5  text       [14, 5]  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]      [1, 2, 3]\n6       6  text       [14, 5]  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]      [1, 2, 3]\n7       7  text       [14, 5]  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]      [1, 2, 3]\n8       8  text       [14, 5]  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]      [1, 2, 3]\n9       9  text       [14, 5]  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]      [1, 2, 3]\n\n"", ""You can add and retrieve a numpy array from dataframe using this:\nimport numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame({'b':range(10)}) # target dataframe\na = np.random.normal(size=(10,2)) # numpy array\ndf['a']=a.tolist() # save array\nnp.array(df['a'].tolist()) # retrieve array\n\nThis builds on the previous answer that confused me because of the sparse part and this works well for a non-sparse numpy arrray.\n"", ""df = pd.DataFrame(np.arange(1,10).reshape(3,3))\ndf['newcol'] = pd.Series(your_2d_numpy_array)\n\n"", 'Consider using a higher dimensional datastructure (a Panel), rather than storing an array in your column:\n\nIn [11]: p = pd.Panel({\'df\': df, \'csc\': csc})\n\nIn [12]: p.df\nOut[12]: \n   0  1  2\n0  1  2  3\n1  4  5  6\n2  7  8  9\n\nIn [13]: p.csc\nOut[13]: \n   0  1  2\n0  0  1  0\n1  0  0  1\n2  1  0  0\n\n\nLook at cross-sections etc, etc, etc.\n\nIn [14]: p.xs(0)\nOut[14]: \n   csc  df\n0    0   1\n1    1   2\n2    0   3\n\n\nSee the docs for more on Panels.\n', ""import numpy as np\nimport pandas as pd\nimport scipy.sparse as sparse\n\ndf = pd.DataFrame(np.arange(1,10).reshape(3,3))\narr = sparse.coo_matrix(([1,1,1], ([0,1,2], [1,2,0])), shape=(3,3))\ndf['newcol'] = arr.toarray().tolist()\nprint(df)\n\n\nyields\n\n   0  1  2     newcol\n0  1  2  3  [0, 1, 0]\n1  4  5  6  [0, 0, 1]\n2  7  8  9  [1, 0, 0]\n\n"""
Pivot table with duplicate vaues with Python Pandas,"""Chat GPT solved it by doing\nend_table = start_table.pivot_table(index='ID', columns='LOCATION', values='STATUS', aggfunc='first').reset_index()\n\n"""
"How to pass variables to ProcessPoolExecutor, got type error","""I'm not sure of the implementation of your function. However, you can use multiprocessing with:\nfrom functools import partial\nfrom multiprocessing import Pool\n\ndef drop_dupplicate_rows(_df,_column,_cuttoff):\n    # your code\n\n# protect the entry point\nif __name__ == '__main__':\n    # Load your dataframe here\n    # df = ...\n\n    CHUNKSIZE=100\n    chunks = (df[i:i+CHUNKSIZE] for i in range(0, len(df), CHUNKSIZE))\n\n    # create the multiprocessing pool (4 CPU)\n    with Pool(4) as pool:\n        partial_func = partial(drop_dupplicate_rows, _column='title', _cuttoff=0.7)\n        out = pool.map(partial_func, chunks)\n\n"""
Best way to take the values of a column that are all related to multiple rows with the same index,"""If df1 and df2 have the same structure and you want merge into large wide df you could give the columns unique name each time a df1, df2... dfn is generated... like this...\nLets say a temporary df is created in the loop each time but in the end you want a large one which contains all data from the others...\nmerged_df = pd.DataFrame()\n\n# go into loop\nfor file in os.listdir(directory_path):\n   temp_df = pd.read_csv(directory_path + '/' + file)\n   temp_df.add_suffix('_unique_string_from_loop')\n   merged_df = pd.concat([merged_df, temp_df], axis=1)\n\nprint(merged_df)\n\nAlternatively, you could try to create a multi-index header for each df in the loop rather than appending the column names. But I would still create the temporary df's and do the concat to merge them.\n"", 'Try:\ndf[\'tmp\'] = df.groupby(\'ID\').cumcount() + 1\n\ndf = df.pivot(index=[\'ID\', \'Class\'], columns=[\'tmp\'])\ndf.columns = [f\'{a}{b}\' for a, b in df.columns]\ndf = df.reset_index()\n\nprint(df)\n\nPrints:\n     ID     Class  X1  X2  Y1  Y2  Z1  Z2\n0  ID 1  Action 1   4   4   2   2   6   5\n1  ID 2  Action 2   3   2   4   2   4   1\n2  ID 3  Action 3   1   2   4   3   5   2\n3  ID 4  Action 1   2   2   1   2   5   5\n\n'"
Is Cufflinks (the pandas plotly-based library) currently broken when it comes to &quot;spread&quot; graphs?,"'I found this same issue while doing a certain Udemy course on Data Science.\nShort answer:\nYes, the release available through pip install (at pypi) is broken. Thought, it can be fixed and it was already fixed in the cufflinks master branch, but it has not been released in +3 years.\nLong answer / Fix:\nStep 1. Go to the install location of cufflinks and find the following file. It could be in either. It will be in the first if you are using a virtual environment.\n\n.venv\\Lib\\site-packages\\cufflinks\\plotlytools.py\n(If not using venv) C:\\users\\{username}\\appdata\\roaming\\python\\Python311\\site-packages\n\nStep 2. At the top of said file, add import numpy as np.\nStep 3. Replace in L850, and L851 (Highlighted in the image below) pd.np.nan with np.nan\n\nStep 4. Whenever you install a new venv, redo these steps.\nIf you did things right, it should be working.\nNote that this only works for Jupyter notebooks.\n\nThis was tested with:\n\npandas 2.0.3,\nnumpy 1.25.0,\ncufflinks 0.17.3,\nchart-studio 1.1.0\npython 3.11\n\nDemo\n    import pandas as pd\n    import numpy as np\n    %matplotlib inline\n    import chart_studio.plotly as py\n    import plotly.figure_factory as ff\n    import plotly.graph_objects as go\n    \n    from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n    import cufflinks as cf\n    \n    # For Notebooks\n    init_notebook_mode(connected=True)\n    \n    # For offline use\n    cf.go_offline()\n    df = pd.DataFrame(np.random.randn(100,4),columns=\'A B C D\'.split())\n    df[[\'A\',\'B\']].iplot(kind=\'spread\')\n\n\nBonus\nYou can have a similar workflow using only plotly.\n    # https://plotly.com/python/pandas-backend/\n    \n    import pandas as pd\n    \n    df = pd.read_csv(\'https://raw.githubusercontent.com/plotly/datasets/master/gapminder2007.csv\')\n    # using Plotly Express via the Pandas backend\n    pd.options.plotting.backend = ""plotly""\n    fig1 = df.plot.bar(x=\'country\', y=\'gdpPercap\')\n    fig1.show()\n\n'"
How fix &quot;AttributeError: module &#39;pandas&#39; has no attribute &#39;plotting&#39;&quot;?,"'found this forum on github\n""Can you check if you have a (maybe empty) PyQt4 directory in your site-packages? Removing that should fix it.""\n'"
How to &quot;move&quot; or &quot;append&quot; items in a table using &#39;awk&#39; or something similar?,"""Perl to the rescue:\nperl -lane '@G = split "" "", <>;\n            print ""@F[0..3] @G[0..3]"";\n            print ""@F[4..7] @G[4..7]""' -- file\n\n\n-n reads the input line by line and runs the code for each line;\n-a splits each input line into the @F array on whitespace;\n-l removes newlines from input and adds them to output;\nafter reading a line and populating @F, we read another line using the diamond operator <> and populate @G.\n\n"""
How to merge data from all files?,"""Assuming you want a pandas Series when you say you want one column, you can find the max from each DataFrame and append them to one another with the name of the file as the index.\nvalues = []\nindex = []\nfor z in csv_files:\n    # read the csv file\n    values.append(max(z))\n    index.append(z)\nprint(pd.Series(values, index = index, name = 'Max Amp'))\n\n"", ""Another way would be to obtain the data as a list and then put it into the desired dataframe\ndata_list = []\nfor z in csv_files:\n    af = pd.read_csv(z, skiprows=3)\n    data_list.append(max(af['Max Amp']))\n\nnew_df['Max Amp'] = data_list\n    \n\n\n"", 'Assuming the csv files are the same format and columns, try using pd.concat(dataframelist)\nPut the dataframes to be joined in a list:\ndataframelist = [df1, df2, ..]\ncombined_df = pd.concat(dataframelist)\n\n'"
TypeError: category type does not support sum operations (in pandas),"'This snippet works in pandas 1.* but not in pandas 2.\nimport seaborn as sns\n\nflights = sns.load_dataset(\'flights\') \nflights.groupby([\'year\']).sum() # Error\n\nThe issue is that the month column has type category:\nflights.info(True)\n\n<class \'pandas.core.frame.DataFrame\'>\nRangeIndex: 144 entries, 0 to 143\nData columns (total 3 columns):\n #   Column      Non-Null Count  Dtype   \n---  ------      --------------  -----   \n 0   year        144 non-null    int64   \n 1   month       144 non-null    category\n 2   passengers  144 non-null    int64   \ndtypes: category(1), int64(2)\nmemory usage: 2.9 KB\n\nIn pandas 1.*, the month column is automatically dropped because its type does not support the sum method.\nTo get to the same result in pandas 2, you\'ll want to specifically select the passengers column (and any other col of interest):\nflights.groupby(\'year\')[[\'passengers\']].sum()\n\nyields:\n      passengers\nyear            \n1949        1520\n1950        1676\n1951        2042\n1952        2364\n1953        2700\n1954        2867\n1955        3408\n1956        3939\n1957        4421\n1958        4572\n1959        5140\n1960        5714\n\n', ""you should try to convert the year column to a numerical or integer type\nflights['year'] = flights['year'].astype(int)\nresult = flights.groupby(['year']).sum()\n\n"""
"After applying groupby, how to fill missing values with mode value of each group respectively?","'Looking at the dataframe, there are 2 NaN values in Embarked column:\nurl = \'https://docs.google.com/spreadsheets/d/e/2PACX-1vQjh5HzZ1N0SU7ME9ZQRzeVTaXaGsV97rU8R7eAcg53k27GTstJp9cRUOfr55go1GRRvTz1NwvyOnuh/pub?gid=1562145139&single=true&output=csv\'\ntitanic_df = pd.read_csv(url)\n\nx = titanic_df[titanic_df[\'Embarked\'].isna()]\nprint(x)\n\nPrints:\n     PassengerId  Survived  Pclass                                       Name     Sex   Age  SibSp  Parch  Ticket  Fare Cabin Embarked\n61            62         1       1                        Icard, Miss. Amelie  female  38.0      0      0  113572  80.0   B28      NaN\n829          830         1       1  Stone, Mrs. George Nelson (Martha Evelyn)  female  62.0      0      0  113572  80.0   B28      NaN\n\nEvery NaN value is in Pclass 1. Looking further at .value_counts():\nx = titanic_df.groupby(\'Pclass\')[\'Embarked\'].value_counts()\nprint(x)\n\nPrints:\nPclass  Embarked\n1       S           127\n        C            85\n        Q             2\n2       S           164\n        C            17\n        Q             3\n3       S           353\n        Q            72\n        C            66\nName: Embarked, dtype: int64\n\nSo we want to replace these two NaNs with value S. We can find this value with .mode():\ntitanic_df[\'Embarked\'] = titanic_df.groupby(\'Pclass\', group_keys=False)[\'Embarked\'].transform(lambda x: x.fillna(x.mode()[0]))\n\nThe value counts is now:\nPclass  Embarked\n1       S           129\n        C            85\n        Q             2\n2       S           164\n        C            17\n        Q             3\n3       S           353\n        Q            72\n        C            66\nName: Embarked, dtype: int64\n\n'"
Issue with the `apply` method in `Pandas` when it is used on a subset of rows and applied to a dictionary on a column,"'Categories in Pandas are set initially to save memory. Internally the data is stored as an array for the categories and the data is saved as an integer array with each integer pointing to the actual value in the category array (https://pandas.pydata.org/docs/user_guide/categorical.html#categorical-data).\n\nInternally, the data structure consists of a categories array and an integer array of codes which point to the real value in the categories array.\n\nThe idea is when a large dataset has fixed values, to set the categories and then when data is modified it acts the same as it would with originally, such as ordering rules being preserved when data is removed and then added back in. Since the category for dfs is set with df[\'B\'] where the available values were \'foo\', \'bar\', and \'baz\', those are set as the categories and won\'t be changed when the data is changed. Conversely, if new data (a new value for the \'B\' column) is added that wasn\'t in the original category the column turns back into objects.\nIf data is modified to have less categories and you want to set those as the current set of categories you can use remove_unused_categories and that will reset the categories.\ndf = pd.DataFrame({\'A\':[11,22,33], \'B\':[\'foo\',\'baz\', \'bar\']})\ndf[\'B\'] = df[\'B\'].astype(\'category\')\ndfs = df[df[\'A\'] > 20].reset_index(drop=True)\nd = {\'baz\': 14, \'bar\': 19}\n\ndfs[\'B\'] = dfs[\'B\'].cat.remove_unused_categories()\n\ndfs[\'B\'].apply(lambda x: d[x])\n\nThis is referenced in the pandas docs at https://pandas.pydata.org/docs/user_guide/categorical.html#getting where it states that\n\nIf the slicing operation returns either a DataFrame or a column of\ntype Series, the category dtype is preserved.\n\nSince the line df[df[\'A\'] > 20] is just slicing and reset_index does not affect dtypes, the category dtype remains.\n'"
How to append results to dataframes given an operation whose result is the size of a group,"""Assuming your class doesn't do any aggregation of the grouped data but just adds new columns for each row in the group. You can just do the below changes to make it work and return the full dataframe\ndef some_dummy_function(df):\n  df['new_col'] = df['particle'].mean()*2\n  return df\n\ndf.groupby('particle').apply(some_dummy_function)\n\n"""
tag nsmallest and nlargest values in a column,"""In the end I have done the following:\ndf['nsmallest'] = df['val'].apply(lambda x: x.isin(x.nsmallest(2))*1)\n\n"", ""You can do something like below\ndf = pd.DataFrame({'nums':[1,2,3,4,5,6]})\nn=3 # n items\ndf['nlargest'] = df['nums'].apply(lambda x: int(x in df['nums'].sort_values(ascending=False).tolist()[:n]) )\ndf['nsmallest'] = df['nums'].apply(lambda x: int(x in df['nums'].sort_values(ascending=True).tolist()[:n]) )\nprint(df)\n\n"""
How to optimise an inequality join in Pandas?,"'Looks like some form of inequality join; conditional_join offers an efficient way to handle this.\nNote that if your dates in block are not overlapping, then pd.IntervalIndex is suitable and performant.\nKindly install the dev version which has an optimised version of the function:\n# pip install pyjanitor\n# install the dev for an optimised version\n# pip install git+https://github.com/pyjanitor-devs/pyjanitor.git\nimport janitor\nimport pandas as pd\n\n# convert date_range to either a named series, or a dataframe\ndate_range = pd.Series(date_range, name = \'date\')\n\n(block\n.conditional_join(\n    date_range, \n    # column from the left,\n    # column from the right,\n    # operator\n    (\'start_date\', \'date\', \'<=\'), \n    (\'end_date\', \'date\', \'>=\'),\n    # in some scenarios,\n    # numba might offer a perf boost\n    use_numba=False,\n   )\n)\n     Identifier start_date   end_date   color       date\n0      4913151F 2020-11-03 2023-07-11     red 2021-09-14\n1      4913151F 2020-11-03 2023-07-11     red 2021-09-15\n2      4913151F 2020-11-03 2023-07-11     red 2021-09-16\n3      4913151F 2020-11-03 2023-07-11     red 2021-09-17\n4      4913151F 2020-11-03 2023-07-11     red 2021-09-18\n...         ...        ...        ...     ...        ...\n2644   D0C57FCA 2022-06-13 2023-06-13  yellow 2023-06-09\n2645   D0C57FCA 2022-06-13 2023-06-13  yellow 2023-06-10\n2646   D0C57FCA 2022-06-13 2023-06-13  yellow 2023-06-11\n2647   D0C57FCA 2022-06-13 2023-06-13  yellow 2023-06-12\n2648   D0C57FCA 2022-06-13 2023-06-13  yellow 2023-06-13\n\n[2649 rows x 5 columns]\n\n'"
Fitting a line matplotlib,"'If you want your fitted curve go through the lowest point, you can use the sigma parameter. sigma can act as weight to ""force"" the fitted curve. What you need is to fix the std to 1 for this particular point. In fact, you fix the degree of freedom to fit the curve.\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.optimize import curve_fit\nimport uncertainties.unumpy as unp\nimport uncertainties as unc\n\nclass FatigueCurve:\n    def __init__(self) -> None:\n\n        # make the data\n        y = [125,190,335,279,212,186,186,345,345,345,276,276,190,207,276,207]\n        x = [1996768,593319,674,17700,138596,1612585,920132,2080,1244,779,19034,38474,18865901,3000000,21100,8266515]\n\n        # HERE: force the weights\n        w = np.zeros_like(y) + np.std(y)\n        w[np.argmin(y)] = 1\n\n        # HERE: use sigma=w as parameter of curve_fit\n        popt, pcov = curve_fit(self.powerLaw, x, y, sigma=w, maxfev=1000)\n        a,b = unc.correlated_values(popt, pcov)\n        px = np.linspace(min(x), max(x), len(x))\n        py = a*np.power(px,b)\n        nom = unp.nominal_values(py)\n        print(px)\n        print(py)\n\n        # plot\n        fig, ax = plt.subplots()\n\n        ax.semilogx(x, y, \'bo\', label=\'data\')\n        ax.plot(px, nom, c=\'red\', label=\'curve fit\')\n\n        plt.show()\n\n    def powerLaw(self, S, A, B):\n        return A * np.power(S,B)\n    \nif __name__ == ""__main__"":\n    FatigueCurve()\n\n\n'"
For loop with an SQL Query based on multiple columns,"""If I understood all the problem, the solution you expect is something like:\n    end_date = '7/9/2023'\n    sql_template = ""SELECT * FROM TABLE WHERE KEY IN {} AND TIME_KEY BETWEEN {} AND {}""\n    \n    promo_groups = df.groupby('Promo Name')\n    \n    results_df = pd.DataFrame()\n    \n    for promo_name, promo_group in promo_groups:                       \n        sql_query = sql_template.format(str(promo_group['Key'].tolist()), \npromo_group['Start_Date'].iloc[0]  # Assuming each promo has a single start date\nend_date)\n            \n        promo_results = execute_sql_query(sql_query)  # Replace 'execute_sql_query' with your SQL execution function\n        results_df = results_df.append(promo_results)\n    \n    print(results_df)\n\n"""
Pandas: How to Find Data from Last Filled-in Column,"""I've created the following function:\nimport pandas as pd\nimport numpy as np\n\ndef extract_lowest_level(df):\n    result_df = df.copy()\n    result_df = df.replace(0, np.nan)\n    result_df['Desired Output'] = result_df.apply(lambda row: row[row.last_valid_index()], axis=1)\n    result_df = result_df.fillna(0)\n    return result_df\n\nThat generates your desired input.\nYou can test it with the following code:\ndata = {\n    'ID': [1, 2, 3, 4],\n    'Lvl 1': ['aaa', 'aaa', 'aaa', 'aaa'],\n    'Lvl 2': [0, 'bbb', 'bbb', 'bbb'],\n    'Lvl 3': [0, 'ccc', 'ccc', 'ccc'],\n    'Lvl 4': [0, 0, 0, 'ddd']\n}\n\ndf = pd.DataFrame(data)\nprint(df)\n\nresult_df = extract_lowest_level(df)\n\nprint(result_df)\n\n"", 'You can create a custom function:\ndef last_filled_column(row):\n    out = 0\n    for value in row.values():\n        if value != 0:\n            out = value\n    return out\n\nAnd then you can just apply this function to your df like so:\ndf[""output""] = df.apply(last_filled_column, axis=1)\n\n'"
"Create multiple calculated fields which are referenced to other columns, based on conditions using Pandas","""With this code:\nimport pandas as pd\n\ndata = {\n    'ID': ['AA', 'BB'],\n    'status': [False, True],\n    'used': [3, 2]\n}\n\ndf = pd.DataFrame(data)\n\ndf['new'] = df.apply(lambda row: row['used'] * 0.2 if row['status'] == False else row['used'] * 1, axis=1)\n\nprint(df)\n\nI can create your desired output:\n   ID  status  used  new\n0  AA   False     3  0.6\n1  BB    True     2  2.0\n\n"""
Extracting certain data from txt file,"""You can make this more dynamic by identifying where the document details start. There's a line that starts with 'Description' and on that line are, effectively, a number of column headings. With that knowledge we can do this:\nimport json # just for output formatting\n\nwith open('/Volumes/G-Drive/data.txt') as data:\n    details = False\n    skip = False\n    columns = []\n    d = {}\n    for line in map(str.rstrip, data):\n        if skip:\n            skip = False\n            continue\n        if len(tokens := line.split()) == 0:\n            continue # blank line\n        if details:\n            if len(tokens) != len(columns):\n                break # finished\n            for token, column in zip(tokens, columns):\n                d.setdefault(column, []).append(token)\n        else:\n            if tokens[0] == 'Description':\n                details = True\n                skip = True\n                columns = tokens\n            else:\n                if len(tokens) == 2 and tokens[0][-1] == ':':\n                    d[tokens[0][:-1]] = tokens[1]\n    print(json.dumps(d, indent=2))\n\nFor my testing I modified the first part of the data file to look like this:\nClient:   client_1                                                                    \nProject:  project_1                                                                   \nOrder:    xxxx-xxxx-xxxx                                                              \nRun:      run_1                                                                       \nCopy:     copy_1                                                                                                         \nBatch:    batch_1\n\n...just to get some meaningful data rather than just Xs\nWe then get this output:\n{\n  ""Client"": ""client_1"",\n  ""Project"": ""project_1"",\n  ""Order"": ""xxxx-xxxx-xxxx"",\n  ""Run"": ""run_1"",\n  ""Copy"": ""copy_1"",\n  ""Batch"": ""batch_1"",\n  ""Description"": [\n    ""INPUT1_xxxxxx_170014260_20230316.pdf"",\n    ""INPUT2_xxxxxx_200406_20230316.pdf"",\n    ""INPUT2_xxxxxx_200806_20230316.pdf"",\n    ""INPUT1_xxxxxx_200012_20230316.pdf"",\n    ""INPUT1_xxxxxx_200034312_20230316.pdf"",\n    ""INPUT1_xxxxxx_200116_20230316.pdf"",\n    ""INPUT1_xxxxxx_200326_20230316.pdf"",\n    ""INPUT1_xxxxxx_200342_20230316.pdf"",\n    ""INPUT1_xxxxxx_200346_20230316.pdf"",\n    ""INPUT2_xxxxxx_200233221_20230316.pdf"",\n    ""INPUT2_xxxxxxx_200416_20230316.pdf""\n  ],\n  ""DOCUMENTS"": [\n    ""92"",\n    ""18"",\n    ""20"",\n    ""48"",\n    ""100"",\n    ""49"",\n    ""100"",\n    ""100"",\n    ""100"",\n    ""62"",\n    ""6""\n  ],\n  ""SHEETS"": [\n    ""46"",\n    ""18"",\n    ""139"",\n    ""288"",\n    ""342"",\n    ""343"",\n    ""400"",\n    ""100"",\n    ""684"",\n    ""262"",\n    ""24""\n  ],\n  ""PAGES"": [\n    ""92"",\n    ""36"",\n    ""278"",\n    ""576"",\n    ""684"",\n    ""686"",\n    ""800"",\n    ""200"",\n    ""1368"",\n    ""524"",\n    ""48""\n  ]\n}\n\n...which can easily be transformed into a DataFrame if needed.\nThere is a potential problem with this approach if/when the description is made up of more than one token\n"", ""If you want to extract all files started by 'INPUT' and ended by .pdf, try this:\nwith open(""file.txt"") as f: #open your txt file\n    data=f.read() \n    idx=0\n    while True:\n        idxstart=data.find(""INPUT"",idx) \n        if idxstart==-1:break #end of files\n        idx=data.find("".pdf"",idxstart)+4\n        file=data[idxstart:idx] #find founded\n        print(file)\n\nI not actually tested this, adjust this code.\n"""
How To Not Have &quot;=&quot; Changed To 0 From DataFrame.to_excel Using XlsxWriter,"'Pandas uses XlsxWriter\'s write() method which by default treats strings beginning with = as formulas.\nYou can change this behaviour by setting strings_to_formulas to False (in addition to the strings_to_urls that you are already using):\nwriter = pd.ExcelWriter(\'Test.xlsx\', \n                        engine=\'xlsxwriter\', \n                        options={\'strings_to_urls\': False, \n                                 \'strings_to_formulas\': False})\n\nSee the docs for more details on XlsxWriter constructor options.\nNote: the Pandas options syntax changed to engine_kwargs after version 1.3.0. See the updated XlsxWriter Pandas docs for the new syntax.\n', 'Since pandas 1.3.0, the correct syntax is:\nwriter = pd.ExcelWriter(\n    ""Text.xlsx"",\n    engine=""xlsxwriter"",\n    engine_kwargs={""options"": {""strings_to_formulas"": False}},\n)\n\n'"
Plot groupby data using secondary_y axis,"""Nowadays, you can just do\nfor key, group in df2:\n    group.plot(secondary_y='C')\n\n"", 'You can capture the axes which the Pandas plot() command returns and use it again to plot C specifically on the right axis.\n\nindex=pd.date_range(\'2011-1-1 00:00:00\', \'2011-12-31 23:50:00\', freq=\'1h\')\ndf=pd.DataFrame(np.random.randn(len(index),3).cumsum(axis=0),columns=[\'A\',\'B\',\'C\'],index=index)\n\ndf2 = df.groupby(lambda x: x.month)\nfor key, group in df2:\n    ax = group[[\'A\', \'B\']].plot()\n    group[[\'C\']].plot(secondary_y=True, ax=ax)\n\n\n\n\nTo get all lines in a single legend see:\nLegend only shows one label when plotting with pandas\n'"
Convert dot-separated values into Go structs using Python,"'Rukshan.  I see you did some work and came up with a solution you like.  I tried it and got some weird errors, still.  Maybe the CSV or the code has changed and it does indeed run without errors for you... I don\'t know.\nI see problems in output like Endpoint being duplicated, and I see Go\'s basic string type being redefined:\ntype string struct {\n    FromAddress string `json:""fromAddress,omitempty"" toml:""from_address""`\n    Username string `json:""username,omitempty"" toml:""username""`\n    Password string `json:""password,omitempty"" toml:""password""`\n    Hostname string `json:""hostname,omitempty"" toml:""hostname""`\n    ...\n\nI\'ve worked out a different solution.\nStarting with a simple, sample CSV, like:\n| toml_key | yaml_key | go_var | go_type |\n|----------|----------|--------|---------|\n| a        | a        | A      | A       |\n| a.b      | a.b      | B      | string  |\n| a.c      | a.c      | C      | string  |\n| a.d      | a.d      | D      | bool    |\n| e        | e        | E      | E       |\n| e.f      | e.f      | F      | F       |\n| e.f.g    | e.f.g    | G      | string  |\n| e.h      | e.h      | H      | string  |\n\nMy program generates the following Go code:\n// Code generated by ... DO NOT EDIT.\npackage main\n\ntype A struct {\n    B string `json:""b,omitempty"" toml:""b""`\n    C string `json:""c,omitempty"" toml:""c""`\n    D bool   `json:""d,omitempty"" toml:""d""`\n}\n\ntype E struct {\n    F struct {\n        G string `json:""g,omitempty"" toml:""g""`\n    } `json:""f,omitempty"" toml:""f""`\n    H string `json:""h,omitempty"" toml:""h""`\n}\n\nI chose to just create nested structs since I don\'t see the need for F to be its own type that is referenced in E.  This allowed for a direct approach to parsing the structure from the rows of CSV, and finally generating the Go code.  My program also allows for F and H to be siblings, even with the more-deeply nested G coming before H (in the CSV).\nI started with a typed dict named TypeDef:\nclass TypeDef(TypedDict):\n    """"""A (possible) hierarchy of config values that will be materialized into (possibly nested) Go structs.""""""\n\n    yaml_key_path: str\n    """"""The original dot-separated YAML key path.""""""\n\n    toml_name: str\n    yaml_name: str\n    go_name: str\n    go_type: str\n\n    fields: list[""TypeDef""]\n\nThe fields parameter allows me to recursively add more TypeDefs, to mimic nested structs we saw above.\nI struggled with the approach to reading the CSV and settled on this:\ndef add(td: TypeDef, tl_tds: list[TypeDef]):\n    """"""\n    Adds td either as a top-level TypeDef, directly to a top-level TypeDef\'s fields, or to\n    a sub-field of a top-level TypeDef.\n    """"""\n    print(f""adding {td[\'yaml_key_path\']}"")\n\n    path_keys = td[""yaml_key_path""].split(""."")\n\n    if len(path_keys) == 1:\n        tl_tds.append(td)\n        return\n\n    tl_key = path_keys[0]\n    tl_td: TypeDef | None = None\n    for x_td in tl_tds:\n        if x_td[""yaml_name""] == tl_key:\n            tl_td = x_td\n            break\n    assert tl_td is not None, f""could not find top-level TypeDef with key {tl_key}""\n\n    if len(path_keys) == 2:\n        tl_td[""fields""].append(td)\n        return\n\n    parent_td = tl_td  # rename top-level to parent (same object)\n\n    # Skip top-level key and omit final key.  If not all intermediate keys exist as\n    # prescribed by YAML key the last-found parent will be used.\n    intermediate_keys = path_keys[1:-1]\n    for key in intermediate_keys:\n        for child_td in parent_td[""fields""]:\n            if child_td[""yaml_name""] == key:\n                parent_td = child_td\n                break\n    parent_td[""fields""].append(td)\n\nThe function makes special cases for a TypeDef:\n\nwithout a parent: just add it to the list of structs\n\nwith just one parent: add it to the fields of the top-level TypeDef\n\nfinally, any number of intermediate (parent) TypeDefs: find the last the parent TypeDef.  Notice the comment about missing intermediate keys/TypeDefs.  Some of the configs specify YAML key paths that don\'t exist:\nuser_store.connection_password,userStore.connectionPassword,ConnectionPassword,string\nuser_store.properties.CaseInsensitiveUsername,userStore.properties.caseInsensitiveUsername,CaseInsensitiveUsername,bool\n\nCaseInsensitiveUsername appears to be a child of properties, but properties was never defined on its own, so the program will add CaseInsensitiveUsername directly to userStore, like:\n[\n    ...\n    {\n        ""yaml_key_path"": ""userStore"",\n        ""toml_name"": ""user_store"",\n        ""yaml_name"": ""userStore"",\n        ""go_name"": ""UserStore"",\n        ""go_type"": ""UserStore"",\n        ""fields"": [\n            {\n                ""yaml_key_path"": ""userStore.type"",\n                ""toml_name"": ""type"",\n                ""yaml_name"": ""type"",\n                ""go_name"": ""Type"",\n                ""go_type"": ""string"",\n                ""fields"": [],\n            },\n            ...\n            {\n                ""yaml_key_path"": ""userStore.properties.caseInsensitiveUsername"",\n                ""toml_name"": ""CaseInsensitiveUsername"",\n                ""yaml_name"": ""caseInsensitiveUsername"",\n                ""go_name"": ""CaseInsensitiveUsername"",\n                ""go_type"": ""bool"",\n                ""fields"": [],\n            },\n            ...\n        ],\n    }\n    ...\n]\n\n\n\nWriting that sample structure above to Go code becomes fairly simple.\nI start iterating the top-level TypeDefs (structs) in the list-of-TypeDefs:\nw(""// Code generated by ... DO NOT EDIT.\\n"")\nw(""package main\\n"")\nw(""\\n"")\n\n# Top-level TypeDefs are \'types\' in Go\nfor tl_td in tl_tds:\n    w(""type "")\n    write_td(tl_td)\n    w(""\\n\\n"")\n\nand for each top-level TypeDef, recursively write its fields:\ndef write_td(td: TypeDef):\n    """"""Write a TypeDef (and recursively its fields).""""""\n\n    def write_struct_tag(td: TypeDef):\n        w(f""`json:\\""{td[\'yaml_name\']},omitempty\\"" toml:\\""{td[\'toml_name\']}\\""`"")\n\n    w(""\\n"")\n    w(td[""go_name""] + "" "")\n\n    # If not a struct (simple Go type)\n    if td[""fields""] == []:\n        w(td[""go_type""])\n        return\n\n    # Else a struct\n    w(""struct {"")\n    for x_td in td[""fields""]:\n        write_td(x_td)\n        write_struct_tag(x_td)\n    w(""}"")\n\nThe final Go code looks a bit sloppy:\n// Code generated by ... DO NOT EDIT.\npackage main\n\ntype \nA struct {\nB string`json:""b,omitempty"" toml:""b""`\nC string`json:""c,omitempty"" toml:""c""`\nD bool`json:""d,omitempty"" toml:""d""`}\n\ntype \nE struct {\nF struct {\nG string`json:""g,omitempty"" toml:""g""`}`json:""f,omitempty"" toml:""f""`\nH string`json:""h,omitempty"" toml:""h""`}\n\nbut it has the minimum number of line breaks and spaces to be syntactically correct, which is all Gofmt needs:\nimport subprocess\n\ntry:\n    subprocess.run([""go"", ""fmt"", ""output.go""], check=True)\nexcept subprocess.CalledProcessError as e:\n    print(f""could not run go fmt output.go: {e}"")\n\nto make it look like the sample I shared at the top of the post.\nYou can see all the Python code, CSVs, and generated Go code I used/made in this solution, here.\nIn the full code you\'ll see I don\'t use Pandas (in my opinion, not the correct application for Pandas), and you\'ll see me validating each row:\nfor row in reader:\n    for i, x in enumerate(row):\n        assert x != """", f""on line {reader.line_num} field {i+1} was empty""\n\nbecause I found a row in my_configs.csv that was missing a Go type:\nAssertionError: on line 165 field 4 was empty\n\nwhich corresponds to this row:\nauthentication.endpoint.enableMergingCustomClaimMappingsWithDefault,authentication.endpoint.enableMergingCustomClaimMappingsWithDefault,EnableMergingCustomClaimMappingsWithDefault,\n\nwhich I believe is missing (and I corrected to) the Go type bool.\n', 'I was able to solve the issue. But, I had to completely use a new approach to solve the problem, i.e. using a tree data structure and then traversing it. Here\'s the main logic behind it - https://www.geeksforgeeks.org/level-order-tree-traversal/\nHere\'s the working python code.\nimport pandas as pd\nfrom collections import deque\n\nstructs=[]\nclass TreeNode:\n    def __init__(self, name):\n        self.name = name\n        self.children = []\n        self.path=""""\n\n    def add_child(self, child):\n        self.children.append(child)\n\ndef create_tree(data):\n    root = TreeNode(\'\')\n    for item in data:\n        node = root\n        for name in item.split(\'.\'):\n            existing_child = next((child for child in node.children if child.name == name), None)\n            if existing_child:\n                node = existing_child\n            else:\n                new_child = TreeNode(name)\n                node.add_child(new_child)\n                node = new_child\n    return root\n\ndef generate_go_struct(struct_data):\n    struct_name = struct_data[\'struct_name\']\n    fields = struct_data[\'fields\']\n    \n    go_struct = f""type {struct_name} struct {{\\n""\n\n    for field in fields:\n        field_name = field[\'name\']\n        field_type = field[\'type\']\n        field_default_val = str(field[\'default_val\'])\n        json_key=field[\'json_key\']\n        toml_key=field[\'toml_key\']\n\n        tail_part=f""\\t{field_name} {field_type} `json:\\""{json_key},omitempty\\"" toml:\\""{toml_key}\\""`\\n\\n""\n\n        if pd.isna(field[\'default_val\']):\n            go_struct += tail_part\n        else:\n            field_default_val = ""\\t// +kubebuilder:default:="" + field_default_val\n            go_struct += field_default_val + ""\\n"" + tail_part\n\n    go_struct += ""}\\n\\n""\n    return go_struct\n\ndef write_go_file(go_structs, file_path):\n    with open(file_path, \'w\') as file:\n        for go_struct in go_structs:\n            file.write(go_struct)\n\ndef create_new_struct(struct_name):\n    struct_name = ""Configurations"" if struct_name == """" else struct_name\n    struct_dict = {\n        ""struct_name"": struct_name,\n        ""fields"": []\n    }\n    \n    return struct_dict\n\ndef add_field(struct_dict, field_name, field_type,default_val,json_key, toml_key):\n    field_dict = {\n        ""name"": field_name,\n        ""type"": field_type,\n        ""default_val"": default_val,\n        ""json_key"":json_key,\n        ""toml_key"":toml_key\n    }\n    struct_dict[""fields""].append(field_dict)\n    \n    return struct_dict\n\ndef traverse_tree(root):\n    queue = deque([root])  \n    while queue:\n        node = queue.popleft()\n        filtered_df = df[df[\'yaml_key\'] == node.path]\n        go_var = filtered_df[\'go_var\'].values[0] if not filtered_df.empty else None\n        go_type = filtered_df[\'go_type\'].values[0] if not filtered_df.empty else None\n\n        if node.path=="""":\n            go_type=""Configurations""\n\n        # The structs themselves\n        current_struct = create_new_struct(go_type)\n        \n        for child in node.children:  \n            if (node.name!=""""):\n                child.path=node.path+"".""+child.name   \n            else:\n                child.path=child.name\n\n            filtered_df = df[df[\'yaml_key\'] == child.path]\n            go_var = filtered_df[\'go_var\'].values[0] if not filtered_df.empty else None\n            go_type = filtered_df[\'go_type\'].values[0] if not filtered_df.empty else None\n            default_val = filtered_df[\'default_val\'].values[0] if not filtered_df.empty else None\n\n            # Struct fields\n            json_key = filtered_df[\'yaml_key\'].values[0].split(\'.\')[-1] if not filtered_df.empty else None\n            toml_key = filtered_df[\'toml_key\'].values[0].split(\'.\')[-1] if not filtered_df.empty else None\n            \n            current_struct = add_field(current_struct, go_var, go_type,default_val,json_key, toml_key)\n\n            if (child.children):\n                # Add each child to the queue for processing\n                queue.append(child)\n\n        go_struct = generate_go_struct(current_struct)\n        # print(go_struct,""\\n"")        \n        structs.append(go_struct)\n\n    write_go_file(structs, ""output.go"")\n\ncsv_file = ""~/Downloads/my_configs.csv""\ndf = pd.read_csv(csv_file) \n\nsample_data=df[\'yaml_key\'].values.tolist()\n\n# Create the tree\ntree = create_tree(sample_data)\n\n# Traverse the tree\ntraverse_tree(tree)\n\n\nThanks a lot for all of your helps!\n'"
pandas rolling cumulative weighted average,"'Just use rolling with the same operations:\ndf[""rolling_cum_wt_avg_2periods""] = df[""val""].mul(df[""wt""]).rolling(2, min_periods=1).sum().div(df[""wt""].rolling(2, min_periods=1).sum())\n\n>>> df\n   val  wt  rolling_cum_wt_avg_2periods\n0  100   2                        100.0\n1  300   3                        220.0\n2  200   5                        237.5\n\n'"
Create months since current month column in pandas,"""If this isn't it, then please show me how you get to the desired output (I'm just throwing out random ideas at this point\ndf['weeks from max date'] = (df['IndexData'] - df['IndexData'].max()).dt.days // 7\n\n    IndexData  Week_number  weeks from max date\n0  2022-12-28           53                  -14\n1  2022-12-29           53                  -14\n2  2022-12-30           53                  -13\n3  2022-12-31           53                  -13\n4  2023-01-01            1                  -13\n5  2023-01-02            1                  -13\n6  2023-01-03            1                  -13\n7  2023-01-04            1                  -13\n8  2023-02-27            9                   -5\n9  2023-02-28            9                   -5\n10 2023-03-01            9                   -5\n11 2023-03-02            9                   -5\n12 2023-03-29           13                   -1\n13 2023-03-30           13                   -1\n14 2023-03-31           13                    0\n\n"", ""Using datetime operations:\ndate = pd.to_datetime(df['IndexData'])\nweek = date.dt.to_period('W-SAT')\nmonth = date.dt.to_period('M')\n\ndf['new_column'] = week.sub(week[month.eq(month.max())].min()).apply(lambda x: x.n)\n\nOutput:\n     IndexData  Week_number  new_column\n0   2022-12-28           53          -9\n1   2022-12-29           53          -9\n2   2022-12-30           53          -9\n3   2022-12-31           53          -9\n4   2023-01-01            1          -8\n5   2023-01-02            1          -8\n6   2023-01-03            1          -8\n7   2023-01-04            1          -8\n8   2023-02-27            9           0\n9   2023-02-28            9           0\n10  2023-03-01            9           0\n11  2023-03-02            9           0\n12  2023-03-29           13           4\n13  2023-03-30           13           4\n14  2023-03-31           13           4\n``\n\n"", 'you can use datatime from python to get the current date. Parse that date for the month and then insert a new column where the desired conditions are met based on the parsed month.\nimport datetime\n\ndate_is = str(datetime.datetime.now())\nmonth_is = date_is[5:7]\n\n\nuse month_is and figure out your logic for how far a different date is from your desired current date\n'"
Pandas Multi-Index with multiple conditions,"'You can use get_level_values:\n>>> df[(df.index.get_level_values(\'class\').isin([\'First\',\'Second\',\'Third\'])) \n       & (df.index.get_level_values(\'group\') != \'\')]\n\n              Column1\nclass  group         \nFirst  A        123.0\nSecond B        123.0\nThird  C        123.0\n\nDetails:\n>>> df.index.get_level_values(\'class\').isin([\'First\',\'Second\',\'Third\'])\narray([ True,  True,  True,  True, False])\n\n>>> df.index.get_level_values(\'group\').notna()\narray([ True, False,  True,  True,  True])\n\n'"
parallelize a function that fill missing values from duplicates in pandas dataframe,"'You can try use parallel-pandas library. It has much more functionality that pandarallel and also supports lambda functions\nimport pandas as pd\nimport numpy as np\nfrom parallel_pandas import ParallelPandas\n\n#initialize parallel-pandas\nParallelPandas.initialize(n_cpu=16, split_factor=4, disable_pr_bar=True)\n\n# create DataFrame\ndf = pd.DataFrame(np.random.random((1_000, 100))) \n\ndf.head()\n      0            1            2          3            4\n0   0.525561    0.342411    0.546397    0.016009    0.810697\n1   0.206626    0.794180    0.856513    0.492897    0.446797\n2   0.795895    0.790188    0.651192    0.196008    0.415761\n3   0.214247    0.307092    0.873755    0.518329    0.166529\n4   0.059282    0.306833    0.137190    0.206785    0.314207\n\n#parallel analogue of apply method\n#just as an example\ndf.p_apply(lambda x: x[0], axis=1)\n\n0      0.525561\n1      0.206626\n2      0.795895\n3      0.214247\n4      0.059282\n         ...   \n995    0.490312\n996    0.239747\n997    0.893300\n998    0.395077\n999    0.710804\nLength: 1000, dtype: float64\n\n\n'"
How to create new rows based on the calculation of previous rows?,"""Here's something which appears (based on the screenshots) roughly what you are wanting to do:\nimport pandas as pd\n\ndf = pd.DataFrame({\n    ""Year"" : [2016, 2016, 2016],\n    ""Sector"" : [""Mining"", ""Agriculture"", ""Manufacturing""],\n    ""Employment Index"" : [0.5, 0.6, 0.7],\n    ""VA Index"" : [0.4, 0.5, 0.6]\n})\nemp_index = (df.pivot(index=""Year"", columns=""Sector"", values=""Employment Index"")\n             .assign(Total = lambda x: x.sum(axis=1)))\n\nva_index = (df.pivot(index=""Year"", columns=""Sector"", values=""VA Index"")\n            .assign(Total = lambda x: x.sum(axis=1)))\n\nemp_index\nSector  Agriculture  Manufacturing  Mining  Total\nYear                                             \n2016            0.6            0.7     0.5    1.8\n\nva_index\nSector  Agriculture  Manufacturing  Mining  Total\nYear                                             \n2016            0.5            0.6     0.4    1.5\n\nIf it isn't, please post the intended output, along with a reproducible example.\n"""
Python Pandas: how to find rows in one dataframe but not in another?,"'I would combine (by stacking) the data frames and then perform a .drop_duplicates method. Documentation found here:\n\nhttp://pandas.pydata.org/pandas-docs/version/0.17.1/generated/pandas.DataFrame.drop_duplicates.html\n', 'Here is another similar to SQL Pandas method: .query():\n\npeople_all.query(\'ID not in @people_usa.ID\')\n\n\nor using NumPy\'s in1d() method:\n\npeople_all.[~np.in1d(people_all, people_usa)]\n\n\nNOTE: for those who have experience with SQL it might be worth to read Pandas comparison with SQL\n', ""use isin and negate the boolean mask:\n\npeople_usa[~people_usa['ID'].isin(people_all ['ID'])]\n\n\nExample:\n\nIn [364]:\npeople_all = pd.DataFrame({ 'ID' : np.arange(5)})\npeople_usa = pd.DataFrame({ 'ID' : [3,4,6,7,100]})\npeople_usa[~people_usa['ID'].isin(people_all['ID'])]\n\nOut[364]:\n    ID\n2    6\n3    7\n4  100\n\n\nso 3 and 4 are removed from the result, the boolean mask looks like this:\n\nIn [366]:\npeople_usa['ID'].isin(people_all['ID'])\n\nOut[366]:\n0     True\n1     True\n2    False\n3    False\n4    False\nName: ID, dtype: bool\n\n\nusing ~ inverts the mask\n"""
Pivot with 2 columns in pandas python,"'You can use concat:\nout = pd.concat([df[\'unnamed: 1\'].drop_duplicates(),\n                 df[\'unnamed: 0\']]).sort_index()\n\nOr with a stack and drop_duplicates:\nout = (df.iloc[:, ::-1].stack().reset_index(level=1, name=\'out\')\n         .drop_duplicates().drop(columns=\'level_1\')\n       )\n\nOutput:\n  out\n0   b\n0   B\n1   C\n2   D\n3   c\n3   E\n4   F\n\nOr based on your original idea of aggregating (but likely inefficient):\nout = (df.groupby(\'unnamed: 1\', as_index=False)[\'unnamed: 0\']\n         .agg(list).stack().explode().droplevel(1)\n       )\n\nOr a custom groupby.apply:\nout = (df.groupby(\'unnamed: 1\', group_keys=False)[\'unnamed: 0\']\n         .apply(lambda g: pd.concat([pd.Series([g.name]), g]))\n       )\n\n'"
I don&#39;t understand groupby behavior in Pandas with categorical data,"'When your keys are category dtype, the output contains the product of all combinations even if groups are missing due to observed=False default setting of groupby method.\n\nobserved: bool, default False\nThis only applies if any of the groupers are Categoricals. If True: only show observed values for categorical groupers. If False: show all values for categorical groupers.\n\nIf you use sample.groupby([\'A\', \'B\'], observed=True)[\'C\'].count().reset_index(), the output will be the same when A and B are integers.\n'"
"OpenPyXL - Change font for entire worksheet, column or row","'If you want to change the font for the whole workbook, you can modify DEFAULT_FONT as described here:  https://stackoverflow.com/a/50195113\n'"
Optimize for-loop in pandas for calculating distances and speed of a dataframe consisting of coordinates,"'How about this?  It calculates the distances before the for loop using the vectorized method.\nNote: since you didn\'t specify a value for line_length I used 1.\ndf[[\'dx\', \'dy\']] = df[[\'x\', \'y\']] - df[[\'x\', \'y\']].shift(1)\ndf[\'dist\'] = np.linalg.norm(df[[\'dx\', \'dy\']], axis=1)\ndf[\'cum_dist\'] = df[\'dist\'].cumsum()\n\nline_length = 1\ndist_prev = 0\nts_prev = df.index[0]\nresampled_pts = [{\'Time\': ts_prev, \'x\': df.iloc[0][\'x\'], \'y\': df.iloc[0][\'y\']}]\nspeeds = []\nfor ts, row in df.iloc[1:].iterrows():\n    if (row[\'cum_dist\'] - dist_prev) < line_length:\n        continue\n    resampled_pts.append({\n        \'Time\': ts, \n        \'x\': row[\'x\'],\n        \'y\': row[\'y\']\n    })\n    speeds.append({\n        \'Time\': ts,\n        \'Speed\': (row[\'cum_dist\'] - dist_prev) / (ts - ts_prev).total_seconds()\n    })\n    ts_prev = ts\n    dist_prev = row[\'cum_dist\']\n\nresampled_pts = pd.DataFrame.from_records(resampled_pts).set_index(\'Time\')\nspeeds = pd.DataFrame.from_records(speeds).set_index(\'Time\')\n\nprint(df)\nprint(resampled_pts)\nprint(speeds)\n\nOutput:\n                            x         y        dx        dy      dist  cum_dist\n2023-07-11 00:00:00  0.726821  0.218366       NaN       NaN       NaN       NaN\n2023-07-11 00:00:05  0.649865  0.030703 -0.076956 -0.187663  0.202829  0.202829\n2023-07-11 00:00:10  0.104377  0.620235 -0.545488  0.589531  0.803184  1.006013\n2023-07-11 00:00:15  0.649527  0.628917  0.545150  0.008682  0.545219  1.551232\n2023-07-11 00:00:20  0.897746  0.931917  0.248218  0.303000  0.391690  1.942923\n2023-07-11 00:00:25  0.325597  0.847569 -0.572149 -0.084348  0.578333  2.521255\n2023-07-11 00:00:30  0.149648  0.785716 -0.175949 -0.061853  0.186504  2.707760\n2023-07-11 00:00:35  0.452338  0.864169  0.302690  0.078452  0.312692  3.020451\n2023-07-11 00:00:40  0.865927  0.375195  0.413589 -0.488973  0.640430  3.660881\n2023-07-11 00:00:45  0.430665  0.259064 -0.435262 -0.116131  0.450488  4.111369\n                            x         y\nTime                                   \n2023-07-11 00:00:00  0.726821  0.218366\n2023-07-11 00:00:10  0.104377  0.620235\n2023-07-11 00:00:25  0.325597  0.847569\n2023-07-11 00:00:40  0.865927  0.375195\n                        Speed\nTime                         \n2023-07-11 00:00:10  0.100601\n2023-07-11 00:00:25  0.101016\n2023-07-11 00:00:40  0.075975\n\nAlso, note that in your code you use (end_ts - start_ts).microseconds. The microseconds method does not return the total duration in microseconds.  It returns the microseconds residual of the time-delta.  Use (end_ts - start_ts).total_seconds * 1000 instead.\n'"
"Python yfinance: Failed downloads - &quot;No data found, symbol may be delisted&quot;","""Try using the original tickers maybe, for example, the NSE stickers have a '.NS' suffix at the end.\nSo the index you are searching for may have a suffix you might need to verify about.\n"", ""You got trailing whitespace in your ticker names, which leads to invalid ticker names e. g. 'AAPL ' instead of 'AAPL'.\nYou can fix it by by replacing:\ndf = yf.download(tickers.Symbol.to_list(),'2022-1-1',today, auto_adjust=True)['Close']\n\nwith:\ndf = yf.download(tickers.Symbol.str.strip().to_list(),'2022-1-1',today, auto_adjust=True)['Close']\n\n"""
"Using linear optimisation, how do I minimize the Total Cost in a dataframe","'The problem description lends itself to solving with a Mixed-Integer-Linear Program (MIP).\nA convenient library for solving mixed integer problems is PuLP that ships with the built-in Coin-OR suite and in particular the integer solver CBC.\nWe formulate a model that describes your problem:\nimport pandas as pd\nfrom pulp import LpProblem, LpVariable, lpSum, LpMinimize\n\ndf = pd.DataFrame({\n    \'Product\': [\'Product 1\', \'Product 2\', \'Product 3\', \'Product 4\', \'Product 1\', \'Product 2\', \'Product 3\', \'Product 4\', \'Product 1\', \'Product 2\'],\n    \'Weight\': [1, 1, 1, 1, 2, 2, 2, 2, 3, 3],\n    \'Total Cost\': [\'2,773.47\', \'2,665.23\', \'23,421.24\', \'17,666.58\', \'1,592.09\', \'1,678.04\', \'12,798.46\', \'9,425.80\', \'1,246.30\', \'1,396.98\']\n})\n\n#Fix the total cost to numeric\ndf[\'Total Cost\'] = pd.to_numeric(df[\'Total Cost\'].str.replace(\',\', \'\'), errors=\'coerce\')\n\n# Decision variable for each weight product combnination\nlp_vars = {}\n\n# Cost coefficient for each product weight combination\ncosts = {}\n\nfor _, row in df.iterrows():\n    product = row[\'Product\']\n    weight = row[\'Weight\']\n    var_name = f""{product},{weight}""  \n    lp_vars[(product, weight)] = LpVariable(var_name, cat=\'Binary\') \n    costs[(product, weight)] = row[\'Total Cost\'] \nproblem = LpProblem(""Total_Cost_Minimization"", LpMinimize)\n\n\n#Minimize the total cost of all selected product,weight combinations\nproblem += lpSum([costs[(product,weight)] * lp_vars[(product,weight)] for product,weight in lp_vars.keys()])\n\n#We\'re only allowed to overall select 2 weights\nproblem += lpSum([lp_var for lp_var in lp_vars.values()]) == 2\n\n#We\'re only allowed to select up to 1 weight per product\nfor product in set(df[\'Product\']):\n    problem += lpSum([lp_vars[product,weight]  for weight in df.query(f\'Product == ""{product}""\')[\'Weight\'].tolist()]) <= 1\n\nproblem.solve()\n\n\n\n\nTo visualize the solution:\nfrom pulp import LpStatus\n\nprint(""Status:"", LpStatus[problem.status])\nprint(""Optimal Total Cost:"", problem.objective.value())\nprint(""Selected Weights and Assigned Products:"")\nfor lp_var in problem.variables():\n    if lp_var.varValue == 1:\n        print(lp_var.name)\n        product, weight = lp_var.name.split("","")\n        print(f""Product: {product}, Weight: {weight}"")\n\nOutput\nStatus: Optimal\nOptimal Total Cost: 2643.2799999999997\nSelected Weights and Assigned Products:\nProduct_1,3\nProduct: Product_1, Weight: 3\nProduct_2,3\nProduct: Product_2, Weight: 3\n\n', ""Sort your dataframe first then pivot it (without sort). The right answer is on the diagonal\n# Convert Total Cost as numeric\ndf['Total Cost'] = pd.to_numeric(df['Total Cost'].str.replace(',', ''))\n\nout = (df.sort_values('Total Cost')\n         .pivot_table(index='Product', columns='Weight', \n                      values='Total Cost', sort=False))\n\nOutput:\n>>> out\nWeight           3         2         1\nProduct                               \nProduct 1  1246.30   1592.09   2773.47\nProduct 2  1396.98   1678.04   2665.23\nProduct 4      NaN   9425.80  17666.58\nProduct 3      NaN  12798.46  23421.24\n\nObviously it works because you have only 2 variables.\nTo extract the result, you can do:\ndata = list(zip(out.index, out.columns, np.diag(out)))[:2]\nsol = pd.DataFrame(data, columns=df.columns)\nprint(sol)\n\n# Output\n     Product  Weight  Total Cost\n0  Product 1       3     1246.30\n1  Product 2       2     1678.04\n\n"", ""Below solution uses the groupby and nsmallest methods from the pandas library to find the two smallest Total Costs for each Product, then selects the two Weights with the lowest Total Cost overall.\nFirst, you’ll need to convert the ‘Total Cost’ column to a numeric type, since it’s currently stored as strings. You can do this using the pd.to_numeric method.\nUse the groupby method to group the rows by Product and find the two smallest Total Costs for each Product using the nsmallest method.\ndf['Total Cost'] = pd.to_numeric(df['Total Cost'].str.replace(',', ''), errors='coerce')\n\ngrouped = df.groupby('Product')['Total Cost'].nsmallest(2)\ngrouped = grouped.reset_index(level=0).reset_index(drop=True)\n\nresult = grouped.nsmallest(2, columns='Total Cost')\n\nprint(result)\n\nabove code will return a DataFrame with two rows showing the Product and Weight assigned to each of the two selected Weights.\nOutput: output shows that the two selected Weights are both assigned to Product 1 and have Total Costs of 1246.30 and 1592.09, respectively.\n  Products      Total Cost\n  Product 1     1246.30\n  Product 1     1592.09\n\n"""
how to compare 12 consecutive rows in a dataframe,"'A modification of Sebastian\'s approach:\ndf[\'is_peak\'] = df.rolling(12)[\'d_max\'].apply(lambda x: x.iloc[:-1].eq(x.iloc[-1]).all()).eq(1)\n\nOr a numpy approach with sliding_window_view:\nfrom numpy.lib.stride_tricks import sliding_window_view as swv\n\na = swv(df[\'d_max\'], 12)\n\n# df[\'is_peak\'] = False # optional, if you don\'t want NaNs\n\ndf.loc[df.index[-len(a):], \'is_peak\'] = (a[:, [0]] == a[:, 1:]).all(axis=1)\n\nOutput:\n    d_max is_peak\n0       2     NaN\n1       3     NaN\n2       6     NaN\n3       6     NaN\n4       6     NaN\n5       6     NaN\n6       6     NaN\n7       6     NaN\n8       6     NaN\n9       6     NaN\n10      6     NaN\n11      6   False\n12      6   False\n13      6    True\n14      1   False\n15      5   False\n16      5   False\n17      5   False\n\nComparison of approaches:\nInterestingly, numpy is faster until a few tens of thousands of rows, then slightly less efficient than shift, although the difference gets negligible for very large arrays.\n\nedit: the difference is due to assignment. I also added Sebastian nice var trick:\n\n', 'You can use pandas.Series.rolling to solve this, together with a window function. Window functions are available since pandas version 1.3.0 and are very well suited for looking at cumulations (e.g. cumsum,cumproduct,all-time-mean...) or for sliding window aggregations (mean of last N values).\ndf[\'is_peak\'] = df[\'d_max\'].rolling(12, closed=\'left\').apply(lambda x:len(set(x))==1)\n\nWe need to supply closed=\'left\' to make it exclude the current value of the window as it\'s being dragged over your data.\nFor example:\nimport pandas as pd\nimport numpy as np\ndata = {\'d_max\': [2, 3] + [6] * 12 + [1, 5, 5, 5]}\ndf = pd.DataFrame(data)\ndf[\'is_peak\'] = df[\'d_max\'][::-1].rolling(12, closed=\'left\').apply(lambda x:len(set(x))==1)[::-1]\ndf\n\n    d_max   is_peak\n0   2   0.0\n1   3   1.0\n2   6   0.0\n3   6   0.0\n4   6   0.0\n5   6   0.0\n6   6   NaN\n7   6   NaN\n8   6   NaN\n9   6   NaN\n10  6   NaN\n11  6   NaN\n12  6   NaN\n13  6   NaN\n14  1   NaN\n15  5   NaN\n16  5   NaN\n17  5   NaN\n\nThere is a performance difference between different approaches. Providing a custom lambda function in apply is incredibly slow compared to other approaches so we need to get creative to get something more performant than your original code.\nIf all values in a Series are equal, its variance is 0. We can apply this by using Rolling.var():\ndf[""d_max""].rolling(12, closed=""left"").var() == 0\n\n\nimport numpy as np\nimport pandas as pd\n\nfrom performance_measurement import run_performance_comparison\n\n\ndef shift_approach(df):\n    df[""is_peak""] = (\n        (df[""d_max""].shift(1) == df[""d_max""])\n        & (df[""d_max""].shift(2) == df[""d_max""])\n        & (df[""d_max""].shift(3) == df[""d_max""])\n        & (df[""d_max""].shift(4) == df[""d_max""])\n        & (df[""d_max""].shift(5) == df[""d_max""])\n        & (df[""d_max""].shift(6) == df[""d_max""])\n        & (df[""d_max""].shift(7) == df[""d_max""])\n        & (df[""d_max""].shift(8) == df[""d_max""])\n        & (df[""d_max""].shift(9) == df[""d_max""])\n        & (df[""d_max""].shift(10) == df[""d_max""])\n        & (df[""d_max""].shift(11) == df[""d_max""])\n    )\n    return df\n\n\n\n\ndef rolling_approach_var(df):\n    df[""is_peak""] = (\n        df[""d_max""].rolling(12, closed=""left"").var()==0\n    )\n    return df\n\n\ndef mozway_rolling_approach(df):\n    df[""is_peak""] = (\n        df.rolling(12)[""d_max""].apply(lambda x: x.iloc[:-1].eq(x.iloc[-1]).all()).eq(1)\n    )\n    return df\n\n\nfrom numpy.lib.stride_tricks import sliding_window_view as swv\n\n\ndef mozway_striding_approach(df):\n    a = swv(df[""d_max""], 12)\n    df.loc[df.index[-len(a) :], ""is_peak""] = (a[:, [0]] == a[:, 1:]).all(axis=1)\n    return df\n\n\napproaches = [\n    rolling_approach_var,\n    shift_approach,\n    mozway_striding_approach,\n]\n\n\ndef generate_data(dataset_size):\n    """"""Generates some random data of size dataset size, while making sure some runs of 12 equal numbers are included""""""\n    data = {""d_max"": np.random.randint(1, 10, dataset_size)}\n    df = pd.DataFrame(data)\n    insert_indices = np.random.choice(\n        range(dataset_size - 11), size=dataset_size // 12, replace=False\n    )\n    for idx in insert_indices:\n        df.iloc[idx : idx + 12] = df.iloc[idx]\n\n    return [df]\n\n\ndef generate_same_data(dataset_size):\n    """"""Generates a dataframe where the series always has the same value""""""\n    data = {""d_max"": [12] * dataset_size}\n    df = pd.DataFrame(data)\n\n    return [df]\n\n\nrun_performance_comparison(\n    approaches,\n    [1000, 3000, 5000, 10000, 30000, 50000, 100000, 200000, 300000,500000,1000000],\n    setup=generate_same_data,\n    title=""Same data"",\n    number_of_repetitions=1,\n)\n\nProfiling code:\nimport timeit\nimport matplotlib.pyplot as plt\nfrom typing import List, Dict, Callable\n\nfrom contextlib import contextmanager\n\n\n@contextmanager\ndef data_provider(data_size, setup=lambda N: N, teardown=lambda: None):\n    data = setup(data_size)\n    yield data\n    teardown()\n\n\ndef run_performance_comparison(approaches: List[Callable],\n                               data_size: List[int],\n                               setup=lambda N: N,\n                               teardown=lambda: None,\n                               number_of_repetitions=5, title=\'Performance Comparison\',data_name=\'N\'):\n    approach_times: Dict[Callable, List[float]] = {approach: [] for approach in approaches}\n\n    for N in data_size:\n        with data_provider(N, setup, teardown) as data:\n            for approach in approaches:\n                approach_time = timeit.timeit(lambda: approach(*data), number=number_of_repetitions)\n                approach_times[approach].append(approach_time)\n\n    for approach in approaches:\n        plt.plot(data_size, approach_times[approach], label=approach.__name__)\n    plt.yscale(\'log\')\n    plt.xscale(\'log\')\n    plt.xlabel(data_name)\n    plt.ylabel(\'Execution Time (seconds)\')\n    plt.title(title)\n    plt.legend()\n    plt.show()\n\n\n'"
Neural network .csv file header is not being read,"""data = pd.read_csv(data_path, delimiter='\\t')\nYou are using tab as delimiter. Is your data tab separated or comma separated?\nIf it's tab separated, try printing data after reading it.\n"""
How to find values that can be found in other columns in polars quickly,"""Suppose you have\ndf1 = pl.DataFrame({\n    'a' : np.random.randint(1, int(1e6), size=int(1e6)),\n    'b' : np.random.randint(1, int(1e6), size=int(1e6))\n}).with_columns(\n    (pl.lit(""v"")+pl.col(x).cast(pl.Utf8())).alias(x) for x in ['a','b']\n)\n\ndf2 = pl.DataFrame({\n    'c' : np.random.randint(1, int(1e6), size=int(1e6)),\n    'd' : np.random.randint(1, int(1e6), size=int(1e6))\n}).with_columns(\n    (pl.lit(""v"")+pl.col(x).cast(pl.Utf8())).alias(x) for x in ['c','d']\n)\n\nIf you want to know for each row of df1 where a is in c of df2 then you could do:\ndf1.with_columns(\n    structcol=pl.when(pl.col('a').is_in(df2.get_column('c')))\n        .then(pl.struct(\n            b=pl.lit(""here's one""),\n            f=pl.lit(""i'm redundant"")\n        ))\n        .otherwise(pl.struct(\n            b=pl.col('b'),\n            f=pl.lit(""also redundant"")\n        ))\n    ).drop('b').unnest('structcol')\n\nYou mentioned that you want to change some columns so this uses a struct as the new column so that you can output multiple columns from a single when/then. A struct is like a DataFrame or a dict inside a column. The caveat here is that in both the then and the otherwise the struct needs to have the same fields. In other words you couldn't have the then assign b and f while the otherwise only assigns the b. Another caveat is that unnest will throw an error if any of the columns in the struct conflicts with an existing column so that's why we drop the b before the unnest.\nYou could also do this as a join which is analogous to merge\ndf1.join(df2, left_on='a', right_on='c')\n\nThis will only return the rows where there's a match and it brings the d column too. If you want all the rows from df1, you can add the how='left'. It will omit the right_on column since it's essentially a duplicate of the left_on column at this point. You can chain on another with_columns like this...\n(\n    df1\n        .join(df2, \n              left_on='a', right_on='c', how='left')\n        .with_columns(\n            structcol=pl.when(pl.col('d').is_not_null())\n                .then(pl.struct(\n                    b=pl.lit(""here's one""),\n                    f=pl.lit(""i'm redundant"")\n                ))\n                .otherwise(pl.struct(\n                    b=pl.col('b'),\n                    f=pl.lit(""also redundant"")\n                ))\n        )\n    .drop('b')\n    .unnest('structcol')\n)\n\nIn this case I changed the when clause to whether or not d is null instead of having it perform is_in again.\n"", 'Benchmarking this polars statement\ndf.select(pl.col(\'a\').filter(pl.col(\'a\').is_in(pl.col(\'b\'))))\n\nOn the sample df\ndf = pl.DataFrame({\n  \'a\' : np.random.randint(1, 1_000_000_000, size=300_000),\n  \'b\' : np.random.randint(1, 1_000_000_000, size=300_000)\n})\n\nI get an average of 9-10ms per run.\n'"
I am getting this error : Key Error: &#39;text&#39;,"""A Python KeyError exception is what is raised when you try to access a key that isn’t in a dictionary (dict). Most likely, df['text'] doesn't always exist as a key. What you might want to do is use a try/except:\ntry:\n    df['text'] = df['text'].map(lambda text: clean(text))\n\nexcept KeyError:\n    ""Whatever you want it to do if it doesn't find a 'text' key in the df dictionary""\n\nHope that helps and have a nice day :)\n"""
Pandas merge two dataframes to get new dataframe by matching columns,"'Use sets operation:\n>>> df1.merge(df2, on=df1.columns.intersection(df2.columns).tolist())\n   A  B  C  D\n0  2  5  8  2\n1  3  6  9  1\n2  4  7  0  6\n\nAccording the documentation:\n\non: (label or list)\nColumn or index level names to join on. These must be found in both DataFrames. If on is None and not merging on indexes then this defaults to the intersection of the columns in both DataFrames.\n\nSo df1.merge(df2) works too.\n', ""Try the following:\nimport pandas as pd\n\ndf1 = pd.DataFrame({'A': [2, 3, 4], 'B': [5, 6, 7], 'C': [8, 9, 0]})\ndf2 = pd.DataFrame({'B': [6, 7, 5], 'C': [9, 0, 8], 'D': [1, 6, 2]})\n\ndf3 = df1.merge(df2, on=['B', 'C'])\n\n"""
Get pandas.read_csv to treat missing values differently depending on column type,"""Unfortunately, no. Not if you want to benefit from the automatic detection of the dtypes.\nIf you had NaN (or whichever string) in the numeric columns you could use:\ndf = pd.read_csv('test.csv', keep_default_na=False, na_values=['NaN'])\n\nBut here, the empty strings are important to automatically define the numeric columns.\nIn your case this would incorrectly set everything as object:\n  StrCol FloatCol\n0      a      1.0\n1      b      1.1\n2             1.2\n3      d         \n4      e      1.3\n\ndf.dtypes\n\nStrCol      object\nFloatCol    object\ndtype: object\n\nAn alternative to your approach (still the same logic):\nout = (pd.read_csv(data)\n         .pipe(lambda d: d.combine_first(d.select_dtypes(exclude='number')\n                                          .fillna(''))[d.columns])\n      )\n\nOutput:\n  StrCol  FloatCol\n0      a       1.0\n1      b       1.1\n2              1.2\n3      d       NaN\n4      e       1.3\n\ndf.dtypes\n\nStrCol       object\nFloatCol    float64\ndtype: object\n\n"", 'Here is the more precise code:\nimport pandas as pd\n\ndf = pd.read_csv(""test.csv"", keep_default_na=False, na_values={\'FloatCol\': \'\'})\n\n'"
How to open my files in data_folder with pandas using relative path?,"""You could use os.path.join with the name of the file which you are trying to read and the path where this file is located. Here is the example below:\nimport pandas as pd\nimport os\npath='/home/user/d_directory/'\nfile= 'xyz.csv'\ndata= pd.read_csv(os.path.join(path, file)\n\n"", 'Just replace your ""/"" with """"\n', 'Try this:\nOpen a new terminal window.\nDrag and drop the file (that you want Pandas to read) in that terminal window.\nThis will return the full address of your file in a line.\nCopy and paste that line into read_csv command as shown here:\nimport pandas as pd\npd.read_csv(""the path returned by terminal"")\n\nThat\'s it.\n', 'You can use . to represent now working path.\n#Linux\ndf = pd.read_csv(""../data_folder/data.csv"")\n#Wins\ndf = pd.read_csv(""..\\\\data_folder\\\\data.csv"")\n\n', ""If you want to keep your tidy, then I would suggest you to assign the path and file separately and then read:\npath = 'C:/Users/username/Documents/folder'\nfile_name = 'file_name.xlsx'\n\nfile=pd.read_excel(f""{path}{file_name}"")\n"", 'import os\n\ns_path = os.getcwd()\n# s_path = ""...folder/folder2/scripts_folder/script.py""\ns_path = s_path.split(\'/\')\nprint(s_path)\n# [,..., \'folder\', \'folder2\', \'scripts_folder\', \'script.py\']\n\nd_path = s_path[:len(s_path)-2] + [\'data_folder\', \'data.csv\']\nprint(os.path.join(*d_path))\n# ...folder/folder2/data_folder/data.csv```\n\n', 'You can try with this.\ndf = pd.read_csv(""E:\\working datasets\\sales.csv"")\nprint(df.head())\n\n', 'This link here answers it. Reading file using relative path in python project\nBasically using Path from pathlib you\'ll do the following in script.py\nfrom pathlib import Path\npath = Path(__file__).parent / ""../data_folder/data.csv""\npd.read_csv(path)\n\n', 'You can always point to your home directory using ~ then you can refer to your data folder.\nimport pandas as pd\ndf = pd.read_csv(""~/mydata/data.csv"")\n\nFor your case, it should be like this\nimport pandas as pd\ndf = pd.read_csv(""~/folder/folder2/data_folder/data.csv"")\n\nYou can also set your data directory as a prefix\nimport pandas as pd\nDATA_DIR = ""~/folder/folder2/data_folder/""\ndf = pd.read_csv(DATA_DIR+""data.csv"")\n\nYou can take advantage of f-strings as @nikos-tavoularis said\nimport pandas as pd\nDATA_DIR = ""~/folder/folder2/data_folder/""\nFILE_NAME = ""data.csv""\ndf = pd.read_csv(f""{DATA_DIR}{FILE_NAME}"")\n\n', 'I was also looking for the relative path version, this works OK. Note when run (Spyder 3.6) you will see (unicode error) \'unicodeescape\' codec can\'t decode bytes at the closing triple quote. Remove the offending comment lines 14 and 15 and adjust the file names and location for your environment and check for indentation.\n\n-- coding: utf-8 --\n\n""""""\nCreated on Fri Jan 24 12:12:40 2020\n\nSource:\n    Read a .csv into pandas from F: drive on Windows 7\n\nDemonstrates:\n    Load a csv not in the CWD by specifying relative path - windows version\n\n@author: Doug\n\nFrom CWD C:\\Users\\Doug\\.spyder-py3\\Data Camp\\pandas we will load file\n\nC:/Users/Doug/.spyder-py3/Data Camp/Cleaning/g1803.csv\n\n\n""""""\n\nimport csv\n\ntrainData2 = []\n\nwith open(r\'../Cleaning/g1803.csv\', \'r\') as train2Csv:\n\n  trainReader2 = csv.reader(train2Csv, delimiter=\',\', quotechar=\'""\')\n\n  for row in trainReader2:\n\n      trainData2.append(row)\n\nprint(trainData2)\n\n', 'With python or pandas when you use read_csv or pd.read_csv, both of them look into current working directory, by default where the python process have started. So you need to use os module to chdir() and take it from there.\n\nimport pandas as pd \nimport os\nprint(os.getcwd())\nos.chdir(""D:/01Coding/Python/data_sets/myowndata"")\nprint(os.getcwd())\ndf = pd.read_csv(\'data.csv\',nrows=10)\nprint(df.head())\n\n', 'Keeping things tidy with f-strings:\n\nimport os\nimport pandas as pd\n\ndata_files = \'../data_folder/\'\ncsv_name = \'data.csv\'\n\npd.read_csv(f""{data_files}{csv_name}"")\n\n', ""import pandas as pd\ndf = pd.read_csv('C:/data_folder/data.csv')\n\n"", 'For non-Windows users:\n\nimport pandas as pd\nimport os\n\nos.chdir(""../data_folder"")\ndf = pd.read_csv(""data.csv"")\n\n\nFor Windows users:\n\nimport pandas as pd\n\ndf = pd.read_csv(r""C:\\data_folder\\data.csv"")\n\n\nThe prefix r in location above saves time when giving the location to the pandas Dataframe.\n', ""Pandas will start looking from where your current python file is located. Therefore you can move from your current directory to where your data is located with '..'\nFor example:\n\npd.read_csv('../../../data_folder/data.csv')\n\n\nWill go 3 levels up and then into a data_folder (assuming it's there)\nOr\n\npd.read_csv('data_folder/data.csv')\n\n\nassuming your data_folder is in the same directory as your .py file.\n"", 'You could use the __file__ attribute:\n\nimport os\nimport pandas as pd\ndf = pd.read_csv(os.path.join(os.path.dirname(__file__), ""../data_folder/data.csv""))\n\n', ""# script.py\ncurrent_file = os.path.abspath(os.path.dirname(__file__)) #older/folder2/scripts_folder\n\n#csv_filename\ncsv_filename = os.path.join(current_file, '../data_folder/data.csv')\n\n"", 'Try\n\nimport pandas as pd\npd.read_csv(""../data_folder/data.csv"")\n\n'"
Is there a dictionary comprehension that can retrieve key-values for each row of a pandas dataframe?,"'The exact logic is unclear, but you can convert to a long format using json_normalize:\ntmp = df.explode([\'x_ticks\', \'y_ticks\'])\n\nout = pd.concat({col: pd.json_normalize(tmp[col]) for col in tmp},\n                axis=1).set_axis(tmp.index)\n\nOutput:\n  x_ticks                     y_ticks                    \n       id tick_pt.x tick_pt.y      id tick_pt.x tick_pt.y\n0      13        77       186      13        55        51\n0      14       117       186      14       155       151\n1       9       101       248       3        63        45\n1      14       117       186       4       163       145\n\nUsed input:\ndf = pd.DataFrame({\'x_ticks\': [[{\'id\': 13, \'tick_pt\': {\'x\': 77, \'y\': 186}}, {\'id\': 14, \'tick_pt\': {\'x\': 117, \'y\': 186}}],\n                               [{\'id\': 9, \'tick_pt\': {\'x\': 101, \'y\': 248}}, {\'id\': 14, \'tick_pt\': {\'x\': 117, \'y\': 186}}],\n                              ],\n                   \'y_ticks\': [[{\'id\': 13, \'tick_pt\': {\'x\': 55, \'y\': 51}}, {\'id\': 14, \'tick_pt\': {\'x\': 155, \'y\': 151}}],\n                               [{\'id\': 3, \'tick_pt\': {\'x\': 63, \'y\': 45}}, {\'id\': 4, \'tick_pt\': {\'x\': 163, \'y\': 145}}],\n                              ]\n                  })\n\n', 'You can do it easily like this using apply method:\nimport pandas as pd\n\n# Sample DataFrame\ndf = pd.DataFrame({\'x_ticks\': [{\'id\': \'A\', \'tick_pt\': {\'x\': 1, \'y\': 2}},\n                               {\'id\': \'B\', \'tick_pt\': {\'x\': 3, \'y\': 4}},\n                               {\'id\': \'C\', \'tick_pt\': {\'x\': 5, \'y\': 6}}],\n                   \'y_ticks\': [{\'id\': \'A\', \'tick_pt\': {\'x\': 7, \'y\': 8}},\n                               {\'id\': \'B\', \'tick_pt\': {\'x\': 9, \'y\': 10}},\n                               {\'id\': \'C\', \'tick_pt\': {\'x\': 11, \'y\': 12}}]})\n\n\ndf[\'X.x\'] = df[\'x_ticks\'].apply(lambda row: row[\'tick_pt\'][\'x\'])\ndf[\'X.y\'] = df[\'x_ticks\'].apply(lambda row: row[\'tick_pt\'][\'y\'])\ndf[\'Y.x\'] = df[\'y_ticks\'].apply(lambda row: row[\'tick_pt\'][\'x\'])\ndf[\'Y.y\'] = df[\'y_ticks\'].apply(lambda row: row[\'tick_pt\'][\'y\'])\n\nprint(df)\n\nOutput:\n                                    x_ticks  \\\n0  {\'id\': \'A\', \'tick_pt\': {\'x\': 1, \'y\': 2}}   \n1  {\'id\': \'B\', \'tick_pt\': {\'x\': 3, \'y\': 4}}   \n2  {\'id\': \'C\', \'tick_pt\': {\'x\': 5, \'y\': 6}}   \n\n                                      y_ticks  X.x  X.y  Y.x  Y.y  \n0    {\'id\': \'A\', \'tick_pt\': {\'x\': 7, \'y\': 8}}    1    2    7    8  \n1   {\'id\': \'B\', \'tick_pt\': {\'x\': 9, \'y\': 10}}    3    4    9   10  \n2  {\'id\': \'C\', \'tick_pt\': {\'x\': 11, \'y\': 12}}    5    6   11   12\n\n'"
Cast String field to datetime64[ns] in parquet file using pandas-on-spark,"'To recast the col3 column from a string to datetime64[ns] using pandas_on_spark, you can utilize the to_timestamp function provided by pandas_on_spark.\n'"
Python: How can I iterate within columns to make the difference the value and its previous one?,"'You can shift after temporarily setting your dates as index:\ntmp = df.set_index(\'Business Date\')\nout = (tmp/tmp.shift()).reset_index()\n\nNB. in python the decimal separator is ., not ,, make sure to use the correct format. Or convert from strings using tmp = df.set_index(\'Business Date\').apply(lambda s: pd.to_numeric(s.str.replace(\',\', \'.\'))).\nOutput:\n  Business Date    dic-22    gen-23    feb-23\n0    03/10/2022       NaN       NaN       NaN\n1    04/10/2022  0.776786  0.850537  0.933689\n2    05/10/2022  1.313793  0.998058  0.813061\n3    06/10/2022  0.889764  1.134241  1.047189\n4    07/10/2022  1.146509  0.889365  1.062320\n5    10/10/2022  0.933105  1.034716  1.010830\n\nIf you want to fill with NaNs with zeros and round:\nout = (tmp/tmp.shift()).fillna(0).round(1).reset_index()\n\nOutput:\n  Business Date  dic-22  gen-23  feb-23\n0    03/10/2022     0.0     0.0     0.0\n1    04/10/2022     0.8     0.9     0.9\n2    05/10/2022     1.3     1.0     0.8\n3    06/10/2022     0.9     1.1     1.0\n4    07/10/2022     1.1     0.9     1.1\n5    10/10/2022     0.9     1.0     1.0\n\nhandling non consecutive dates differently\nFor the sake of generalization, as your dates are not consecutive. In your example you shift to the previous available date. If instead, you wanted to access the exact previous day (10/10/2022 -> 09/10/2022), then you would need to change the code to:\ndf[\'Business Date\'] = pd.to_datetime(df[\'Business Date\'], dayfirst=True)\n\ntmp = df.set_index(\'Business Date\')\nout = (tmp/tmp.shift(freq=\'1D\')).fillna(0).round(1).reset_index()\n\nOutput:\n  Business Date  dic-22  gen-23  feb-23\n0    2022-10-03     0.0     0.0     0.0\n1    2022-10-04     0.8     0.9     0.9\n2    2022-10-05     1.3     1.0     0.8\n3    2022-10-06     0.9     1.1     1.0\n4    2022-10-07     1.1     0.9     1.1\n5    2022-10-08     0.0     0.0     0.0\n6    2022-10-10     0.0     0.0     0.0\n7    2022-10-11     0.0     0.0     0.0\n\n'"
get specific id after group by condition of time stamp difference,"""You can use pandas indexing:\ntmp = pd.to_datetime(df.set_index(['reference', 'id'])['timestamp'])\n\nout = tmp.loc['ref1'] - tmp.loc['ref2']\n\nout = set(out.index[out.gt('0')])\n\nNB. If you have several identical dates for one of the references, it will compute all combinations. Here the set acts as a any.\nOutput:\n{'a2'}\n\nOr:\nout = (tmp.loc['ref1'] - tmp.loc['ref2']\n      ).loc[lambda x: x.gt('0')].index.unique().tolist()\n\nOutput: ['a2']\n"""
.vcf data to pandas dataframe,"'Simple function:\nimport io\nimport os\nimport pandas as pd\n\n\ndef read_vcf(path):\n    with open(path, \'r\') as f:\n        lines = [l for l in f if not l.startswith(\'##\')]\n    return pd.read_csv(\n        io.StringIO(\'\'.join(lines)),\n        dtype={\'#CHROM\': str, \'POS\': int, \'ID\': str, \'REF\': str, \'ALT\': str,\n               \'QUAL\': str, \'FILTER\': str, \'INFO\': str},\n        sep=\'\\t\'\n    ).rename(columns={\'#CHROM\': \'CHROM\'})\n\nTakes a VCF file, removes the metadata lines, and converts the remaining data into a Pandas DataFrame, with specific column data types and renaming applied.\n', 'GATK Variantstotable is what you need to avoid any issue due to the flexibility of the format of the VCF. Then, when having the csv, import it into pandas. I would say that this is the most robust way to do this.\nhttps://gatk.broadinstitute.org/hc/en-us/articles/360036896892-VariantsToTable\n', 'import pandas as pd\nwith open(filename, ""r"") as f:\n    lines = f.readlines()\n    chrom_index = [i for i, line in enumerate(lines) if line.strip().startswith(""#CHROM"")]\n    data = lines[chrom_index[0]:]  \n    header = data[0].strip().split(""\\t"")\n    informations = [d.strip().split(""\\t"") for d in data[1:]]\n\nvcf = pd.DataFrame(informations, columns=header)\n\n', ""There is no need to read line by line.\nPandas has an option called comment which can be used to skip unwanted lines.\nYou can directly load VCF files into pandas by running the following line.\nIn [9]: pd.read_csv('clinvar_final.txt', sep=""\\t"", comment='#')\nOut[9]: \n        CHROM        POS      ID REF ALT FILTER QUAL                                               INFO\n0           1    1014O42  475283   G   A      .    .  AF_ESP=0.00546;AF_EXAC=0.00165;AF_TGP=0.00619;...\n1           1    1O14122  542074   C   T      .    .  AF_ESP=0.00015;AF_EXAC=0.00010;ALLELEID=514926...\n2           1    1014143  183381   C   T      .    .  ALLELEID=181485;CLNDISDB=MedGen:C4015293,OMIM:...\n3           1    1014179  542075   C   T      .    .  ALLELEID=514896;CLNDISDB=MedGen:C4015293,OMIM:...\n4           1    1014217  475278   C   T      .    .  AF_ESP=0.00515;AF_EXAC=0.00831;AF_TGP=0.00339;...\n...       ...        ...     ...  ..  ..    ...  ...                                                ...\n102316      3  179210507  403908   A   G      .    .  ALLELEID=393412;CLNDISDB=MedGen:C0018553,Orpha...\n102317      3  179210511  526648   T   C      .    .  ALLELEID=519163;CLNDISDB=MedGen:C0018553,Orpha...\n102318      3  179210515  526640   A   C      .    .  AF_EXAC=0.00002;ALLELEID=519178;CLNDISDB=MedGe...\n102319      3  179210516  246681   A   G      .    .  AF_EXAC=0.00001;ALLELEID=245287;CLNDISDB=MedGe...\n102320      3  179210538  259958   A   T      .    .  AF_EXAC=0.00001;ALLELEID=251013;CLNDISDB=MedGe...\n\n"""
How to transform a dataframe to merge two columns&#39; information for one-hot encoding,"'In pure pandas, you can go with a pivot_table with max as aggregation function:\nout = (df.pivot_table(index=\'Index\', columns=\'Card\', values=\'Success\',\n                      aggfunc=\'max\', fill_value=0)\n         # below is optional\n         .rename(columns=lambda x: f\'Card_{x}_Success\')\n         .reset_index().rename_axis(columns=None)\n      )\n\nOutput:\n   Index  Card_Master_Success  Card_Visa_Success\n0      1                    1                  0\n1      2                    0                  1\n2      3                    0                  1\n3      4                    1                  0\n\n', 'If you have a multi-indexed pd.DataFrame as shown in the input data I think you can get the result by using unstack to pull one index level from the row -index into a column-index.\nAs the unstack operation gives us a multi-index for the columns, we need to merge the multi-index into a regular ones using a map operation.\nI assume if there is no content in the DataFrame for a specific card-index combination it is not considered ""successful"", so we fill the missing values with 0s.\nimport pandas as pd\n\ndata = {\n    \'Index\': [1, 1, 2, 2, 3, 4],\n    \'Card\': [\'Visa\', \'Master\', \'Visa\', \'Master\', \'Visa\', \'Master\'],\n    \'Success\': [0, 1, 1, 0, 1, 1]\n}\n\ndf = pd.DataFrame(data).set_index([\'Index\',\'Card\']).unstack().fillna(0.0)\ndf.columns = df.columns.map(\'{0[0]}_{0[1]}\'.format) \nprint(df)\n\n       Success_Master  Success_Visa\nIndex                              \n1                 1.0           0.0\n2                 0.0           1.0\n3                 0.0           1.0\n4                 1.0           0.0\n\nTo fit this in an sklearn pipeline, you just need to implement a transform method on a Transformer:\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass CardSuccessTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        X_transformed = X.copy()\n        X_transformed = X_transformed.set_index([\'Index\', \'Card\']).unstack().fillna(0.0)\n        X_transformed.columns = X_transformed.columns.map(\'{0[0]}_{0[1]}\'.format)\n        return X_transformed\n\n\nYou can find more information about how to write custom transformers and use them in your pipeline here:\npipe = Pipeline(\n    steps=[\n        (""card_success_transformer"", CardSuccessTransformer())\n    ]\n)\ntransformed_df = pipe.fit_transform(df)\n\n'"
how to fill missing timestamp values with mean value in pandas dataframe,"'Create DatetimeIndex and add missing values by DataFrame.asfreq, then use DatetimeIndex.time and Series.interpolate:\nout = df.set_index(pd.to_datetime(df[\'Time\'], format=\'%H:%M:%S\')).asfreq(\'S\')\n\n#alternative with resample, e.g. by aggregate first value\n#out = df.set_index(pd.to_datetime(df[\'Time\'], format=\'%H:%M:%S\')).resample(\'S\').first()\n\n\nout[\'Time\'] = out.index.time\nout[\'rpm\'] = out[\'rpm\'].interpolate()\n\nout = out.reset_index(drop=True)\nprint (out)\n       Time   rpm\n0  12:47:56   5.5\n1  12:47:57   7.0\n2  12:47:58   9.0\n3  12:47:59  10.5\n4  12:48:00  12.0\n5  12:48:01  16.0\n6  12:48:02  19.0\n7  12:48:03  20.0\n\n'"
Python application won&#39;t shut down after calling Vaex df.sum or df.unique,'This is happening due to access of threads related issues. Kindly go through it: here\n'
how to fill missing seconds in pandas dataframe,"'Create DatetimeIndex first and then use DataFrame.resample, last set columns values:\ndf.index = pd.to_datetime(df[\'Date\'] + df[\'Time\'].astype(str), \n                          format=\'%m/%d%H:%M:%S\', \n                          errors=\'coerce\')\n\nout = df.resample(\'S\').first()\n\nout[\'Time\'] = out.index.time\nout[\'Date\'] = out.index.strftime(\'%m/%d\')\nout[\'rpm\'] = out[\'rpm\'].fillna(0)\nout[\'sec\'] = out.groupby(\'Date\').cumcount().add(1)\nprint (out)\n                     sec   Date      Time  rpm\n1900-07-07 12:47:30    1  07/07  12:47:30  0.0\n1900-07-07 12:47:31    2  07/07  12:47:31  0.0\n1900-07-07 12:47:32    3  07/07  12:47:32  0.0\n1900-07-07 12:47:33    4  07/07  12:47:33  0.0\n1900-07-07 12:47:34    5  07/07  12:47:34  0.0\n1900-07-07 12:47:35    6  07/07  12:47:35  0.0\n1900-07-07 12:47:36    7  07/07  12:47:36  0.0\n1900-07-07 12:47:37    8  07/07  12:47:37  0.0\n\n\nout = out.reset_index(drop=True)\nprint (out)\n   sec   Date      Time  rpm\n0    1  07/07  12:47:30  0.0\n1    2  07/07  12:47:31  0.0\n2    3  07/07  12:47:32  0.0\n3    4  07/07  12:47:33  0.0\n4    5  07/07  12:47:34  0.0\n5    6  07/07  12:47:35  0.0\n6    7  07/07  12:47:36  0.0\n7    8  07/07  12:47:37  0.0\n\nAnother solution with forward filling dates by Series.ffill with add second for non times values created by GroupBy.cumcount and to_timedelta:\ndates = pd.to_datetime(df[\'Date\'] + df[\'Time\'].astype(str), \n                          format=\'%m/%d%H:%M:%S\', \n                          errors=\'coerce\')\n\nsec = pd.to_timedelta(df.groupby(dates.notna().cumsum()).cumcount(), unit=\'s\')\n\ndf[\'Time\'] = dates.ffill().add(sec).dt.strftime(\'%H:%M:%S\')\nprint (df)\n   sec Date      Time  rpm\n0    1  7/7  12:47:30  0.0\n1    2    0  12:47:31  0.0\n2    3    0  12:47:32  0.0\n3    4  7/7  12:47:33  0.0\n4    5  7/7  12:47:34  0.0\n5    6    0  12:47:35  0.0\n6    7  7/7  12:47:36  0.0\n7    8  7/7  12:47:37  0.0\n8    9    0  12:47:38  0.0\n9   10    0  12:47:39  0.0\n\n', ""Another possible solution, which uses linear interpolation to fill the null times:\nfrom scipy.interpolate import interp1d\n\ndf['Time'] = pd.to_datetime(df['Time'], format='%H:%M:%S', errors='coerce')\ndf_nonan = df[['sec', 'Time']].dropna()\nf = interp1d(df_nonan.iloc[:, 0], df_nonan.iloc[:, 1],\n             fill_value='extrapolate')\ndf['Time'] = pd.to_datetime(f(df['sec']))\ndf['Time'] = df['Time'].dt.time\n\nOutput:\n   sec Date      Time  rpm\n0    1  7/7  12:47:30  0.0\n1    2    0  12:47:31  0.0\n2    3    0  12:47:32  0.0\n3    4  7/7  12:47:33  0.0\n4    5  7/7  12:47:34  0.0\n5    6    0  12:47:35  0.0\n6    7  7/7  12:47:36  0.0\n7    8  7/7  12:47:37  0.0\n8    9    0  12:47:38  0.0\n9   10    0  12:47:39  0.0\n\n"", ""Convert Time column .to_datetime and add one second to previous time, as show below\nCode:\n# Convert 'Time' column to datetime and '0' values to NaT (Not a time)\ndf['Time'] = pd.to_datetime(df['Time'], format='%H:%M:%S', errors='coerce')\n\n# Iterate over the 'Time' column and replace NaT values \n# with the time by adding one second to the previous time\nprevious_time = None\nfor i, time in enumerate(df['Time']):\n    if pd.isnull(time):\n        new_time = (previous_time + timedelta(seconds=1))\n        df.at[i, 'Time'] = new_time\n        previous_time = new_time\n    else:\n        previous_time = time\n\ndf['Time'] = df['Time'].apply(lambda x: x.strftime('%H:%M:%S'))\n\n\nOutput:\nsec Date    Time    rpm\n0   1   7/7 12:47:30    0.0\n1   2   0   12:47:31    0.0\n2   3   0   12:47:32    0.0\n3   4   7/7 12:47:33    0.0\n4   5   7/7 12:47:34    0.0\n5   6   0   12:47:35    0.0\n6   7   7/7 12:47:36    0.0\n7   8   7/7 12:47:37    0.0\n8   9   0   12:47:38    0.0\n9   10  0   12:47:39    0.0\n\n"", ""Since your column always increments by one second, you can just ""create"" it with pd.date_range\nThe following line gives the desired output.\ndf['Time'] = pd.date_range(start='12:47:30', end='12:47:39', freq='s')\n\nIf you have a big dataset, instead of specifying the end, you can simply pass the number of values to create with the periods parameter.\n"", 'Here is the code that you want:\nimport pandas as pd\n\ndf = pd.DataFrame({\n    \'sec\': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n    \'Date\': [\'7/7\', \'0\', \'0\', \'7/7\', \'7/7\', \'0\', \'7/7\', \'7/7\', \'0\', \'0\'],\n    \'Time\': [\'12:47:30\', \'0\', \'0\', \'12:47:33\', \'12:47:34\', \'0\', \'12:47:36\', \'12:47:37\', \'0\', \'0\'],\n    \'rpm\': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n})\n\n# Create a mask for rows with \'0\' time values\nmask = df[\'Time\'] == \'0\'\n\n# Find the index of the first non-zero time value\nfirst_nonzero_idx = df.loc[~mask, \'Time\'].index[0]\n\n# Convert the \'Time\' column to a list for easier manipulation\ntimes = df[\'Time\'].tolist()\n\n# Fill in the missing time values by incrementing from the previous non-zero time value\nfor i in range(first_nonzero_idx + 1, len(times)):\n    if mask[i]:\n        prev_time = pd.to_datetime(times[i-1])\n        times[i] = (prev_time + pd.DateOffset(seconds=1)).strftime(\'%H:%M:%S\')\n\n# Update the \'Time\' column in the dataframe\ndf[\'Time\'] = times\n\nprint(df)\n\nOutput:\n   sec Date      Time  rpm\n0    1  7/7  12:47:30  0.0\n1    2    0  12:47:31  0.0\n2    3    0  12:47:32  0.0\n3    4  7/7  12:47:33  0.0\n4    5  7/7  12:47:34  0.0\n5    6    0  12:47:35  0.0\n6    7  7/7  12:47:36  0.0\n7    8  7/7  12:47:37  0.0\n8    9    0  12:47:38  0.0\n9   10    0  12:47:39  0.0\n\n'"
how to make pandas row value to zero when row above values are zeros and below value not equal to zero using python pandas,"'You can use shift to check the condition:\ndf.loc[df[\'rpm\'].shift(1).eq(0) & df[\'rpm\'].shift(-1).ne(0), \'rpm\'] = 0\nprint(df)\n\n# Output:\n     rpm\n0    2.0\n1    4.5\n2    5.6\n3    6.0\n4    7.0\n5    6.0\n6    0.0\n7    0.0\n8    0.0  # HERE, old value: 3\n9    5.0\n10   9.0\n11   8.9\n12   9.3\n13   0.0\n14   0.0\n15   0.0\n16   0.0  # HERE, old value: 6\n17   7.0\n18   8.0\n19   9.0\n20  13.0\n\nDetails:\nm1 = df[\'rpm\'].shift(1).eq(0)\nm2 = df[\'rpm\'].shift(-1).ne(0)\nout = pd.concat([df[\'rpm\'], m1, m2, m1&m2], keys=[\'rpm\', \'m1\', \'m2\', \'all\'], axis=1)\nprint(out)\n\n# Output\n     rpm    rpm    rpm    rpm\n0    2.0  False   True  False\n1    4.5  False   True  False\n2    5.6  False   True  False\n3    6.0  False   True  False\n4    7.0  False   True  False\n5    6.0  False  False  False\n6    0.0  False  False  False\n7    0.0   True   True   True  # HERE, already 0\n8    3.0   True   True   True  # HERE, set to 0\n9    5.0  False   True  False\n10   9.0  False   True  False\n11   8.9  False   True  False\n12   9.3  False  False  False\n13   0.0  False  False  False\n14   0.0   True  False  False\n15   0.0   True   True   True  # HERE, already 0\n16   6.0   True   True   True  # HERE, set to 0\n17   7.0  False   True  False\n18   8.0  False   True  False\n19   9.0  False   True  False\n20  13.0  False   True  False\n\n'"
Pandas: Change subset of rows that contain duplicate values for a particular column based on values across all duplicates,"'Use ordered Categorical for Length column, so possible create mask by DataFrame.sort_values and DataFrame.duplicated, DataFrame.sort_index is for original order of rows and set NaNs for not matched values in Series.mask with GroupBy.transform for get first non NaN value:\ndf[\'Length\'] = pd.Categorical(df[\'Length\'], \n                              categories=[\'Long\',\'Medium\',\'Short\'],\n                              ordered=True)\n\nmask = df.sort_values(\'Length\').duplicated([\'Class\']).sort_index()\ndf[\'Head Teacher\'] = df[\'Head Teacher\'].mask(mask).groupby(df[\'Class\']).transform(\'first\')\ndf[\'Premium Course\'] = df[\'Premium Course\'].mask(mask)\n\nprint (df)\n     Class  Length Head Teacher Premium Course\n0    Maths  Medium   Mr. Bloggs            Yes\n1  English   Short   Mrs. Green            NaN\n2  English    Long   Mrs. Green            Yes\n3  English  Medium   Mrs. Green            NaN\n4  Science    Long    Mrs. Blue            Yes\n5  Science    Long    Mrs. Blue            NaN\n\n', ""Example Code\nimport pandas as pd\ndata1 = {'Class': ['Maths', 'English', 'English', 'English', 'Science', 'Science'], \n         'Length': ['Medium', 'Short', 'Long', 'Medium', 'Long', 'Long'], \n         'Head Teacher': ['Mr. Bloggs', 'Mr. Plum', 'Mrs. Green', 'Mr. Top', 'Mrs. Blue', 'Mr. Red'], \n         'Premium Course': ['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes']}\ndf = pd.DataFrame(data1)\n\nStep1\nmake condition\nm = {'Short':0, 'Medium':1, 'Long':2}\ncond = df.groupby('Class')['Length'].transform(lambda x: x.index == x.map(m).idxmax())\n\ncond\n0     True\n1    False\n2     True\n3    False\n4     True\n5    False\nName: Length, dtype: bool\n\nStep2\nedit columns\ndf['Head Teacher'] = df['Head Teacher'].where(cond).groupby(df['Class']).ffill().bfill()\ndf['Premium Course'] = df['Premium Course'].where(cond)\n\ndf\n    Class   Length  Head Teacher    Premium Course\n0   Maths   Medium  Mr. Bloggs      Yes\n1   English Short   Mrs. Green      NaN\n2   English Long    Mrs. Green      Yes\n3   English Medium  Mrs. Green      NaN\n4   Science Long    Mrs. Blue       Yes\n5   Science Long    Mrs. Blue       NaN\n\n"""
Read multiple csv files with first 5 rows in 5 minutes in python,"""import pandas as pd\nimport glob\nimport time\n\ncsv_files = glob.glob('/root/*.csv') \nskip=0\nwhile True:\n    for file in csv_files:\n        df = pd.read_csv(file, skiprows=skip, nrows=5)   file\n        print(df)\n        skip+=5\n    time.sleep(300)  # Pause execution for 5 minutes (300 seconds)\n\n"""
Cleaner way of getting date out of pandas timestamp,'You can use dt.normalize:\nfoo[\'date\'] = pd.to_datetime(foo[\'b\']).dt.normalize()\n\nOutput:\n>>> foo\n   a                 b       date\n0  1  2021-01-05 05:15 2021-01-05\n1  2  2021-01-06 11:10 2021-01-06\n2  3  2021-03-01 09:00 2021-03-01\n\n>>> foo.dtypes\na                int64\nb               object\ndate    datetime64[ns]\ndtype: object\n\nHowever your last solution is a good solution pd.to_datetime(foo.b.str[:11]).\n'
How to compare two lists of pandas dataframe?,"'Seems like you are trying to compare dataframes with different indexes or column names.\nIf you want to compare their values you can use:\nimport numpy as np\nare_values_equal = np.array_equal(df1.values, df2.values)\nprint(are_values_equal)\n\nAlso, you can try to compare the specific columns like this:\nare_columns_equal = df1[\'A\'].equals(df2[\'A\'])\nprint(are_columns_equal)\n\nCheck this two methods in docs:\nhttps://pandas.pydata.org/docs/reference/api/pandas.DataFrame.equals.html\nhttps://numpy.org/doc/stable/reference/generated/numpy.array_equal.html\n', 'Can use\nall(x.equals(y) for x, y in zip(a, b))\n\n'"
Create multi index from columns in pandas dataframe,"'You can use the str accessor for pd.Index objects and create a pd.MultiIndex with split and the expand=True argument\n\ndf.columns = df.columns.str.split(\' \', 1, expand=True)\n\n\nThen you can stack the first level of the column index you just created\n\ndf.stack(0)\n\n                   High     Low    Open  Px_last   US Equity  Volume\nDate                                                                \n12/31/2012 SPOM  0.4575  0.2925  0.4575   0.3975  12/31/2012    8890\n           VMGI  0.0110  0.0090  0.0090   0.0090  12/31/2012  105726\n1/1/2013   SPOM  0.4575  0.2925  0.4575   0.3975    1/1/2013    8890\n           VMGI  0.0110  0.0090  0.0090   0.0090    1/1/2013  105726\n1/2/2013   SPOM  0.3975  0.3225  0.3975   0.3225    1/2/2013    3400\n           VMGI  0.0100  0.0080  0.0090   0.0100    1/2/2013  188150\n1/3/2013   SPOM  0.3738  0.2800  0.3400   0.2900    1/3/2013   48933\n           VMGI  0.0180  0.0110  0.0110   0.0150    1/3/2013  169890\n1/4/2013   SPOM  0.4000  0.3175  0.3600   0.3175    1/4/2013    3610\n           VMGI  0.0180  0.0140  0.0150   0.0180    1/4/2013   33500\n\n\n\n\nA variation of this without editing the columns object in place would be to use the set_axis method.  pd.DataFrame.set_axis as of Pandas version 0.21 now accepts an inplace=False argument which allows for pipelining.\n\ndf.set_axis(df.columns.str.split(\' \', 1, expand=True), 1, 0).stack(0)\n\n                   High     Low    Open  Px_last   US Equity  Volume\nDate                                                                \n12/31/2012 SPOM  0.4575  0.2925  0.4575   0.3975  12/31/2012    8890\n           VMGI  0.0110  0.0090  0.0090   0.0090  12/31/2012  105726\n1/1/2013   SPOM  0.4575  0.2925  0.4575   0.3975    1/1/2013    8890\n           VMGI  0.0110  0.0090  0.0090   0.0090    1/1/2013  105726\n1/2/2013   SPOM  0.3975  0.3225  0.3975   0.3225    1/2/2013    3400\n           VMGI  0.0100  0.0080  0.0090   0.0100    1/2/2013  188150\n1/3/2013   SPOM  0.3738  0.2800  0.3400   0.2900    1/3/2013   48933\n           VMGI  0.0180  0.0110  0.0110   0.0150    1/3/2013  169890\n1/4/2013   SPOM  0.4000  0.3175  0.3600   0.3175    1/4/2013    3610\n           VMGI  0.0180  0.0140  0.0150   0.0180    1/4/2013   33500\n\n\n\n\nTo take is one step further, we can swap the levels of the index and sort to improve the layout.\n\ndf.set_axis(df.columns.str.split(\' \', 1, expand=True), 1, 0).stack(0) \\\n    .swaplevel(0, 1).sort_index().reindex(df.index, level=1)\n\n                   High     Low    Open  Px_last   US Equity  Volume\n     Date                                                           \nSPOM 12/31/2012  0.4575  0.2925  0.4575   0.3975  12/31/2012    8890\n     1/1/2013    0.4575  0.2925  0.4575   0.3975    1/1/2013    8890\n     1/2/2013    0.3975  0.3225  0.3975   0.3225    1/2/2013    3400\n     1/3/2013    0.3738  0.2800  0.3400   0.2900    1/3/2013   48933\n     1/4/2013    0.4000  0.3175  0.3600   0.3175    1/4/2013    3610\nVMGI 12/31/2012  0.0110  0.0090  0.0090   0.0090  12/31/2012  105726\n     1/1/2013    0.0110  0.0090  0.0090   0.0090    1/1/2013  105726\n     1/2/2013    0.0100  0.0080  0.0090   0.0100    1/2/2013  188150\n     1/3/2013    0.0180  0.0110  0.0110   0.0150    1/3/2013  169890\n     1/4/2013    0.0180  0.0140  0.0150   0.0180    1/4/2013   33500\n\n\nStrictly speaking, that last bit with the reindex isn\'t exactly necessary.  But it bothered me that I might be rearranging dates.  So I put them back in place. \n'"
AttributeError: &#39;numpy.float64&#39; object has no attribute &#39;log10&#39;,"""I had a similar error message when using the standard deviation (np.std) instead of np.log10: \n\n\n  'AttributeError: 'numpy.float64' object has no attribute 'sqrt', \n\n\nand this although I had previously converted the Pandas object X to a numpy array via np.asarray(X). \n\nI could solve this problem by applying the above-mentioned solution:\n\nX = pd.read_excel('file.xls')\nY = np.asarray(X).astype(np.float64)\nZ = np.std(Y,axis=0)\n\n"", 'numpy.log10 is a ""ufunc"", and the method Series.apply(func) has a special test for numpy ufuncs which makes test.apply(log10) equivalent to np.log10(test).  This means test, a Pandas Series instance, is passed to log10.  The data type of test is object, which means that the elements in test can be arbitrary Python objects. np.log10 doesn\'t know how to handle such a collection of objects (it doesn\'t ""know"" that those objects are, in fact, all np.float64 instances), so it attempts to dispatch the calculation to the individual elements in the Series.  To do that, it expects the elements themselves to have a log10 method.  That\'s when the error occurs: the elements in the Series (in this case, np.float64 instances) do not have a log10 method.\n\nA couple alternative expression that should do what you want are np.log10(test.astype(np.float64)) or test.astype(np.float64).apply(np.log10).  The essential part is that  test.astype(np.float64) converts the data type of the Series object from object to np.float64.\n'"
Pandas apply function read in list horizontally as an input,"""Try this:\ndf['Result'] = df.apply(lambda row: f(row[['A', 'B', 'C']], row['Val']), axis=1)\n\nimport pandas as pd\n\ncols = ['Name', 'A', 'B', 'C', 'Type', 'Val']\ndata = [['Front', 1, 2, 3, 'Up', 11],\n        ['Front', 4, 5, 6, 'Dw', 22]]\ndf = pd.DataFrame(data, columns=cols)\n\ndef f(x, y):\n    return sum(x) * y\n\ndf['Result'] = df.apply(lambda row: f(row[['A', 'B', 'C']], row['Val']), axis=1)\n\nprint(df)\n\n"", ""import pandas as pd\n\ncols = ['Name', 'A', 'B', 'C', 'Type', 'Val']\ndata = [['Front', 1, 2, 3, 'Up', 11],\n        ['Front', 4, 5, 6, 'Dw', 22]]\ndf = pd.DataFrame(data, columns=cols)\n\ndef f(x, y):\n    return sum(x) * y\n\ndf['Result'] = df.apply(lambda row: f(row[['A', 'B', 'C']].values.tolist(), row['Val']), axis=1)\nprint(df)\n\nResult:\nName  A  B  C Type  Val  Result\n0  Front  1  2  3   Up   11      66\n1  Front  4  5  6   Dw   22     330\n\n"", 'You can directly vectorize this!\ndf[""Result""] = (df[""A""] + df[""B""] + df[""C""]) * df[""Val""]\n\n'"
"The truth value of a series is ambiguous, when I made a calculation on a loop on Python","'Here’s a link to a popular question that should help you. It’s still out of my grasp but I get what you’re trying to do and I think the answers here will help you fix it without over complicating. Please let me know if I’m wrong!\nTruth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()\n'"
intersection 2 pandas dataframe,"'One option is with conditional_join\nNote that this uses the dev version, which is optimised:\n# pip install pyjanitor\n# dev version, optimised\n# pip install git+https://github.com/pyjanitor-devs/pyjanitor.git\n\n(df2\n.conditional_join(\n    df1.loc[df1.Remove==1], \n    (\'Start\', \'End\', \'<=\'), \n    (\'End\', \'Start\', \'>=\'), \n    how=\'left\', \n    right_columns=\'Remove\')\n.loc[lambda df: df.Remove.isna(), df2.columns]\n) \n   Start  End\n2    151  154\n\n', 'How about this:\n\nmydataframe1[\'key\']=1\nmydataframe2[\'key\']=1\n\ndf3 = mydataframe2.merge(mydataframe1, on=""key"")\n\ndf3[\'s_gt_s\'] = df3.Start_y > df3.Start_x\ndf3[\'s_lt_e\'] = df3.Start_y < df3.End_x\ndf3[\'e_gt_s\'] = df3.End_y > df3.Start_x\ndf3[\'e_lt_e\'] = df3.End_y < df3.End_x\n\ndf3[\'s_in\'] = df3.s_gt_s & df3.s_lt_e\ndf3[\'e_in\'] = df3.e_gt_s & df3.e_lt_e\n\ndf3[\'overlaps\'] = df3.s_in | df3.e_in\n\nmy_new_dataframe = df3[df3.overlaps & df3.Remove==1][[\'End_x\',\'Start_x\']].drop_duplicates()\n\n', ""You could use pd.IntervalIndex for intersections\n\nGet rows to be removed\n\nIn [313]: dfr = df1.query('Remove == 1')\n\n\nConstruct IntervalIndex from to be removed ranges\n\nIn [314]: s1 = pd.IntervalIndex.from_arrays(dfr.Start, dfr.End, 'both')\n\n\nConstruct IntervalIndex from to be tested\n\nIn [315]: s2 = pd.IntervalIndex.from_arrays(df2.Start, df2.End, 'both')\n\n\nSelect rows of s2 which are not in s1 ranges\n\nIn [316]: df2.loc[[x not in s1 for x in s2]]\nOut[316]:\n   Start  End\n2    151  154\n\n\n\n\nDetails\n\nIn [320]: df1\nOut[320]:\n   Start  End  Remove\n0     50   60       1\n1     61  105       0\n2    106  150       1\n3    151  160       0\n4    161  180       1\n5    181  200       0\n6    201  400       1\n\nIn [321]: df2\nOut[321]:\n   Start  End\n0     55  100\n1    105  140\n2    151  154\n3    155  185\n4    220  240\n\nIn [322]: dfr\nOut[322]:\n   Start  End  Remove\n0     50   60       1\n2    106  150       1\n4    161  180       1\n6    201  400       1\n\n\nIntervalIndex details\n\nIn [323]: s1\nOut[323]:\nIntervalIndex([[50, 60], [106, 150], [161, 180], [201, 400]]\n              closed='both',\n              dtype='interval[int64]')\n\nIn [324]: s2\nOut[324]:\nIntervalIndex([[55, 100], [105, 140], [151, 154], [155, 185], [220, 240]]\n              closed='both',\n              dtype='interval[int64]')\n\nIn [326]: [x not in s1 for x in s2]\nOut[326]: [False, False, True, False, False]\n\n"", 'We can use Medial- or length-oriented tree: Overlap test:\n\nIn [143]: d1 = d1.assign(s=d1.Start+d1.End, d=d1.End-d1.Start)\n\nIn [144]: d2 = d2.assign(s=d2.Start+d2.End, d=d2.End-d2.Start)\n\nIn [145]: d1\nOut[145]:\n   Start  End  Remove    d    s\n0     50   60       1   10  110\n1     61  105       0   44  166\n2    106  150       1   44  256\n3    151  160       0    9  311\n4    161  180       1   19  341\n5    181  200       0   19  381\n6    201  400       1  199  601\n\nIn [146]: d2\nOut[146]:\n   Start  End   d    s\n0     55  100  45  155\n1    105  140  35  245\n2    151  154   3  305\n3    155  185  30  340\n4    220  240  20  460\n\n\nnow we can check for overlapping intervals and filter:\n\nIn [148]: d2[~d2[[\'s\',\'d\']]\\\n     ...:       .apply(lambda x: ((d1.loc[d1.Remove==1, \'s\'] - x.s).abs() <\n     ...:                         d1.loc[d1.Remove==1, \'d\'] +x.d).any(),\n     ...:              axis=1)]\\\n     ...:   .drop([\'s\',\'d\'], 1)\n     ...:\nOut[148]:\n   Start  End\n2    151  154\n\n', ""You can get all the unique range values from the columns marked Remove then evaluate the Start and End dates contained in mydataframe2 are not in any of the range values.  The first part will define all unique values falling with the Start/End values were Remove = 1.\n\nstart_end_remove = mydataframe1[mydataframe1['Remove'] == 1][['Start', 'End']].as_matrix()\nremove_ranges = set([])\nfor x in start_end_remove:\n    remove_ranges.update(np.arange(x[0], x[1] + 1))\n\n\nNext you can evaluate mydataframe2 against the unique set of range values.  If the Start/End values of mydataframe2 are in the range of values they are removed from the dataframe by flagging whether they should be removed in a new columns.  A function is defined to see if there is overlap between any of the ranges, then that function is applied to each row in mydataframe2 and remove the rows where the ranges do overlap.\n\ndef evaluate_in_range(x, remove_ranges):\n    s = x[0]\n    e = x[1]\n    eval_range = set(np.arange(s, e + 1))\n    if len(eval_range.intersection(remove_ranges)) > 0:\n        return 1\n    else:\n        return 0\n\nmydataframe2['Remove'] = mydataframe2[['Start', 'End']].apply(lambda x: evaluate_in_range(x, remove_ranges), axis=1)\nmydataframe2.drop(mydataframe2[mydataframe2['Remove'] == 1].index, inplace=True)\n\n"", 'I think that this should work:\n\nmydataframe2[mydataframe2.Start.isin(mydataframe1[mydataframe1.Remove != 0].Start)]\n\n\nBreaking it down:\n\n# This filter will remove anything which has Remove not 0\nfilter_non_remove = mydataframe1.Remove != 0\n\n# This provides a valid Sequence of Start values\nvalid_starts = mydataframe1[mydataframe1.Remove != 0].Start\n\n# Another filter, that checks whether the Start \n# value is in the valid_starts Sequence\nis_df2_valid = mydataframe2.Start.isin(valid_starts)\n\n# Final applied filter\noutput = mydataframe2[is_df2_valid]\n\n'"
Can one use comparisons to merge two pandas data-frames?,"'conditional_join from pyjanitor covers inequality joins efficiently :\nUsing @hernamesbarbara\'s fake data:\n# pip install pyjanitor\nimport pandas as pd\nimport janitor\n\n(df1.conditional_join(\n         df2, \n         (\'timepoint\', \'from_date\', \'>=\'), \n         (\'timepoint\', \'to_date\', \'<=\'))\n)\n \n         left              right                   \n    timepoint measure  from_date    to_date measure\n0  2014-01-03       5 2014-01-02 2014-01-06      89\n1  2014-01-03       5 2014-01-03 2014-01-07      80\n2  2014-01-04      73 2014-01-02 2014-01-06      89\n3  2014-01-04      73 2014-01-03 2014-01-07      80\n4  2014-01-04      73 2014-01-04 2014-01-05      44\n5  2014-01-05      40 2014-01-02 2014-01-06      89\n6  2014-01-05      40 2014-01-03 2014-01-07      80\n7  2014-01-05      40 2014-01-04 2014-01-05      44\n8  2014-01-05      40 2014-01-05 2014-01-12      68\n9  2014-01-06      45 2014-01-02 2014-01-06      89\n10 2014-01-06      45 2014-01-03 2014-01-07      80\n11 2014-01-06      45 2014-01-05 2014-01-12      68\n12 2014-01-06      45 2014-01-06 2014-01-11      62\n13 2014-01-08       2 2014-01-05 2014-01-12      68\n14 2014-01-08       2 2014-01-06 2014-01-11      62\n15 2014-01-08       2 2014-01-07 2014-01-14       5\n16 2014-01-08       2 2014-01-08 2014-01-09      23\n17 2014-01-09      96 2014-01-05 2014-01-12      68\n18 2014-01-09      96 2014-01-06 2014-01-11      62\n19 2014-01-09      96 2014-01-07 2014-01-14       5\n20 2014-01-09      96 2014-01-08 2014-01-09      23\n21 2014-01-10      82 2014-01-05 2014-01-12      68\n22 2014-01-10      82 2014-01-06 2014-01-11      62\n23 2014-01-10      82 2014-01-07 2014-01-14       5\n24 2014-01-11      61 2014-01-05 2014-01-12      68\n25 2014-01-11      61 2014-01-06 2014-01-11      62\n26 2014-01-11      61 2014-01-07 2014-01-14       5\n27 2014-01-12      68 2014-01-05 2014-01-12      68\n28 2014-01-12      68 2014-01-07 2014-01-14       5\n29 2014-01-13       8 2014-01-07 2014-01-14       5\n30 2014-01-14      94 2014-01-07 2014-01-14       5\n\n', ""I found a solution, I think. However, I am not sure if it is elegant and optimal:\ndf_1['A'] = 'A'\ndf_2['A'] = 'A'\ndf = pandas.merge(df_1, df_2, on=['A'])\ndf = df[(df['date'] >= df['from']) & (df['date'] < df['upto'])]\ndel df['A']\n\nPosted on behalf of the question asker\n"", 'pandasql is a pretty useful tool for querying pandas DataFrames using SQLite query syntax. \n\nResources\n\n\npandasql - PyPI Documentation\nyhat/pandasql - Source on Github\nBlog post with more examples\n\npip install -U pandasql\n\n\nHere\'s an example similar to the one you describe.\n\nImports\n\n#!/usr/bin/env python\n# -*- coding: utf-8 -*- \nimport pandas as pd\nfrom pandas.io.parsers import StringIO\nfrom pandasql import sqldf\n\n# helper func useful for saving keystrokes\n# when running multiple queries\ndef dbGetQuery(q):\n    return sqldf(q, globals())\n\n\nFake some data\n\nsample_a = """"""timepoint,measure\n2014-01-01 00:00:00,78\n2014-01-03 00:00:00,5\n2014-01-04 00:00:00,73\n2014-01-05 00:00:00,40\n2014-01-06 00:00:00,45\n2014-01-08 00:00:00,2\n2014-01-09 00:00:00,96\n2014-01-10 00:00:00,82\n2014-01-11 00:00:00,61\n2014-01-12 00:00:00,68\n2014-01-13 00:00:00,8\n2014-01-14 00:00:00,94\n2014-01-15 00:00:00,16\n2014-01-16 00:00:00,31\n2014-01-17 00:00:00,10\n2014-01-18 00:00:00,34\n2014-01-19 00:00:00,27\n2014-01-20 00:00:00,75\n2014-01-21 00:00:00,49\n2014-01-23 00:00:00,28\n2014-01-24 00:00:00,91\n2014-01-25 00:00:00,88\n2014-01-27 00:00:00,98\n2014-01-28 00:00:00,39\n2014-01-29 00:00:00,90\n2014-01-30 00:00:00,63\n2014-01-31 00:00:00,77\n""""""\n\nsample_b = """"""from_date,to_date,measure\n2014-01-02 00:00:00,2014-01-06 00:00:00,89\n2014-01-03 00:00:00,2014-01-07 00:00:00,80\n2014-01-04 00:00:00,2014-01-05 00:00:00,44\n2014-01-05 00:00:00,2014-01-12 00:00:00,68\n2014-01-06 00:00:00,2014-01-11 00:00:00,62\n2014-01-07 00:00:00,2014-01-14 00:00:00,5\n2014-01-08 00:00:00,2014-01-09 00:00:00,23\n""""""\n\n\nRead datasets to create 2 DataFrames\n\ndf1 = pd.read_csv(StringIO(sample_a), parse_dates=[\'timepoint\'])\ndf2 = pd.read_csv(StringIO(sample_b), parse_dates=[\'from_date\', \'to_date\'])\n\n\nWrite a SQL query\n\nNote that this one uses the SQLite BETWEEN operator. You can also swap that out and use something like ON timepoint >= from_date AND timepoint < to_date if you prefer.\n\nquery = """"""\nSELECT\n    DATE(df1.timepoint) AS timepoint\n    , DATE(df2.from_date) AS start\n    , DATE(df2.to_date) AS end\n    , df1.measure AS measure_a\n    , df2.measure AS measure_b\nFROM\n    df1 \nINNER JOIN df2\n    ON df1.timepoint BETWEEN \n        df2.from_date AND df2.to_date\nORDER BY\n    df1.timepoint;\n""""""\n\n\nRun the query using the helper func\n\ndf3 = dbGetQuery(query)\n\ndf3\n     timepoint       start         end  measure_a  measure_b\n0   2014-01-03  2014-01-02  2014-01-06          5         89\n1   2014-01-03  2014-01-03  2014-01-07          5         80\n2   2014-01-04  2014-01-02  2014-01-06         73         89\n3   2014-01-04  2014-01-03  2014-01-07         73         80\n4   2014-01-04  2014-01-04  2014-01-05         73         44\n5   2014-01-05  2014-01-02  2014-01-06         40         89\n6   2014-01-05  2014-01-03  2014-01-07         40         80\n7   2014-01-05  2014-01-04  2014-01-05         40         44\n8   2014-01-05  2014-01-05  2014-01-12         40         68\n9   2014-01-06  2014-01-02  2014-01-06         45         89\n10  2014-01-06  2014-01-03  2014-01-07         45         80\n11  2014-01-06  2014-01-05  2014-01-12         45         68\n12  2014-01-06  2014-01-06  2014-01-11         45         62\n13  2014-01-08  2014-01-05  2014-01-12          2         68\n14  2014-01-08  2014-01-06  2014-01-11          2         62\n15  2014-01-08  2014-01-07  2014-01-14          2          5\n16  2014-01-08  2014-01-08  2014-01-09          2         23\n17  2014-01-09  2014-01-05  2014-01-12         96         68\n18  2014-01-09  2014-01-06  2014-01-11         96         62\n19  2014-01-09  2014-01-07  2014-01-14         96          5\n20  2014-01-09  2014-01-08  2014-01-09         96         23\n21  2014-01-10  2014-01-05  2014-01-12         82         68\n22  2014-01-10  2014-01-06  2014-01-11         82         62\n23  2014-01-10  2014-01-07  2014-01-14         82          5\n24  2014-01-11  2014-01-05  2014-01-12         61         68\n25  2014-01-11  2014-01-06  2014-01-11         61         62\n26  2014-01-11  2014-01-07  2014-01-14         61          5\n27  2014-01-12  2014-01-05  2014-01-12         68         68\n28  2014-01-12  2014-01-07  2014-01-14         68          5\n29  2014-01-13  2014-01-07  2014-01-14          8          5\n30  2014-01-14  2014-01-07  2014-01-14         94          5\n\n'"
View dataframe while debugging in VS Code,"""Another way to do it if breakpoint is clunky:\ncommand + shift + p (mac os)\n'Jupyter: Open variables view' then click on the small two boxes with the arrow.\nPersonally, I find this easier and you could create a custom shortcut to bring  up the 'Jupyter: Open variables view' for speed\n"", 'Here\'s a way to do it on vs code - https://github.com/microsoft/vscode-jupyter/issues/1286#issuecomment-765059459\nNeatly shown in GIF on Git Hub.\n', 'Microsoft VSCode team finally made this feature available with the January 2021 product update. More details could be found in official blog\nIt works like a charm and is very intuitive. In short:\n\nSet up a break point (by clicking at the left most point of code area, before line number)\nStart debugging (Run menu at top have Start Debugging option)\nWhen debugger stops at the debug point, find the required dataframe inside VARIABLES panel. (VARIABLES panel is inside Run and Debug area)\nRight click on dataframe and select option View Value in Data Viewer. TADA :)\n\n', 'Two more options for vscode are the following ones:\n\njupyter notebooks\nsaving to CSV and using the edit csv extension\n\nBoth require more effort but the view is more helpful.\n', ""you can use the view() function from xlwings library. It will show you the DataFrame in Excel:\nimport pandas as pd\nfrom xlwings import view\n\ndf = pd.DataFrame({'A':[1,2], 'B':[3,4]})\nview(df)\n\nA better way would be to convert the function to pandas method:\nfrom pandas.core.base import PandasObject\nPandasObject.view = view\n\nnow you only need to type:\ndf.view()\n\n"", 'My solution for viewing DataFrames in a tabular format while debugging is to simply copy and paste them into an Excel spreadsheet using\ndf.to_clipboard()\n\nfrom the debug console. Even some of my colleagues running PyCharm are using this technique, since it gives you way more flexibility to inspect your data.\n', 'You can now print the DataFrame in the DEBUG CONSOLE:\n\nFrom the Github issue mentioned in @Christina Zhou\'s answer.\n', 'It seems like currently you can do it only using the Jupyter notebook in VS Code, using the variables explorer.\n\n', ""The interactive shell looks like a good start.  Right click the .py file in your explorer. You'll be able to view pandas dataframes from there.\n"", 'So it looks like this isn\'t a thing right now in VS Code.\n\nIf anyone wants to show their support for the development of this feature, I found this open issue here:\nhttps://github.com/microsoft/vscode-python/issues/7063\n'"
How to convert DataFrame.append() to pandas.concat()?,"'You can bring it back by creating a module\nimport pandas as pd\n\n\ndef my_append(self, x, ignore_index=False):\n    if ignore_index:\n        return pd.concat([self, x])\n    else:\n        return pd.concat([self, x]).reset_index(drop=True)\n\n\nif not hasattr(pd.DataFrame, ""append""):\n    setattr(pd.DataFrame, ""append"", my_append)\n\n\nThis will add the implementation and can be tested as follows\nimport pandas as pd\nimport lib.pandassupport\n\n\ndef test_append_ignore_index_is_true():\n    df = pd.DataFrame(\n        [\n            {""Name"": ""John"", ""Age"": 25, ""City"": ""New York""},\n            {""Name"": ""Emily"", ""Age"": 30, ""City"": ""San Francisco""},\n            {""Name"": ""Michael"", ""Age"": 35, ""City"": ""Chicago""},\n        ]\n    )\n    new_row = pd.DataFrame([{""Name"": ""Archie"", ""Age"": 27, ""City"": ""Boston""}])\n    df = df.append(new_row, ignore_index=True)\n    print(df)\n    assert df.equals(\n        pd.DataFrame(\n            [\n                {""Name"": ""John"", ""Age"": 25, ""City"": ""New York""},\n                {""Name"": ""Emily"", ""Age"": 30, ""City"": ""San Francisco""},\n                {""Name"": ""Michael"", ""Age"": 35, ""City"": ""Chicago""},\n                {""Name"": ""Archie"", ""Age"": 27, ""City"": ""Boston""},\n            ],\n            [0, 1, 2, 0],\n        )\n    )\n\n\ndef test_append():\n    df = pd.DataFrame(\n        [\n            {""Name"": ""John"", ""Age"": 25, ""City"": ""New York""},\n            {""Name"": ""Emily"", ""Age"": 30, ""City"": ""San Francisco""},\n            {""Name"": ""Michael"", ""Age"": 35, ""City"": ""Chicago""},\n        ]\n    )\n    new_row = pd.DataFrame([{""Name"": ""Archie"", ""Age"": 27, ""City"": ""Boston""}])\n    df = df.append(new_row)\n    assert df.equals(\n        pd.DataFrame(\n            [\n                {""Name"": ""John"", ""Age"": 25, ""City"": ""New York""},\n                {""Name"": ""Emily"", ""Age"": 30, ""City"": ""San Francisco""},\n                {""Name"": ""Michael"", ""Age"": 35, ""City"": ""Chicago""},\n                {""Name"": ""Archie"", ""Age"": 27, ""City"": ""Boston""},\n            ],\n            [0, 1, 2, 3],\n        )\n    )\n\n', 'There is another unpleasant edge case here: If input_vars is a series (not a dataframe) that represents one row to be appended to features, the deprecated use of features = features.append(input_vars) works fine and adds one row to the dataframe.\nBut the version with concat features = pd.concat([features, input_vars]) does something different and produces lots of NaNs. To get this to work, you need to convert the series to a dataframe:\nfeatures = pd.concat([features, input_vars.to_frame().T])\n\nSee also this question: Why does concat Series to DataFrame with index matching columns not work?\n', ""For example, you have a list of dataframes called collector, e.g. for cryptocurrencies, and you want to harvest first rows from two particular columns from each datafarme in our 'collector'. You do as follows\npd.concat([cap[['Ticker', 'Market Cap']].iloc[:1] for cap in collector] )\n\n"", ""You can store the DataFrames generated in the loop in a list and concatenate them with features once you finish the loop.\nIn other words, replace the loop:\nfor count in range(num_samples):\n    # .... code to produce `input_vars`\n    features = features.append(input_vars)        # remove this `DataFrame.append`\n\nwith the one below:\ntmp = []                                  # initialize list\nfor count in range(num_samples):\n    # .... code to produce `input_vars`\n    tmp.append(input_vars)                        # append to the list, (not DF)\nfeatures = pd.concat(tmp)                         # concatenate after loop\n\nYou can certainly concatenate in the loop but it's more efficient to do it only once.\n"", 'This will ""append"" the blank df and prevent errors in the future by using the concat option\nfeatures= pd.concat([features, input_vars])\n\nHowever, still, without having access to actually data and data structures this would be hard to test replicate.\n'"
Does XGBoost Classifier Model pay attention to all X-data entries when making the prediction? Or only same indexed X-data?,'\nDoes XGBoost Classifier Model pay attention to all X-data entries when making the prediction?\n\nNo. Each row is classified separately.\n'
Calculating the correlation of each row with one in Group in Python,"'The following will give you the desired output, using groupby.apply() with a function:\ndf = df2.loc[:, ~df2.columns.isin(\n    [""Worker_in_dpt"", ""mean_salary_per_period""])] \\\n    .set_index([\'head_department\', \'serial_number\']).stack(dropna=False)\n\ndef func(frame):\n    # unstack for serial number as columns\n    frame = frame.unstack(level=[""serial_number""])\n    # correlations and take first row\n    corr = frame.corr().iloc[0]\n    # find months that are nan for serial number 1 (first column)\n    nan = frame.iloc[:, 0].isna()\n    # mean of non-nans\n    mean_not_nan = frame.loc[~nan, :].mean(axis=0)\n    # if nan, mean for these, else same as non-nans\n    if nan.sum() > 0:    \n        mean_nan = frame.loc[nan, :].mean(axis=0)\n    else:\n        mean_nan = mean_not_nan.copy()\n    # sum for january\n    sum_jan = pd.Series(data=frame.loc[\n        frame.index.get_level_values(-1).str.contains(""jan"")].sum(axis=1)[0],\n        index=frame.columns)\n    \n    # concat all series and name columns\n    return pd.concat([\n        corr, mean_nan, mean_not_nan, sum_jan], axis=1) \\\n        .set_axis(\n            [\'corr with head in dpt\',\n             \'mean _when_1_in _NAN\',\n             \'mean _when_1_in_NAN\',\n             \'sum all dpt in Jan\'], axis=1)\n\n# groupby head_department and apply func\nadditional_cols = df.groupby(level=""head_department"").apply(func)\n\n# merge df2 with grouped outputs\nout = pd.merge(df2, additional_cols,\n               left_on=[\'head_department\', \'serial_number\'],\n               right_index=True)\n\n'"
Add string Column B where string exists in column Columns A + np.where() + pandas,"'Use map:\ndmap = {\'1\': \'One\', \'2\': \'Two\', \'3\': \'Three\', \'4\': \'Four\'}\n\ndf[\'Add\']  = df[\'Example\'].map(dmap).fillna(\'\')\n\nOutput:\n>>> df\n  Example    Add\n0       1    One\n1       2    Two\n2       3  Three\n3       4   Four\n\n'"
XLSX Writer num_format function is not visually appearing in excel,"'The issue here is that Pandas sets a cell format for datetimes and that overrides the column format. Instead you can set the default date format pd.ExcelWriter() parameters:\nimport pandas as pd\nfrom datetime import datetime, date\n\n# Create a Pandas dataframe from some datetime data.\ndf = pd.DataFrame(\n    {\n        ""Date and time"": [\n            datetime(2023, 7, 11, 11, 30, 55),\n            datetime(2023, 7, 12, 1, 20, 33),\n            datetime(2023, 7, 13, 11, 10),\n            datetime(2023, 7, 14, 16, 45, 35),\n            datetime(2023, 7, 15, 12, 10, 15),\n        ],\n        ""Dates only"": [\n            date(2023, 7, 11),\n            date(2023, 7, 12),\n            date(2023, 7, 13),\n            date(2023, 7, 14),\n            date(2023, 7, 15),\n        ],\n    }\n)\n\n\n# Create a Pandas Excel writer using XlsxWriter as the engine.\n# Also set the default datetime and date formats.\nwriter = pd.ExcelWriter(\n    ""pandas_datetime.xlsx"",\n    engine=""xlsxwriter"",\n    datetime_format=""[$-en-US,1]dd-mm-yy"",\n    date_format=""[$-en-US,1]dd-mm-yy"",\n)\n\n# Convert the dataframe to an XlsxWriter Excel object.\ndf.to_excel(writer, sheet_name=""Sheet1"")\n\n# Get the xlsxwriter workbook and worksheet objects in order\n# to set the column widths, to make the dates clearer.\nworkbook = writer.book\nworksheet = writer.sheets[""Sheet1""]\n\n# Get the dimensions of the dataframe.\n(max_row, max_col) = df.shape\n\n# Set the column widths, to make the dates clearer.\nworksheet.set_column(1, max_col, 20)\n\n# Close the Pandas Excel writer and output the Excel file.\nwriter.close()\n\n\nOutput:\n\n'"
Pandas Dataframe merge to get only non-existing records,"'When using a merge to align two DataFrames, you can avoid suffixes by just slicing the merging columns:\ncols = [\'symbolid\', \'timeframeid\', \'datetime\']\n\ndf2 = (df.merge(df_existing[cols],\n                on=cols, how=\'left\',\n                indicator=True)\n         .query(\'_merge == ""left_only""\')\n         .drop(columns = \'_merge\')\n       )\n\nAlternative with pop and loc to filter and drop in a single step:\ncols = [\'symbolid\', \'timeframeid\', \'datetime\']\n\ndf2 = (df.merge(df_existing[cols],\n                on=cols, how=\'left\',\n                indicator=True)\n         .loc[lambda d: d.pop(\'_merge\').eq(\'left_only\')\n     )\n\n'"
How do I get the last record in a groupby() in pandas?,"'One option is to use groupby.tail:\ndf.groupby(\'student\').tail(1)\n\nOutput:\n  student  value_a  timestamp\n2       A      NaN          4\n1       B      5.0          6\n\nNote that if you want the last timestamp, another option is to index with groupby.idxmax:\ndf.loc[df.groupby(\'student\')[\'timestamp\'].idxmax()]\n\n', ""You have to sort your dataframe first if you choose to use nth or tail. After that you can drop the dupes:\n>>> df.sort_values('timestamp').drop_duplicates('student', keep='last')\n  student  value_a  timestamp\n2       A      NaN          4\n1       B      5.0          6\n\n"", 'You can try .nth:\nout = df.groupby(\'student\').nth(-1)\nprint(out)\n\nPrints:\n         value_a  timestamp\nstudent                    \nA            NaN          4\nB            5.0          6\n\n'"
DateFormatter is bringing 1970 as year not the original year in the dataset,"'I was having the same issue. The solution is really contained in the answer from @Ruthger Righart which I had trouble understanding at first, but the key change is  using matplotlib pyplot plotting instead of the pandas plotting as mentioned here and in the link in the comments mentioned by @Trenton McKinney.  First filter the data to the years you want to plot then call the plot like @Ruthger Righart did:\nimport matplotlib.pyplot as plt\nfig,ax = plt.subplots()\nax.bar(df_month[\'Date\'], df_month[\'Volume\'])\n\nAdd the labels as you did originally.\n', ""The following gives the right x-axis labels.\nImport modules\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.dates import DateFormatter\nimport matplotlib.dates as mdates\n\nExample data\ndf_month = pd.DataFrame({'Date':['2006-01-03', '2006-02-04', '2006-02-08'], 'Volume':[24232729, 20553479, 20500000]}) # '2006-01-03', '2006-01-04'\n\ndf_month['Date'] = pd.to_datetime(df_month['Date'])\n\nPlotting\nfig,ax = plt.subplots()\nax.set_ylabel(""Volume"")\nax.set_title(""Volume"")\n\nax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\nax.bar(df_month['Date'], df_month['Volume'])\n\nplt.xticks(df_month['Date'], rotation=90)\nplt.show()\n\n"""
How to join 2 dataframes and pivot them,"""Another possible solution:\n(pd.concat([dfPts, dfICU])\n .pivot(index=['PtsID', 'VisitID'], columns='ICU', values='Frequency')\n .dropna(how='all').reset_index().fillna(0))\n\nOutput:\nICU  PtsID  VisitID   A1   A2   A3   B1  B2Closed  B2Covid   B7  C1West  \\\n0    105.0     44.0  0.0  0.0  0.0  0.0       0.0      0.0  0.0     0.0   \n1    105.0     80.0  0.0  0.0  0.0  0.0       0.0      0.0  8.0     0.0   \n2    882.0     35.0  0.0  9.0  0.0  0.0       0.0      0.0  0.0     0.0   \n3    882.0     91.0  0.0  0.0  0.0  0.0       0.0      0.0  0.0     0.0   \n4    934.0     15.0  0.0  0.0  4.0  0.0       0.0      0.0  0.0     0.0   \n5    934.0     62.0  0.0  6.0  0.0  0.0       0.0      5.0  0.0     0.0   \n\nICU  C2South   C3  P53Child  \n0        2.0  0.0       0.0  \n1        0.0  0.0       0.0  \n2        7.0  2.0       0.0  \n3        0.0  0.0       5.0  \n4        2.0  0.0       0.0  \n5        0.0  0.0       0.0 \n\n"", ""Since dfICU only contains a list of values that are already present in the column ICU in dfPts, you only need to use dfPts for the pivot table you want.\nBefore creating the pivot table, make sure Frequency is numeric:\n# get info of columns\ndfPts.info()\n\nThen create pivot table:\n# create a pivot table\npivot_table = (pd.pivot_table(dfPts, index=['PtsID','VisitID'], \n               columns='ICU', values='Frequency',\n               aggfunc='sum') # you can change the aggregation function \n               #.reset_index() # uncomment if you want indices as columns \n               )\n\n# check pivot table\npivot_table.head(10)\n\nIf you find that some ICUs from dfICU are missing in the pivot table columns, you can add them with 0 values like this:\n# create list of missing columns\nICU_list = list(dfICU.ICU.unique())\nmissing_cols = [col for col in ICU_list \n                if col not in pivot_table.columns]\n\n# add missing columns to pivot table\nfor col in missing_cols:\n    pivot_table[col] = 0\n\n# check pivot table\npivot_table.head()\n\n"", ""Pivot the dataframe then reindex to ensure all the icus are present in the column headers, then fill the values in missing icus with 0\nicus = dfICU['ICU'].unique()\n(\n    dfPts\n    .pivot(index=['PtsID', 'VisitID'], columns='ICU', values='Frequency')\n    .reindex(columns=icus)\n    .fillna(0).reset_index()\n)\n\nResult\nICU  PtsID  VisitID   A1   A2   A3   B1  B2Closed  B2Covid   B7  C1West  C2South   C3  P53Child\n0      105       44  0.0  0.0  0.0  0.0       0.0      0.0  0.0     0.0      2.0  0.0       0.0\n1      105       80  0.0  0.0  0.0  0.0       0.0      0.0  8.0     0.0      0.0  0.0       0.0\n2      882       35  0.0  9.0  0.0  0.0       0.0      0.0  0.0     0.0      7.0  2.0       0.0\n3      882       91  0.0  0.0  0.0  0.0       0.0      0.0  0.0     0.0      0.0  0.0       5.0\n4      934       15  0.0  0.0  4.0  0.0       0.0      0.0  0.0     0.0      2.0  0.0       0.0\n5      934       62  0.0  6.0  0.0  0.0       0.0      5.0  0.0     0.0      0.0  0.0       0.0\n\n"""
"Using Pandas to drop columns, in a specific row that don&#39;t contain a specific substring","'You could do something like this:\nfrom numpy import nan\nimport pandas as pd\n\ndata_df = pd.DataFrame(\n    [\n        [""a"", nan, ""empty"", ""empty"", ""empty""],\n        [""b"", nan, """", """", """"],\n        [""c"", nan, """", """", """"],\n        [""d"", nan, nan, """", """"],\n        [""e"", ""e.Description"", ""e.Value"", ""e.Attribute"", ""e.Variable""],\n        [""f"", nan, nan, nan, nan]\n    ]\n)\nfor column_name in data_df.columns:\n    data_df[column_name] = data_df[column_name].str.replace(""\\.Value|\\.Attribute|\\.Variable"", """", regex=True)\n\nThe results looks like:\n   0              1      2      3      4\n0  a            NaN  empty  empty  empty\n1  b            NaN                     \n2  c            NaN                     \n3  d            NaN    NaN              \n4  e  e.Description      e      e      e\n5  f            NaN    NaN    NaN    NaN\n\nThe trick is that you use the str attribute at the column to be able to use the replace method including  regular expression\n', ""Use the pandas.to_dict() method to convert your DataFrame to a dict, and then convert it back to its original state.\nWith df as the variable for your DataFrame:\nimport pandas\n\n\ntitles = df.loc[:0]\nmy_dict = df.to_dict()\n\nfor x in titles:\n    if my_dict[x][3] == 'e.Description' or my_dict[x][3] == 'e.Variable':\n        my_dict.pop(x)\n    elif x != 0:\n        my_dict[x][3] = 'e'\n    \n\nnew_df = pandas.DataFrame.from_dict(my_dict, index=False)\n\n\nprint(new_df)\n\nThe result should be the same, if the numbers themselves are indexes. However, if you want extra indexes, use my previous version of my answer. Otherwise, this should meet your needs.\nThanks for the question!\n"""
Ignoring FutureWarnings in Python pandas,"'To only suppress FutureWarning messages:\nimport warnings\nwarnings.simplefilter(action=\'ignore\', category=FutureWarning)\n\n', 'From my little research, pandas seems to have some tricky way of doing it. have you tried import warnings;   warnings.filterwarnings(""ignore"")?\nYou can also check this thread. And if the warning persists just let it be, it won\'t stop your code from running.\n'"
Calculate business days by iterating over columns,"'Assuming the dates are already of datetime type*, use numpy.busday_count and broadcasting/masking:\nimport numpy as np\n\ntmp = df.filter(regex=r\'\\d{4}-\\d{2}\')\ndf[tmp.columns] = np.where(tmp.notna(),\n                           np.busday_count(df[\'Event_Date\'].to_numpy(dtype=\'datetime64[D]\')[:,None],\n                                           tmp.fillna(\'0\').to_numpy(dtype=\'datetime64[D]\')),\n                           np.nan)\n\n* else convert with cols = df.drop(columns=\'Account\').columns ; df[cols] = df[cols].apply(pd.to_datetime, dayfirst=True).\nOutput:\n  Account Event_Date  2023-01  2023-02  2023-03  2023-04  2023-05  2023-06  2023-07\n0       A 2023-04-25    -77.0      NaN      NaN      NaN      NaN      NaN      NaN\n1       B 2023-06-02      NaN      NaN      NaN     21.0     51.0      NaN      NaN\n2       C 2023-04-25      NaN      NaN      NaN      NaN      7.0      NaN      NaN\n\nUsed input:\nfrom pandas import Timestamp\n\ndf = pd.DataFrame({\'Account\': [\'A\', \'B\', \'C\'],\n                   \'Event_Date\': [Timestamp(\'2023-04-25 00:00:00\'), Timestamp(\'2023-06-02 00:00:00\'), Timestamp(\'2023-04-25 00:00:00\')],\n                   \'2023-01\': [Timestamp(\'2023-01-06 00:00:00\'), NaT, NaT],\n                   \'2023-02\': [NaT, NaT, NaT],\n                   \'2023-03\': [NaT, NaT, NaT],\n                   \'2023-04\': [NaT, Timestamp(\'2023-07-01 00:00:00\'), NaT],\n                   \'2023-05\': [NaT, Timestamp(\'2023-08-12 00:00:00\'), Timestamp(\'2023-05-04 00:00:00\')],\n                   \'2023-06\': [NaT, NaT, NaT],\n                   \'2023-07\': [NaT, NaT, NaT]})\n\n'"
How do I turn a list of times in a list into seconds in pandas/jupyter notebook,"""I believe you are almost there, you need to convert the time strings in romulus to proper datetime objects and then find the differences and output the difference in seconds.  Here is how to update your code:\nfrom datetime import datetime\n\ndef get_rom(romulus):\n    rom_list = []\n    \n    for i in range(len(romulus)):\n        diff = datetime.strptime(romulus[i-1], '%Y/%m/%d %H:%M:%S') - datetime.strptime(romulus[i], '%Y/%m/%d %H:%M:%S') \n        \n        diff_in_seconds = diff.seconds\n        \n        rom_list.append(diff_in_seconds)\n        \n    return rom_list\n\nThen ```get_rom(romulus)\n[770, 86185, 86131, 86276, 86238]\n\nA more succinct approach using list comprehension would look like:\ndef get_diff_in_seconds(rl: list[str]) -> list[int]:\n    diff = [(datetime.strptime(rl[x-1], '%Y/%m/%d %H:%M:%S') - datetime.strptime(rl[x], '%Y/%m/%d %H:%M:%S')) for x in range(1, len(rl)) ]\n    return [x.seconds for x in diff]\n\n"""
Pandas ewm conditional variable span,"'With the dataframe you provided:\nimport pandas as pd\n\ndf = pd.DataFrame({""A"": [1, 1, 2, 4, 1, 3], ""B"": [2, 2, 3, 1, 1, 1]})\n\nHere is one way to do it with Pandas cumsum and iterrows:\nALPHA = 0.9\nTHRESHOLD = 4\n\nmask = df[""A""].cumsum() >= THRESHOLD\n\nnew_df = df[mask].reset_index(drop=True)\nnew_df.at[0, ""EWMA""] = new_df.at[0, ""B""]\n\nfor i, row in new_df.iterrows():\n    if not i:\n        continue\n    new_df.at[i, ""EWMA""] = (\n        ALPHA * new_df.at[i, ""B""] + (1 - ALPHA) * new_df.at[i - 1, ""EWMA""]\n    )\n\nnew_df = pd.concat([df[~mask], new_df], ignore_index=True)\n\nThen:\nprint(new_df)\n# Output\n\n   A  B   EWMA\n0  1  2    NaN\n1  1  2    NaN\n2  2  3  3.000\n3  4  1  1.200\n4  1  1  1.020\n5  3  1  1.002\n\n'"
sklearn predict_proba function only returns probability of 1 (binary classification),"""This is due to some overfitting. I used GridSearchCV to adjust my hyperparameters (max_depth in particular) and now it's far better\n"""
How can I find the number of times a set of ranges exists in a dataset?,"'A much faster method of computation is using various numpy methods.\n# array of all valid permutations of x and y\narr_xy = np.array(np.meshgrid(lb.round(2), ub.round(2))).T.reshape(-1, 2)\narr_xy = arr_xy[arr_xy[:, 0] < arr_xy[:, 1]]\n\n# lbExists - (x >= row[0] and x <= row[1])\nlbExists = (arr_xy[:, 0][:, np.newaxis] >= a[:, 0]) * \\\n    (arr_xy[:, 0][:, np.newaxis] <= a[:, 1])\n# rangeExists - (x >= row[0] and y <= row[1])\nrangeExists = lbExists * (arr_xy[:, 1][:, np.newaxis] <= a[:, 1])\n\n# project based off of lbExists initially (else 0)...\nproject = np.where(lbExists,\n                   # then on value of rangeExists\n                   np.where(rangeExists,\n                            # (y - x) * factor if true\n                            np.tile(np.diff(arr_xy) * factor, a.shape[0]),\n                            # else -1 * factor\n                            np.full(rangeExists.shape, -factor)),\n                   0)\n\n# combine variables\narr_out = np.hstack([\n    # permutations of upper and lower bound\n    np.vstack([arr_xy] * a.shape[0]),\n    # repeated values of Min and Max\n    np.repeat(a, arr_xy.shape[0], axis=0),\n    # lbExists 2d -> 1d\n    lbExists.T.reshape(-1)[:, np.newaxis],\n    # rangeExists 2d -> 1d\n    rangeExists.T.reshape(-1)[:, np.newaxis],\n    # project 2d -> 1d\n    project.T.reshape(-1)[:, np.newaxis].round(2)])\n\nI have added comments throughout the code, but do ask if there is anything that needs clarifying.\nThe difference in timings (about x100 faster):\n# your loop in the question\n5.84 s ± 295 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n# this method\n50.2 ms ± 3.49 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\nAnd to confirm that these are the same:\n(arr_out==np.array(arry)).all()\n#Out: True\n\n'"
rename returns NoneType object,"'None is the expected return value when you use the inplace option. The reason your column is not being renamed is because a dict as a positional argument attempts to rename rows.\nFor example (omitting inplace=True for simplicity):\n>>> import pandas\n>>> df = pandas.DataFrame({""A"": [1,2,3], ""B"": [4,5,6]})\n>>> df\n   A  B\n0  1  4\n1  2  5\n2  3  6\n>>> df.rename({0: 9})\n   A  B\n9  1  4\n1  2  5\n2  3  6\n>>> df.rename({""A"": ""A1""})\n   A  B\n0  1  4\n1  2  5\n2  3  6\n>>> df.rename(columns={""A"": ""A1""})\n   A1  B\n0   1  4\n1   2  5\n2   3  6\n\nAs you can see, a row named ""A"" would be renamed, if it existed. Use the columns keyword argument to rename columns.\n', ""The parameter inplace causes the error. If you use inplace=True, then dataframe changes in place. inplace returns None, hence the error.\nYou can solve this problem in two ways:\n\ngeneral.rename(columns={'Species': 'Common Name'}, inplace=True)\n\ngeneral = general.rename(columns={'Species': 'Common Name'})\n\n\n"", ""you are using inplace=True, which means the method work in place and it will return None. In any case, all you want is to specify the correct axis in rename to get what you want:\nIn [24]: general.rename({'Species': 'Common Name'}, axis=1)\nOut[24]:\n           Common Name  Count\n0 Downy     Woodpecker      1\n1 Northern     Flicker      1\n2 Eastern     Kingbird      2\n\n"""
Condition check based on Pandas time duration,"""You don't need to use sum or mean here instead you just need to check if all of the values are same or not which can be achieved using something like below (Im assuming you have created a rolling window with the power column\ndf['check'] = rolling_window.apply(lambda x: len(set(x)) == 1)\n\n"""
How do I make a horizontal data list print into a series of columns?,"""From your question, it looks like your data in the CSV file is not properly formatted. Hence pd.read_csv() will most likely not work out of the box. Normally in a CSV file, the columns should be separated by commas, but in your case, they are separated by semicolons and the column names are included with the data itself.\nA possible solution to this is to read your data as a simple text file first, parse each line to split the values at the semicolons, then extract the column names and data from each split part. Here's an example which may work.\nimport pandas as pd\n\nN = []\n\nfor f in csv_files:\n    # read the file as a text file\n    with open(f, 'r') as file:\n        data = []\n        for line in file:\n            # split the line at semicolons\n            parts = line.split("";"")\n            row = {}\n            for part in parts:\n                # split the part at the equals sign\n                name_value = part.split(""="")\n                if len(name_value) == 2:\n                    # the part is a column name and value\n                    name = name_value[0].strip()\n                    value = name_value[1].strip()\n                    row[name] = value\n            data.append(row)\n    # convert the data to a DataFrame\n    df = pd.DataFrame(data)\n    N.append(df)\n\n"""
Create empty pandas dataframe from pandera DataFrameModel,"'The current pandera docs have small section on pandas data types\nThis suggests the following solution:\nimport pandera as pa\nimport pandas as pd\n\ndef empty_dataframe_from_model(Model: pa.DataFrameModel) -> pd.DataFrame:\n    schema = Model.to_schema()\n    return pd.DataFrame(columns=schema.dtypes.keys()).astype(\n        {col: str(dtype) for col, dtype in schema.dtypes.items()}\n    )\n\n', 'Yes, it is possible to create empty pandas dataframe using pandera schema with the help of the function schema.to_dataframe().\nHere is the updated version of the function get_empty_df_of_schema\ndef get_empty_df_of_schema(schema: pa.DataFrameModel) -> pd.DataFrame:\n    row_empty = schema({}).astype(str).iloc[0]\n    return pd.DataFrame(columns=row_empty.index).astype(row_empty.to_dict())\n\nAlso, have a look at dataframes schemas through the following link\n'"
AttributeError: can&#39;t delete attribute,"""if this doesn't work:  del df2.index.name\nYou can run this:  df2.index.name = None\n"", 'Use DataFrame.rename_axis:\n\ndf2 = df2.rename_axis(None, axis=1)\n\n', 'A version of this answer. \n\nPandas by default needs column names. \n\nSo you could do something like this to hide it\n\ndf2.rename(columns={\'Type\': \'\'}, inplace=True)\n\n'"
Creating two loops that run simultaneously for a specific directory and extract JSON data,"""Judging from your input examples, after this\nX = json_data.get('X')\nY = json_data.get('Y')\nA = json_data.get('A')\nparameters = json_data.get('parameters')\n\neither X and Y are not None, or parameters is not None.\nThen this code snippet is executed:\nif parameters is not None:\n    B = parameters.get('B')\n    C = parameters.get('C')\n    if B is not None and C is not None:\n        data.append({'A': A, 'B': B, 'C': C, 'X': X, 'Y': Y})\n\nHere it's important that data.append ONLY happens if parameters is not None. So all the entries where X and Y are not None are getting skipped. And your dataset only gets None values for X and Y.\nWhat you probably can do is append the data each time or check if you received at least one not None value for a row. Depends on your task logic. For example:\nfor json_file in json_files:\n    with open(os.path.join(path_to_json, json_file)) as file:\n        json_data = json.load(file)\n        X = json_data.get('X')\n        Y = json_data.get('Y')\n        A = json_data.get('A')\n        parameters = json_data.get('parameters')\n        if parameters is not None:\n            B = parameters.get('B')\n            C = parameters.get('C')\n\n    # your task logic here:\n    if (B is not None and C is not None) or (X is not None and Y is not None):\n        data.append({'A': A, 'B': B, 'C': C, 'X': X, 'Y': Y})\n\ndf = pd.DataFrame(data, columns=['A', 'B', 'C', 'X', 'Y'])\n\nThat would result in\nA                 B      C     X     Y\n20220821_174040   1000   0     None  None\n20220821_174040   None   None  [..]  [..]\n\nI am guessing you then would want to unite the rows for the same A value.\ndef get_not_none_value(values):\n    return next(filter(lambda x: x is not None, values))\n\ndf \\\n    .groupby(""A"", as_index=False) \\\n    .agg(get_not_na_value)\n\nOutput:\n                 A       B    C       X       Y\n0  20220821_174040  1000.0  0.0     [..]   [..]\n\n"", ""If you want simultaneous processing of your JSON files (except those with 'XYZ' in their name) then you should consider multithreading.\nSomething like this:\nfrom concurrent.futures import ThreadPoolExecutor\nimport json\nimport glob\nimport os\nimport pandas\n\ndata = []\n\ndef process(filename):\n    if not 'XYZ' in os.path.basename(filename):\n        with open(filename) as jf:\n            jdata = json.load(jf)\n            data.extend(zip(jdata['X'], jdata['Y']))\n\npath_to_json = 'your_path_goes_here'\n\nwith ThreadPoolExecutor() as executor:\n    files = glob.glob(os.path.join(path_to_json, '*.json'))\n    executor.map(process, files)\n\ndf = pandas.DataFrame(data, columns=['X', 'Y'])\n\n"""
Python- Unable to write to a csv-‘Type Error: “Delimiter” must be a 1-character string,"'Pandas to_csv() method supports only a single-character string as a delimiter, the same is true for Python standard CSV library. So you should choose some single-digit delimiter and than write a script which will read the csv file (as a plain text) and replace chosen delimiters to ‘~|~’ or ‘~|’\n'"
Knn algorithm in regression,"""df_raw.info()\ndf_raw.describe()\n\ndf_corr = df_raw.corr()[['Age']].sort_values(by = 'Age')\nsns.heatmap(df_corr, annot = True)\n\nplt.title('Histogram for xx')\nplt.hist(x = df_raw['Age'])\ndf_raw['Age'].hist()\nplt.show()\n\nsns.boxplot(x = df_raw['SibSp'], y = df_raw['Pclass'])\n\ncounts = df_raw['SibSp'].value_counts()[0]\ncounts\n\ndf_clean = df_raw[df_raw['SibSp'] != 0]\ndf_clean\n\ndf_clean.reset_index(drop=True)\n\navg = df_clean['SibSp'].median()\ndf_raw['SibSp'].replace(0, avg, inplace = True)\ndf_raw['SibSp'].value_counts()\n\ndf_raw['SibSp'].fillna(avg, inplace = True)\ndf_raw['SibSp'].dropna()\n\nlinear reggression\nfrom sklearn.linear_model import LinearRegression\n\nlm = LinearRegression()\n\ndf_clean = df_raw[['age', 'height_cm', 'weight_kg']].dropna()\ndf_predictor = df_clean[['height_cm', 'weight_kg']].copy()\ndf_target = df_clean['age'].copy()\n\nlm.fit(df_predictor, df_target)\n\nfrom sklearn.linear_model import LinearRegression\n\nlm = LinearRegression()\n\ndf_clean = df_raw[['age', 'height_cm', 'weight_kg']].dropna()\ndf_predictor = df_clean[['height_cm', 'weight_kg']].copy()\ndf_target = df_clean['age'].copy()\n\nlm.fit(df_predictor, df_target)\n\ndf_cln1 = df_raw.copy()\ndf_impute = pd.DataFrame(lm.predict(df_raw[['height_cm', 'weight_kg']]))\ndf_impute.rename({0: 'age'}, axis = 1, inplace = True)\ndf_cln1.fillna(df_impute, inplace = True)\nprint(df_cln1.isna().sum())\n\nfeature engineering\nfrom sklearn.preprocessing import OneHotEncoder\n\ncategorical_variables = ['sex', 'fracture', 'medication']\ndata_cat = df_raw[categorical_variables]\n\ncat_encoder = OneHotEncoder()\ndata_cat_onehot = cat_encoder.fit_transform(data_cat)\ndata_cat_onehot_df = pd.DataFrame(data_cat_onehot.toarray())\n\ndf_temp = pd.concat([df_raw, data_cat_onehot_df], axis = 1)\ndf_temp = df_temp.drop(categorical_variables, axis = 1)\ndf_eng = df_temp.copy()\ndf_eng\n\ndf_raw = df_raw.dropna()\n\nsplit data\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_valid, y_train, y_valid = train_test_split(df_raw.drop('Target', axis = 1),\n                                                      df_raw['Target'],\n                                                      test_size = 0.3,\n                                                      random_state = 99)\n\nModel\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.metrics import r2_score\n\npoly_feat = PolynomialFeatures(degree = 2)\n\npoly_mod_x = poly_feat.fit_transform(np.array(x_train))\npoly_model = LinearRegression()\npoly_model.fit(poly_mod_x, y_train)\n\npoly_mod_x_val =  poly_feat.fit_transform(np.array(x_valid))\ny_pred = poly_model.predict(np.array(poly_mod_x_val))\nrmse = np.sqrt(mse(y_valid, y_pred))\nr2_score = r2_score(y_pred, y_valid)\n\nprint(""Polynomial Regression Model Performance Metrics"")\nprint('RMSE: ', rmse)\nprint('R2  : ', r2_score)\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression\n\nlog_reg = LogisticRegression()\nlog_reg.fit(x_train, y_train)\nlin_reg = LinearRegression()\nlin_reg.fit(x_train, y_train)\ny_pred_log = log_reg.predict(x_valid)\ny_pred_lin = lin_reg.predict(x_valid)\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\n\nreport = classification_report(y_valid, y_pred_log)\nrmse = np.sqrt(mean_squared_error(y_valid, y_pred_lin))\nr2 = r2_score(y_valid, y_pred_lin)\n\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\n\nnaive_bayes = GaussianNB()\nrandom_forest = RandomForestClassifier()\ndecision_tree = DecisionTreeClassifier()\nsvm = SVC()\n\ngrid.best_params_\n\nfrom sklearn.ensemble import VotingClassifier\n\nestimator = [('Logistic Regression', LogisticRegression(random_state = 99)),\n             ('Decision Tree', DecisionTreeClassifier(random_state = 99)),\n             ('Random Forest', RandomForestClassifier(random_state = 99)),\n             ('SVM', SVC(C = 1000, gamma = 0.0001, kernel = 'rbf', probability = True, random_state = 99))]\n\nvoting_model = VotingClassifier(estimators = estimator,\n                                voting = 'soft')\nvoting_model.fit(x_train, y_train)\npred = voting_model.predict(x_valid)\nreport = classification_report(y_valid, pred)\nprint(report)\n\nfrom sklearn.ensemble import BaggingClassifier\n\nbagging_model = BaggingClassifier(n_estimators = 100,\n                                  estimator = LogisticRegression())\nbagging_model.fit(x_train, y_train)\npred = bagging_model.predict(x_valid)\nreport = classification_report(y_valid, pred)\n\nfrom sklearn.ensemble import AdaBoostClassifier\n\nadaboost_model = AdaBoostClassifier(n_estimators = 100,\n                                    estimator = \nRandomForestClassifier())\nadaboost_model.fit(x_train, y_train)\npred = adaboost_model.pred(x_valid)\nreport = classification_report(y_valid, pred)\n\nClustering KMeans\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\nk_range = range(1, 10)\ninertias = []\nk_model = []\n\nfor k in k_range:\n    kmeans = KMeans(n_clusters=k, random_state=10)\n    kmeans.fit(df)\n    inertias.append(kmeans.inertia_)\n    k_model.append(kmeans)\n\nplt.plot(list(k_range), inertias, ""s-"", linewidth=1)\nplt.title('Elbow Chart')\nplt.xlabel('Number of Clusters (k)')\nplt.ylabel('Inertia')\nplt.xticks(list(k_range))\nplt.xlim(0, 9)\nplt.ylim(0, 25000)\nplt.grid(True)\nplt.show()\n\nfrom sklearn.metrics import silhouette_score\nsilhoutte_scores = [silhouette_score(df1, model.labels_) for model in k_model[1:]]\n\nplt.plot(range(2, 10), silhoutte_scores, ""o-"")\nplt.title(""Silhoutte Scores for k=1 to k=8"")\nplt.axis([1.5, 8.5, 0.4, 0.8])\nplt.grid(True)\nplt.xlim(0, 12)\nplt.ylim(0,1)\n\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.cluster import DBSCAN\nkmeans = KMeans(n_clusters = 4)\nkmeans.fit(df_raw)\ny_kmeans = kmeans.predict(df_raw)\n\ngaussian = GaussianMixture(n_components = 4)\ngaussian.fit(df_raw)\ny_gaussian = gaussian.predict(df_raw)\n\ndbscan = DBSCAN(eps = 1.3, min_samples = 4)\ndbscan.fit(df_raw)\ny_dbscan = DBSCAN.predict(df_raw)\n\n#standardize\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nx_standardize = scaler.fit_transform(df_raw.drop('Outcome', axis = 1))\n\nfrom sklearn.decomposition import PCA\n\npca = PCA()\npca.fit(x_standardize)\nx_pca = pca.transform(x_standardize)\n\nvariance = pca.explained_variance_ratio\nsns.barplot(x = list(range(1, len(variance)+1)), y = variance)\nvariance\n\n#pca\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nx_standardize = scaler.fit_transform(x)\n\nfrom sklearn.decomposition import PCA\n\npca = PCA()\npca.fit(x_standardize)\nx_pca = pca.transform(x_standardize)\n\ncat = ['sex', 'age', 'BMI']\ndf = df_raw.dropna().copy()\ndf_pred = df_raw[cat]\ndf_resp = df_raw[['healthy']]\n\nlin_reg = LinearRegression()\nlin_reg.fit(df_pred, df_resp)\ndf_new = df['healthy'].fillna(lin_reg.predict(df_raw[cat]))\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.naive_bayes import GaussianNB\n\nkmeans = KMeans(n_clusters = 5)\nkmeans.fit(df_raw)\ny_kmeans = kmeans.predict(df_raw)\n\ngaus_mix = GaussianMixture(n_components = 5)\ngaus_mix.fit(df_raw)\ny_gaus = gaus_mix.predict(df_raw)\n\ndbscan = DBSCAN(eps=1.2)\ndbscan.fit(df_raw)\ny_dbscan = dbscan.predict(df_raw)\n\nnaive_bayes = GaussianNB()\nnaive_bayes.fit(x_train, y_train)\ny_pred = naive_bayes.predict(x_valid)\nreport = classification_report(y_valid, y_pred)\n\ndf_corr = df_raw.corr()[['Outcome']].sort_values(by = 'Outcome')\nsns.heatmap(df_corr, annot = True)\n\nsns.scatterplot(x = , y = , hue = )\n\ninertia = []\nK = range(1, 10)\n\nfor i in K:\n  kmeans = KMeans(n_clsuters = i)\n  kmeans.fit(df_raw)\n  inertia.append(kmeans.inertia_)\n\nplt.plot(K, inertia)\nplt.xlabel('K')\nplt.ylabel('Inertia')\nplt.show()\n\nnumeric_columns = list(df_raw.select_dtypes(include = [np.number]).columns)\nfor i, col in enumerate(numeric_columns):\n  plt.figure()\n  sns.boxplot(data = df_raw, x = 'Target', y = col)\n\nfrom sklearn.preprocessing import OneHotEncoder\ncat = ['Sex', 'Embarked']\ndf_cat = df_raw[cat]\ndf_encoded = pd.get_dummies(df_cat)\n\ndf_new = pd.concat([df_raw, df_encoded])\ndf_new = df_new.drop(cat, axis = 1)\n\nfrom sklearn.model_selection import GridSearchCV\n\nparams_grid = {'C': [1, 10 , 100, 1000],\n               'gamma' : [0.00001, 0.001, 0.01, 0.1],\n               'kernel': ['rbf']}\n\ngrid = GridSearchCV(SVC(), params_grid, refit = True, verbose = 1)\n\ngrid.best_params_\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\n#still broken\npoly_reg = PolynomialFeatures(degree = 2)\n\nlm.fit(poly_reg, y)\n\nx_val_poly = poly_reg(x_val)\n\n"", ""    import pandas as pd\n    import numpy as np\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    import plotly.express as px\n\n# Scaling\n    from sklearn.preprocessing import RobustScaler\n    from sklearn.preprocessing import LabelEncoder\n\n# Train Test Split\n    from sklearn.model_selection import train_test_split\n\n# Models\n    import torch\n    import torch.nn as nn\n    from sklearn.svm import SVC\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.tree import DecisionTreeClassifier\n    from sklearn.ensemble import GradientBoostingClassifier\n\n# Metrics\n    from sklearn.metrics import accuracy_score, classification_report, roc_curve\n\n# Cross Validation\n    from sklearn.model_selection import cross_val_score\n    from sklearn.model_selection import GridSearchCV\n\n    import warnings\n    warnings.filterwarnings(""ignore"")\n\n    df = pd.read_csv(""heart.csv"")\n\n# Preview of the first 10 rows of the data\n    df.head(10)\n\n    df.dtypes\n\n#shape of data\n    print(""The shape of the dataset is : "", df.shape)\n\n**Get Categorical Column**\n    string_col=df.select_dtypes(""string"").columns.to_list()\n\n**#Check the number of unique values in each column**\n    dict = {}\n    for i in list(df.columns):\n    dict[i] = df[i].value_counts().shape[0]\n\n    pd.DataFrame(dict,index=[""unique count""]).transpose()\n\n**check duplicated**\n    df[df.duplicated()]\n\n**Remove duplicate**\n    df.drop_duplicates(keep='first',inplace=True)\n\n**check new shape**\n    print('Number of rows are',df.shape[0], 'and number of columns are ',df.shape[1])\n\n    num_col=df.columns.to_list()\n    for col in string_col:\n    num_col.remove(col)\n    num_col.remove(""output"")\n\n    df[string_col].head()\n    for col in string_col:\n    print(f""The distribution of categorical valeus in the {col} is : "")\n    print(df[col].value_counts())\n\n**check statistical data**\n    df.describe()\n    df.coor()\n\n\n**seperate the column in categorical and continious**\n    cat_cols = ['sex','exng','caa','cp','fbs','restecg','slp','thall']\n    con_cols = [""age"",""trtbps"",""chol"",""thalachh"",""oldpeak""]\n    target_col = [""output""]\n    print(""The categorial cols are : "", cat_cols)\n    print(""The continuous cols are : "", con_cols)\n    print(""The target variable is :  "", target_col)\n\n    df[con_cols].describe().transpose()\n\n# EDA#\n    pair plot\n    plt.figure(figsize=(20, 20))\n    sns.pairplot(df,hue='output',palette = [""#8000ff"",""#da8829""])\n    plt.show()\n\n    violin plot\n    plt.figure(figsize=(18, 10))\n    plt.subplot(2,3,1)\n    sns.violinplot(x = 'sex', y = 'output', data = df)\n    plt.xticks(rotation=45)\n\n    plt.subplot(2,3,2)\n    sns.violinplot(x = 'thall', y = 'output', data = df)\n    plt.xticks(rotation=45)\n\n    plt.subplot(2,3,3)\n    sns.violinplot(x = 'exng', y = 'output', data = df)\n    plt.xticks(rotation=45)\n\n    plt.subplot(2,3,4)\n    sns.violinplot(x = 'restecg', y = 'output', data = df)\n    plt.xticks(rotation=45)\n\n    plt.subplot(2,3,5)\n    sns.violinplot(x = 'cp', y = 'output', data = df)\n    plt.xticks(rotation=45)\n\n    plt.subplot(2,3,6)\n    sns.violinplot(x = 'fbs', y = 'output', data = df)\n    plt.xticks(rotation=45)\n\n    plt.tight_layout()\n    plt.show()\n\n    heatmap\n    px.imshow(df.corr(),title=""Correlation Plot of the Heat Failure Prediction"")\n\n    plt.figure(figsize= (16, 8))\n    sns.heatmap(df.corr(), annot = True, cmap= 'gnuplot2_r', fmt= '.1f');\n\n# Count plot of categorical features#\n    fig = plt.figure(figsize=(18,15))\n    gs = fig.add_gridspec(3,3)\n    gs.update(wspace=0.5, hspace=0.25)\n    ax0 = fig.add_subplot(gs[0,0])\n    ax1 = fig.add_subplot(gs[0,1])\n    ax2 = fig.add_subplot(gs[0,2])\n    ax3 = fig.add_subplot(gs[1,0])\n    ax4 = fig.add_subplot(gs[1,1])\n    ax5 = fig.add_subplot(gs[1,2])\n    ax6 = fig.add_subplot(gs[2,0])\n    ax7 = fig.add_subplot(gs[2,1])\n    ax8 = fig.add_subplot(gs[2,2])\n\n    background_color = ""#ffe6e6""\n    color_palette = [""#800000"",""#8000ff"",""#6aac90"",""#5833ff"",""#da8829""]\n    fig.patch.set_facecolor(background_color)\n    ax0.set_facecolor(background_color)\n    ax1.set_facecolor(background_color)\n    ax2.set_facecolor(background_color)\n    ax3.set_facecolor(background_color)\n    ax4.set_facecolor(background_color)\n    ax5.set_facecolor(background_color)\n    ax6.set_facecolor(background_color)\n    ax7.set_facecolor(background_color)\n    ax8.set_facecolor(background_color)\n\n# Title of the plot\n    ax0.spines[""bottom""].set_visible(False)\n    ax0.spines[""left""].set_visible(False)\n    ax0.spines[""top""].set_visible(False)\n    ax0.spines[""right""].set_visible(False)\n    ax0.tick_params(left=False, bottom=False)\n    ax0.set_xticklabels([])\n    ax0.set_yticklabels([])\n    ax0.text(0.5,0.5,\n         'Count plot for various\\n categorical features\\n_________________',\n         horizontalalignment='center',\n         verticalalignment='center',\n         fontsize=18, fontweight='bold',\n         fontfamily='serif',\n         color=""#000000"")\n\n# Sex count\n    ax1.text(0.3, 220, 'Sex', fontsize=14, fontweight='bold', fontfamily='serif',     color=""#000000"")\n    ax1.grid(color='#000000', linestyle=':', axis='y', zorder=0,  dashes=(1,5))\n    sns.countplot(ax=ax1,data=df,x='sex',palette=color_palette)\n    ax1.set_xlabel("""")\n    ax1.set_ylabel("""")\n\n# Exng count\n    ax2.text(0.3, 220, 'Exng', fontsize=14, fontweight='bold', fontfamily='serif', color=""#000000"")\n    ax2.grid(color='#000000', linestyle=':', axis='y', zorder=0,  dashes=(1,5))\n    sns.countplot(ax=ax2,data=df,x='exng',palette=color_palette)\n    ax2.set_xlabel("""")\n    ax2.set_ylabel("""")\n\n# Caa count\n    ax3.text(1.5, 200, 'Caa', fontsize=14, fontweight='bold', fontfamily='serif', color=""#000000"")\n    ax3.grid(color='#000000', linestyle=':', axis='y', zorder=0,  dashes=(1,5))\n    sns.countplot(ax=ax3,data=df,x='caa',palette=color_palette)\n    ax3.set_xlabel("""")\n    ax3.set_ylabel("""")\n\n# Cp count\n    ax4.text(1.5, 162, 'Cp', fontsize=14, fontweight='bold', fontfamily='serif', color=""#000000"")\n    ax4.grid(color='#000000', linestyle=':', axis='y', zorder=0,  dashes=(1,5))\n    sns.countplot(ax=ax4,data=df,x='cp',palette=color_palette)\n    ax4.set_xlabel("""")\n    ax4.set_ylabel("""")\n\n# Fbs count\n    ax5.text(0.5, 290, 'Fbs', fontsize=14, fontweight='bold', fontfamily='serif', color=""#000000"")\n    ax5.grid(color='#000000', linestyle=':', axis='y', zorder=0,  dashes=(1,5))\n    sns.countplot(ax=ax5,data=df,x='fbs',palette=color_palette)\n    ax5.set_xlabel("""")\n    ax5.set_ylabel("""")\n\n# Restecg count\n    ax6.text(0.75, 165, 'Restecg', fontsize=14, fontweight='bold', fontfamily='serif', color=""#000000"")\n    ax6.grid(color='#000000', linestyle=':', axis='y', zorder=0,  dashes=(1,5))\n    sns.countplot(ax=ax6,data=df,x='restecg',palette=color_palette)\n    ax6.set_xlabel("""")\n    ax6.set_ylabel("""")\n\n# Slp count\n    ax7.text(0.85, 155, 'Slp', fontsize=14, fontweight='bold', fontfamily='serif', color=""#000000"")\n    ax7.grid(color='#000000', linestyle=':', axis='y', zorder=0,  dashes=(1,5))\n    sns.countplot(ax=ax7,data=df,x='slp',palette=color_palette)\n    ax7.set_xlabel("""")\n    ax7.set_ylabel("""")\n\n# Thall count\n    ax8.text(1.2, 180, 'Thall', fontsize=14, fontweight='bold', fontfamily='serif', color=""#000000"")\n    ax8.grid(color='#000000', linestyle=':', axis='y', zorder=0,  dashes=(1,5))\n    sns.countplot(ax=ax8,data=df,x='thall',palette=color_palette)\n    ax8.set_xlabel("""")\n    ax8.set_ylabel("""")\n\n    for s in [""top"",""right"",""left""]:\n    ax1.spines[s].set_visible(False)\n    ax2.spines[s].set_visible(False)\n    ax3.spines[s].set_visible(False)\n    ax4.spines[s].set_visible(False)\n    ax5.spines[s].set_visible(False)\n    ax6.spines[s].set_visible(False)\n    ax7.spines[s].set_visible(False)\n    ax8.spines[s].set_visible(False)\n\n# boxen plot of continious features\n    fig = plt.figure(figsize=(18,16))\n    gs = fig.add_gridspec(2,3)\n    gs.update(wspace=0.3, hspace=0.15)\n    ax0 = fig.add_subplot(gs[0,0])\n    ax1 = fig.add_subplot(gs[0,1])\n    ax2 = fig.add_subplot(gs[0,2])\n    ax3 = fig.add_subplot(gs[1,0])\n    ax4 = fig.add_subplot(gs[1,1])\n    ax5 = fig.add_subplot(gs[1,2])\n\n    background_color = ""#ffe6e6""\n    color_palette = [""#800000"",""#8000ff"",""#6aac90"",""#5833ff"",""#da8829""]\n    fig.patch.set_facecolor(background_color)\n    ax0.set_facecolor(background_color)\n    ax1.set_facecolor(background_color)\n    ax2.set_facecolor(background_color)\n    ax3.set_facecolor(background_color)\n    ax4.set_facecolor(background_color)\n    ax5.set_facecolor(background_color)\n\n# Title of the plot\n    ax0.spines[""bottom""].set_visible(False)\n    ax0.spines[""left""].set_visible(False)\n    ax0.spines[""top""].set_visible(False)\n    ax0.spines[""right""].set_visible(False)\n    ax0.tick_params(left=False, bottom=False)\n    ax0.set_xticklabels([])\n    ax0.set_yticklabels([])\n    ax0.text(0.5,0.5,\n         'Boxen plot for various\\n continuous features\\n_________________',\n         horizontalalignment='center',\n         verticalalignment='center',\n         fontsize=18, fontweight='bold',\n         fontfamily='serif',\n         color=""#000000"")\n\n# Age\n    ax1.text(-0.05, 81, 'Age', fontsize=14, fontweight='bold', fontfamily='serif', color=""#000000"")\n    ax1.grid(color='#000000', linestyle=':', axis='y', zorder=0,  dashes=(1,5))\n    sns.boxenplot(ax=ax1,y=df['age'],palette=[""#800000""],width=0.6)\n    ax1.set_xlabel("""")\n    ax1.set_ylabel("""")\n\n# Trtbps\n    ax2.text(-0.05, 208, 'Trtbps', fontsize=14, fontweight='bold', fontfamily='serif', color=""#000000"")\n    ax2.grid(color='#000000', linestyle=':', axis='y', zorder=0,  dashes=(1,5))\n    sns.boxenplot(ax=ax2,y=df['trtbps'],palette=[""#8000ff""],width=0.6)\n    ax2.set_xlabel("""")\n    ax2.set_ylabel("""")\n\n# Chol\n    ax3.text(-0.05, 600, 'Chol', fontsize=14, fontweight='bold', fontfamily='serif', color=""#000000"")\n    ax3.grid(color='#000000', linestyle=':', axis='y', zorder=0,  dashes=(1,5))\n    sns.boxenplot(ax=ax3,y=df['chol'],palette=[""#6aac90""],width=0.6)\n    ax3.set_xlabel("""")\n    ax3.set_ylabel("""")\n\n# Thalachh\n    ax4.text(-0.0\n\n"", 'You should not use a classifier for a regression task. Try:\nknn = KNeighborsRegressor(n_neighbors=1)\n\n'"
How to add labels in panda dataframe columns with else condition?,"'Try with map:\ndf[\'label\'] = df[\'category\'].map(label_dict).fillna(5).astype(int)\nprint(df)\n\n# Output\n   category  no  label\n0     CRIME  10      1\n1  BUSINESS  15      2\n2    SPORTS  12      3\n3    TRAVEL   2      5\n4  WELLNESS   3      5\n5      ARTS  25      4\n\nOr with replace:\ndf[\'label\'] = df[\'category\'].replace(label_dict | {\'.*\': 5}, regex=True)\n\nOr suggested by @mozway:\ndf[\'label\'] = df[\'category\'].map(lambda x: label_dict.get(x, 5))\n\n'"
Variable assignment for dataframe columns,"""df.rename() without reassignment doesn't change the column name, You need to use df.rename(columns={'col':'new_col'}, inplace=True) and test the time taken\n"""
How to perform a pairwise correlation in a Pandas dataframe containing lists?,"'Suppose the following dataframe:\nimport pandas as pd\nimport numpy as np\n\ndata = [[0.07717473, 0.90724758, 0.80752715, 0.04318562, 0.0569035, 0.12796062, 0.220677, 0.3716013, 0.74646015, -0.41114205],\n        [-0.12252081, 0.03894384, -0.74668061, 0.00310963, 0.10716717, -0.42125924, 0.90771138, 0.10498123, 0.60872, -0.62587628],\n        [-0.24917124, -0.76921359, 0.55519856, 0.56067116, -0.27319101, -0.01258496, 0.66428267, 0.53822299, -0.86883193, -0.15486245],\n        [-0.14676444, 0.21910793, -0.11010598, 0.86445147, -0.92299316, -0.82828022, -0.7274392, 0.66965337, 0.67446502, -0.50343198]]\ndf = pd.Series(data, index=[100610, 102311, 104416, 105923]).to_frame(728)\nprint(df)\n\n# Output\n                                                      728\n100610  [0.07717473, 0.90724758, 0.80752715, 0.0431856...\n102311  [-0.12252081, 0.03894384, -0.74668061, 0.00310...\n104416  [-0.24917124, -0.76921359, 0.55519856, 0.56067...\n105923  [-0.14676444, 0.21910793, -0.11010598, 0.86445...\n\nYou can use np.corrcoef to deal with vectors:\n>>> np.corrcoef(np.vstack(df.loc[:, 728]))\n\narray([[ 1.        ,  0.19332151, -0.23634425,  0.40882581],\n       [ 0.19332151,  1.        , -0.08039743,  0.15825333],\n       [-0.23634425, -0.08039743,  1.        , -0.02293406],\n       [ 0.40882581,  0.15825333, -0.02293406,  1.        ]])\n\n'"
dataframe count unique list values from a column and add the sum as a row,"'This should work if you would like to ignore None:\ndf.sum().map(lambda x: len({i for i in x if i is not None}))\n\nor\ndf.stack().explode().groupby(level=1).nunique()\n\nOutput:\ns1    3\ns2    6\n\n', 'Can be done with a set count apply over each column + flattening with numpy concatenation:\ndata = {\n    \'s1\': [[1, 2], [None], [2, 3]],\n    \'s2\': [[4, 5], [6, 7], [3, 2]]\n}\n\ndf = pd.DataFrame(data)\n\npd.DataFrame( {\'step\': range(1, 1+df.shape[1]),\n               \'count\': df.apply(lambda x : len(set( np.concatenate(x.values))), axis=0)}\n            )\n\n#   step    count\n# s1    1   4\n# s2    2   6\n\nEdit:\nNot counting None values:\npd.DataFrame( {\'step\': range(1, 1+df.shape[1]),\n               \'count\': df.apply(lambda x : len(set( np.concatenate(x.values)).difference({None})), axis=0)}\n            )\n\nor\npd.DataFrame( {\'step\': range(1, 1+df.shape[1]),\n               \'count\': df.apply(lambda x : len(set( value for value in np.concatenate(x.values) if value is not None)), axis=0)}\n            )\n\n', 'So in your case\nout = df.sum().map(set).map(len)\nOut[97]: \ns1    4\ns2    6\ndtype: int64\n\n', 'Another solution:\nout = pd.DataFrame(\n    [\n        {""step"": step, ""count"": len(df[c].explode().unique())}\n        for step, c in enumerate(df, 1)\n    ]\n)\nprint(out)\n\nPrints:\n   step  count\n0     1      4\n1     2      6\n\n\nOr:\nout = pd.DataFrame(\n    [\n        {""step"": c, ""count"": len(df[c].explode().unique())}\n        for c in df\n    ]\n)\nprint(out)\n\nPrints:\n  step  count\n0   s1      4\n1   s2      6\n\n'"
describe when data is value_counts,"'The generic way would be to restore the original data, then compute the statistics:\n# aggregated data\ndf = pd.DataFrame({\'value\': [1, 2, 3], \'count\': [5, 1, 4]})\n\n# replicate rows and compute statistics\nout = df.loc[df.index.repeat(df[\'count\']), \'value\'].describe()\n\nOf course, you can do better depending on which exact statistics you want to compute: min/max would be unchanged; mean and std could be computed using numpy.average/statsmodels.stats.weightstats.DescrStatsW and their weight parameter, etc. You have to see for yourself what you need to compute and decide if you can do so without unaggregating.\nOutput:\ncount    10.000000\nmean      1.900000\nstd       0.994429\nmin       1.000000\n25%       1.000000\n50%       1.500000\n75%       3.000000\nmax       3.000000\nName: value, dtype: float64\n\n'"
Running eval with condition in lambda function,"""For a simple if else, you can change the way you write the condition to:\ns = ""obj['weight'] if obj['height'] > 2 else obj['weight'] * 2""\n\ndf['new_status'] = df.apply(lambda obj: eval(s), axis=1)\n\n"", ""Maybe, this is the syntax you're looking for:\ndf['new_status'] = df.apply(lambda obj : obj.weight if obj.height > 2 else obj.weight * 2 , axis=1)\n\noutput:\nOut[7]: \n   height  weight  new_status\n0       1      10          20\n1       2      20          40\n2       3      30          30\n3       4      40          40\n4       5      50          50\n\n"", 'If for some reason you want just the eval(s) to work, you can define it as below:\n s=""obj.weight  if obj.weight > 2 else obj.weight * 2""\n\nBut this will not work as expected in your apply function. You must use a pure function instead\n', ""This should work if you use triple quotes instead of double quotes around the code.\nIs there any reason you are using eval, though? eval is almost never the answer to your problem, and in this case it can easily be ommitted:\ndf = pd.DataFrame({\n    'height':  [1, 2, 3, 4, 5],\n    'weight': [10, 20, 30, 40, 50]\n})\n\ndef condition(obj):\n   if obj.height > 2:\n       return obj.weight\n   else:\n       return obj.weight * 2\n\ndf['new_status'] = df.apply(condition, axis=1)\n\n"", 'First of all, you can use triple quotes to make a multiline string:\ns = """"""\nif obj.height > 2:\n    obj.weight\nelse:\n    obj.weight * 2\n""""""\n\nOtherwise, you have one line which does not follow python syntax indeed. That should fix your SyntaxError but wouldn\'t work as you expect it to work.\nSecond of all and very important one, try avoiding using eval. You can define a function and pass it to .apply:\ndf = pd.DataFrame({\n    \'height\':  [1, 2, 3, 4, 5],\n    \'weight\': [10, 20, 30, 40, 50]\n})\n\ndef process(obj):\n    if obj.height > 2:\n        return obj.weight\n    else:\n        return obj.weight * 2\n\ndf[\'new_status\'] = df.apply(process, axis=1)\n\n'"
Finding List Index of Values in Dataframe Column,"'You can use pd.cut since your mem_list is sorted:\ndf[\'MemWeight\'] = pd.cut(df[\'Weighting\'], bins=[-np.inf, *mem_list, np.inf], labels=False)\nprint(df)\n\n# Output\n    MemRef MemName  Weighting  MemWeight\n0        1       a       2.00          4\n1        2       a       2.00          4\n2        3       a       2.00          4\n3        4       a       2.00          4\n4        5       a       2.00          4\n5        6       a       2.00          4\n6        7       a       2.00          4\n7        8       a       2.00          4\n8        9       a       2.00          4\n9       10       a       2.00          4\n10      11       a       2.00          4\n11      12       a       1.97          1\n12      13       a       2.00          4\n13      14       a       2.00          4\n14      15       a       2.00          4\n15      16       a       2.00          4\n16      17       a       2.00          4\n17      18       a       2.00          4\n18      19       a       2.00          4\n19      20       a       2.00          4\n\nOr with np.searchsorted:\ndf[\'MemWeight\'] = np.searchsorted(mem_list, df[\'Weighting\'], side=\'left\')\n\n', 'You can use enumerate, a dictionary comprehension, and map:\ndf[\'MemWeight\'] = df[\'Weighting\'].map({k: v for v,k in enumerate(mem_list)})\n\nOr merge (creating a new DataFrame):\nout = df.merge(pd.DataFrame({\'Weighting\': mem_list,\n                             \'MemWeight\': range(len(mem_list)),\n                            }), how=\'left\')\n\nNote that you could also use df[\'MemWeight\'] = df[\'Weighting\'].apply(mem_list.index) but that would be inefficient (searching again the list for all rows).\nOutput:\n    MemRef MemName  Weighting  MemWeight\n0        1       a       2.00          4\n1        2       a       2.00          4\n2        3       a       2.00          4\n3        4       a       2.00          4\n4        5       a       2.00          4\n5        6       a       2.00          4\n6        7       a       2.00          4\n7        8       a       2.00          4\n8        9       a       2.00          4\n9       10       a       2.00          4\n10      11       a       2.00          4\n11      12       a       1.97          1\n12      13       a       2.00          4\n13      14       a       2.00          4\n14      15       a       2.00          4\n15      16       a       2.00          4\n16      17       a       2.00          4\n17      18       a       2.00          4\n18      19       a       2.00          4\n19      20       a       2.00          4\n\n'"
How to Efficiently Merge/Join Large Datasets?,"'The general rule with pandas is that you should have 5-10x the memory on your system as the number of gigabytes the file is. Since you have millions of rows and hundreds of columns, scale is the problem.\nConsider using a local SQL database like DuckDB where you can join the datasets using SQL instead, and let the database handle the processing for you.\nThen pull that dataset into pandas (if your memory permits) and do the analysis you want\n'"
Creating an interactive graph but cannot find defined widget referenced,"""Just solved this. I was calling interactive plot incorrectly so function was the same but instead of:\ninteractive_plot = interactive(plot,{'BR':BR,'TR':TR,'NGw':NGw,'WD':WD,'Ew':Ew,'nuw':nuw,'RR':RR,'e':e,'intv':int,'D':D })\n\nI used:\nout3 = widgets.interactive_output(plot,{'BR':BR,'TR':TR,'NGw':NGw,'WD':WD,'Ew':Ew,'nuw':nuw,'RR':RR,'e':e,'intv':intv,'D':D })\nwidgets.VBox([out3])\n\nwhich worked as I wanted :)\n"""
Matplotlib: data from DataFrame appears incorrectly,"'I needed to use groupby here as well. Without groupby, it was just showing some random things.\nplotData = pd.DataFrame(dataFrame.groupby([\'Living Rooms\'])[\'Price\'].mean()).reset_index()\nplt.bar(plotData[\'Living Rooms\'], plotData[\'Price\'], color=\'darkgreen\')\n\n', 'It can be because of data types, you can try this updated code that I developed for you, but if I know what are the data or share a part of it I can help you better\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Read the dataset\ndataFrame = pd.read_csv(\'dataset.csv\', sep=\',\')\ndataFrame.replace(\'\', np.nan, inplace=True)  # Apply the NaN replacement to the DataFrame\n\n# Create new smaller dataset\nroomsDF = pd.DataFrame(dataFrame[[\'Price\', \'Living Rooms\', \'Halls\', \'Bathrooms\']])\nroomsDF.dropna(axis=\'rows\', inplace=True)  # Drop rows with missing values\n\n# Convert columns to numerical data type\nroomsDF[\'Living Rooms\'] = pd.to_numeric(roomsDF[\'Living Rooms\'])\nroomsDF[\'Halls\'] = pd.to_numeric(roomsDF[\'Halls\'])\nroomsDF[\'Bathrooms\'] = pd.to_numeric(roomsDF[\'Bathrooms\'])\n\n# Plotting\nplt.figure(figsize=(12, 4))\n\nplt.subplot(131)\nplt.bar(roomsDF[\'Living Rooms\'], roomsDF[\'Price\'], color=\'darkgreen\')\nplt.xlabel(\'Living Rooms\')\nplt.ylabel(\'Price\')\n\nplt.subplot(132)\nplt.bar(roomsDF[\'Halls\'], roomsDF[\'Price\'], color=\'limegreen\')\nplt.xlabel(\'Halls\')\nplt.ylabel(\'Price\')\n\nplt.subplot(133)\nplt.bar(roomsDF[\'Bathrooms\'], roomsDF[\'Price\'], color=\'seagreen\')\nplt.xlabel(\'Bathrooms\')\nplt.ylabel(\'Price\')\n\nplt.tight_layout()  # Adjust subplot spacing\nplt.savefig(\'Picture.png\')\n\n'"
