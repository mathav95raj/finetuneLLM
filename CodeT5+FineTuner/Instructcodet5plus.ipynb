{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d519a8a5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install --user --upgrade git+https://github.com/huggingface/transformers.git\n",
    "!pip install --upgrade git+https://github.com/huggingface/accelerate.git\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24ebef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ff1a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bitsandbytes\n",
    "!pip install git+https://github.com/huggingface/peft.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f26c49f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b699e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/envs/ctp/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda113.so\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 113\n",
      "CUDA SETUP: Loading binary /opt/conda/envs/ctp/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda113.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/ctp/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /opt/conda/envs/ctp did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-14 13:09:21,622] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, TrainingArguments, Trainer, AutoConfig\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b389266",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_num=-1\n",
    "max_len = 512\n",
    "cache_data = 'cache_data/full_instruct_set'\n",
    "load = 'Salesforce/codet5p-16b'\n",
    "device = \"cuda\"\n",
    "# Training\n",
    "epochs=1\n",
    "lr=2e-5\n",
    "lr_warmup_steps=30\n",
    "batch_size_per_replica=1\n",
    "grad_acc_steps=16\n",
    "local_rank=-1\n",
    "deepspeed=\"ds.json\"\n",
    "fp16=True\n",
    "\n",
    "# Logging and stuff\n",
    "save_dir=\"saved_models/full_instruct_set\"\n",
    "log_freq=10\n",
    "save_freq=500\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "final_checkpoint_dir = os.path.join(save_dir, \"final_checkpoint\")\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0eca5ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba8cacc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(model, train_data):\n",
    "    print(f\"Starting main loop\")\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        report_to='tensorboard',\n",
    "        output_dir=save_dir,\n",
    "        overwrite_output_dir=False,\n",
    "\n",
    "        do_train=True,\n",
    "        save_strategy='epoch',\n",
    "\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=batch_size_per_replica,\n",
    "        gradient_accumulation_steps=grad_acc_steps,\n",
    "        learning_rate=lr,\n",
    "        weight_decay=0.05,\n",
    "        warmup_steps=lr_warmup_steps,\n",
    "\n",
    "        logging_dir=save_dir,\n",
    "        logging_first_step=True,\n",
    "        logging_steps=log_freq,\n",
    "        save_total_limit=1,\n",
    "\n",
    "        dataloader_drop_last=True,\n",
    "        dataloader_num_workers=2,\n",
    "\n",
    "        local_rank=local_rank,\n",
    "        # deepspeed=deepspeed,\n",
    "#         fp16=fp16,\n",
    "        bf16 = True\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_data,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    if local_rank in [0, -1]:\n",
    "        model.save_pretrained(final_checkpoint_dir)\n",
    "        print(f'  ==> Finish training and save to {final_checkpoint_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "624ad6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for deepspeed\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '9994' # modify if RuntimeError: Address already in use\n",
    "os.environ['RANK'] = \"0\"\n",
    "os.environ['LOCAL_RANK'] = \"0\"\n",
    "os.environ['WORLD_SIZE'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37956e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(load)\n",
    "config = AutoConfig.from_pretrained(load, trust_remote_code=True, revision=\"main\")\n",
    "config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "config.pad_token_id = tokenizer.pad_token_id\n",
    "#for deepspeed\n",
    "config.max_position_embeddings = max_len\n",
    "# print('Model hidden size: ', config.cross_attention_hidden_size)\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "def get_model_size(model):\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    model_size = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    return \"{}M\".format(round(model_size / 1e+6))\n",
    "\n",
    "def freeze_decoder_except_xattn_codegen(model):\n",
    "    print(f'Para before freezing: {model.num_parameters()}, trainable para: {get_model_size(model)}')\n",
    "    \n",
    "    for param in model.decoder.parameters():\n",
    "#         print(param, param.requires_grad)\n",
    "        param.requires_grad = False\n",
    "\n",
    "    num_decoder_layers = model.decoder.config.n_layer\n",
    "    for i in range(num_decoder_layers):\n",
    "        each_decoder_layer = model.decoder.transformer.h[i]\n",
    "        if hasattr(each_decoder_layer, 'crossattention'):\n",
    "            each_decoder_layer.crossattention.to(torch.float32)\n",
    "            continue\n",
    "#             for param in each_decoder_layer.crossattention.parameters():\n",
    "#                 param.requires_grad = True\n",
    "        if hasattr(each_decoder_layer, 'alpha_xattn'):\n",
    "            each_decoder_layer.alpha_xattn.to(torch.float32)\n",
    "#             each_decoder_layer.alpha_xattn.requires_grad = True\n",
    "            continue\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    print(f'Para after freezing: {model.num_parameters()}, trainable para: {get_model_size(model)}')\n",
    "\n",
    "def convert_size(size_bytes):\n",
    "   if size_bytes == 0:\n",
    "       return \"0B\"\n",
    "   size_name = (\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\", \"EB\", \"ZB\", \"YB\")\n",
    "   i = int(math.floor(math.log(size_bytes, 1024)))\n",
    "   p = math.pow(1024, i)\n",
    "   s = round(size_bytes / p, 2)\n",
    "   return \"%s %s\" % (s, size_name[i])\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    prompt_input, prompt_no_input = PROMPT_DICT[\"prompt_input\"], PROMPT_DICT[\"prompt_no_input\"]\n",
    "    source = [prompt_input.format_map({'instruction': instruct, 'input': inp}) if inp != ''\n",
    "                      else prompt_no_input.format_map({'instruction': instruct})\n",
    "                      for instruct, inp in zip(examples[\"instruction\"], examples[\"input\"])]\n",
    "    target = [src + output + tokenizer.eos_token for src, output in zip(source, examples[\"output\"])]\n",
    "\n",
    "    model_inputs = tokenizer(source, max_length=max_len, padding=\"max_length\", truncation=True)\n",
    "    labels = tokenizer(target, max_length=max_len, padding=\"max_length\", truncation=True)\n",
    "    model_inputs[\"decoder_input_ids\"] = copy.deepcopy(labels[\"input_ids\"])\n",
    "\n",
    "    # changing labels: convert all tokens in the duplicate prefix prompt and the padding part to -100\n",
    "    eos_token_id = tokenizer.eos_token_id\n",
    "    for x, y in zip(model_inputs[\"input_ids\"], labels[\"input_ids\"]):\n",
    "        label_prefix_len = x.index(eos_token_id) if eos_token_id in x else len(x)\n",
    "        y[:label_prefix_len] = [-100] * label_prefix_len\n",
    "\n",
    "        if eos_token_id in y:\n",
    "            pad_len = len(y) - y.index(eos_token_id) - 1\n",
    "            if pad_len > 0:\n",
    "                y[y.index(eos_token_id) + 1:] = [-100] * pad_len\n",
    "\n",
    "    # shift labels to the right as the decoder input and add decoder start token id\n",
    "    decoder_start_id = tokenizer.eos_token_id\n",
    "    for z in model_inputs[\"decoder_input_ids\"]:\n",
    "        z[1:] = z[:-1]\n",
    "        z[0] = decoder_start_id\n",
    "\n",
    "    model_inputs[\"labels\"] = copy.deepcopy(labels[\"input_ids\"])\n",
    "    model_inputs[\"decoder_attention_mask\"] = labels[\"attention_mask\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "\n",
    "def load_tokenize_data():\n",
    "#     if os.path.exists(cache_data):\n",
    "#         train_data = load_from_disk(cache_data)\n",
    "#         print(f'  ==> Loaded {len(train_data)} samples')\n",
    "# #         res =  convert_size(train_data.size_in_bytes)\n",
    "# #         print('Dataset Size:', res)\n",
    "#         return train_data, config\n",
    "#     else:\n",
    "        datasets = load_dataset(\"crumb/Clean-Instruct-440k\", split=\"train\")\n",
    "#         datasets = load_dataset(\"semeru/text-code-codesummarization\", split=\"validation\")\n",
    "        datasets = datasets.select(range(200000))\n",
    "        res =  convert_size(datasets.size_in_bytes)\n",
    "        print('Dataset Size:', res)\n",
    "        train_data = datasets.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            remove_columns=datasets.column_names,\n",
    "            num_proc=64,\n",
    "            load_from_cache_file=False,\n",
    "        )\n",
    "        print(f'  ==> Loaded {len(train_data)} samples')\n",
    "        # train_data.save_to_disk(cache_data)\n",
    "        # print(f'  ==> Saved to {cache_data}')\n",
    "\n",
    "        return train_data, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99044549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abce7518",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/unnati/.cache/huggingface/datasets/crumb___parquet/crumb--Clean-Instruct-440k-cbf10416ad8c4248/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Size: 961.89 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=64):   0%|          | 0/200000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Loaded 200000 samples\n"
     ]
    }
   ],
   "source": [
    "train_data, config = load_tokenize_data()\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e4971c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0d9c908f215496491521aab370ecbbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  8.41 GB\n",
      "PEFT Model:  8.42 GB\n",
      "trainable params: 3,743,744 || all params: 8,434,923,520 || trainable%: 0.04438385233871095\n",
      "None\n",
      "  ==> Loaded model from Salesforce/codet5p-16b, model size 8434923520\n",
      "Starting main loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/unnati/.local/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1524' max='12500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1524/12500 5:13:38 < 37:41:49, 0.08 it/s, Epoch 0.12/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>22.171200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>64.239700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>23.540800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>17.837500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>11.003200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>26.384600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>17.587700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>14.697700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>8.028100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>5.007500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>105.949400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>24.072900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>12.802400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>7.921000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>29.758300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>47.804500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>9.904900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>12.792400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>9.022000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>13.867900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>70.842000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>35.993700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>6.796300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>27.714500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>26.673200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>16.445000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>7.357000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>51.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>29.240500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>8.501900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>17.374000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>23.898100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>12.054800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>25.126400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>15.049800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>12.867900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>27.901500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>46.182100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>38.205700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>29.035900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>15.705800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>4.034100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>13.106900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>15.713300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>5.737500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>12.258600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>6.874200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>10.386400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>11.274100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>63.556700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>6.555300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>10.524700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>31.391100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>7.070600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>8.682500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>13.755900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>10.032000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>19.406500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>39.753600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>13.652300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>21.825900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>12.471700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>16.904300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>6.967400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>24.558400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>12.503200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>22.211100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>52.481800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>46.151100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>12.256100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>12.916000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>14.069400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>12.560400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>5.544400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>27.618300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>9.310400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>21.576200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>11.740100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>27.331000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>27.782500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>48.879600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>27.912000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>6.701000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>11.692600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>11.125300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>4.902100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>97.413400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>27.189200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>34.145500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>17.792600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>21.049800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>12.612900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>30.826200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>22.648800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>11.809200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>6.563100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>20.432400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>14.374100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>27.669700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>17.235700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>8.761800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>5.568800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>16.467200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>34.976500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>29.490900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>11.399600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>2.920500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>24.344700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>9.497600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>11.779000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>5.784700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>27.292600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>35.312600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>20.547100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>18.654300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>22.335400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>4.924400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>15.742000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>25.357700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>62.922100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>9.910500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>28.268400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>11.313600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230</td>\n",
       "      <td>120.686200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>27.837600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>24.360100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>15.723400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1270</td>\n",
       "      <td>7.427900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>10.330200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1290</td>\n",
       "      <td>11.210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>7.563200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1310</td>\n",
       "      <td>12.710000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>25.452600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1330</td>\n",
       "      <td>243.996400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>31.383100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>18.469500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>51.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1370</td>\n",
       "      <td>4.779500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>9.841100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1390</td>\n",
       "      <td>18.728600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>18.852300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1410</td>\n",
       "      <td>51.144600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>61.067800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1430</td>\n",
       "      <td>15.597000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>24.461000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>14.143900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>64.686500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1470</td>\n",
       "      <td>97.075900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>116.189200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1490</td>\n",
       "      <td>19.889200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>37.388300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1510</td>\n",
       "      <td>17.587300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>7.570000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "#LORA\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "# lora_config = LoraConfig(\n",
    "#     r=16,\n",
    "#     lora_alpha=32,\n",
    "#     target_modules=[\"q_proj\",\"v_proj\"],\n",
    "#     lora_dropout=0.05,\n",
    "#     bias=\"none\",\n",
    "#     task_type=TaskType.SEQ_2_SEQ_LM\n",
    "# )\n",
    "\n",
    "\n",
    "#QLORA\n",
    "\n",
    "\n",
    "\n",
    "qlora_config = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    target_modules=[\"q_proj\",\"v_proj\"]\n",
    ")\n",
    "\n",
    "\n",
    "if data_num != -1:\n",
    "    train_data = train_data.select([i for i in range(data_num)])\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(load,config=config,trust_remote_code=True,\n",
    "                                              revision=\"main\",\n",
    "                                              torch_dtype = torch.bfloat16,\n",
    "                                              load_in_8bit=True,\n",
    "                                              low_cpu_mem_usage=True, \n",
    "                                              quantization_config=bnb_config)\n",
    "# freeze_decoder_except_xattn_codegen(model)\n",
    "print('Model: ', convert_size(model.get_memory_footprint()))\n",
    "model = get_peft_model(model, qlora_config)\n",
    "print('PEFT Model: ',convert_size(model.get_memory_footprint()))\n",
    "print(model.print_trainable_parameters())\n",
    "\n",
    "print(f\"  ==> Loaded model from {load}, model size {model.num_parameters()}\")\n",
    "\n",
    "run_training(model, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61aca55f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('saved_models/instruct_set/final_checkpoint/tokenizer_config.json',\n",
       " 'saved_models/instruct_set/final_checkpoint/special_tokens_map.json',\n",
       " 'saved_models/instruct_set/final_checkpoint/vocab.json',\n",
       " 'saved_models/instruct_set/final_checkpoint/merges.txt',\n",
       " 'saved_models/instruct_set/final_checkpoint/added_tokens.json',\n",
       " 'saved_models/instruct_set/final_checkpoint/tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(final_checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2be312",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > urqts.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2cf530",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddf6a7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "I want to drop a specific row from a dataset. Provide me all the possible ways to achieve the task.\n",
      "\n",
      "\n",
      "### Response:\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "import time, os\n",
    "\n",
    "load = \"Salesforce/instructcodet5p-16b\"\n",
    "device = \"cuda\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n",
    "    ),\n",
    "}\n",
    "max_len = 512\n",
    "\n",
    "import re\n",
    "def truncate(completion):\n",
    "    import re\n",
    "    \n",
    "    def find_re(string, pattern, start_pos):\n",
    "        m = pattern.search(string, start_pos)\n",
    "        return m.start() if m else -1\n",
    "\n",
    "    terminals = [re.compile(r, re.MULTILINE) for r in [re.escape('<|end|>'),\"^'''\", '^\"\"\"', '\\n\\n\\n']]\n",
    "\n",
    "    prints = list(re.finditer('^print', completion, re.MULTILINE))\n",
    "    if len(prints) > 1:\n",
    "        completion = completion[:prints[1].start()]\n",
    "\n",
    "    defs = list(re.finditer('^def', completion, re.MULTILINE))\n",
    "    if len(defs) > 1:\n",
    "        completion = completion[:defs[1].start()]\n",
    "\n",
    "    start_pos = 0\n",
    "\n",
    "    terminals_pos = [pos for pos in [find_re(completion, terminal, start_pos) for terminal in terminals] if pos != -1]\n",
    "    if len(terminals_pos) > 0:\n",
    "        return completion[:min(terminals_pos)]\n",
    "    else:\n",
    "        return completion  \n",
    "\n",
    "def preprocess_test_ip(test_ip):\n",
    "    prompt_input, prompt_no_input = PROMPT_DICT[\"prompt_input\"], PROMPT_DICT[\"prompt_no_input\"]\n",
    "    instruct, inp = test_ip[\"instruction\"], test_ip[\"input\"]\n",
    "    if inp!=\"\":\n",
    "        source = prompt_input.format_map({'instruction': instruct, 'input': inp})\n",
    "    else:\n",
    "        source = prompt_no_input.format_map({'instruction': instruct})\n",
    "    return source\n",
    "\n",
    "test_ip = {\"instruction\":\"\"\"I want to drop a specific row from a dataset. Provide me all the possible ways to achieve the task.\n",
    "\"\"\", \"input\":\"\"}\n",
    "\n",
    "fmt_test_ip = preprocess_test_ip(test_ip)\n",
    "print(fmt_test_ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e61c201",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_output = \"\"\"\\\n",
    "\"To position the text div from the bottom, you can use the CSS property `top` instead of `bottom`. \n",
    "This will position the top part of the div at a certain distance from the top of the parent element. \n",
    "To make the div overlap the image, you can also use the CSS property `z-index` to ensure that the text div is positioned on \n",
    "top of the image. \n",
    "Here's an updated CSS code:\n",
    "``` #article-txt { min-height: inherit; position: absolute; top: 50%; right: 0; transform: translateY(-50%); z-index: 1; } \n",
    "#article-img { display: block; position: relative; z-index: 0; } ``` \n",
    "The `top: 50%` positions the top edge of the text div at 50% of the parent element's height. \n",
    "The `transform: translateY(-50%)` centers the text div vertically. \n",
    "The `z-index: 1` sets the text div to be positioned on top of the image, which has a default `z-index` of 0. \n",
    "To make the text div overhang the bottom-right corner of the image, you can adjust the `right` value of the text div to \n",
    "control how much of the div sticks out past the right edge of the image: \n",
    "``` #article-txt { min-height: inherit; position: absolute; top: 50%; right: -20%; transform: translateY(-50%); z-index: 1; } ``` \n",
    "In this example, the text div's right edge is positioned 20% to the left of the right edge of the parent element, which would \n",
    "make it overhang the bottom-right corner of the image. Adjust the value as needed to achieve your desired effect\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de8e41ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"To position the text div from the bottom, you can use the CSS property `top` instead of `bottom`. \n",
      "This will position the top part of the div at a certain distance from the top of the parent element. \n",
      "To make the div overlap the image, you can also use the CSS property `z-index` to ensure that the text div is positioned on \n",
      "top of the image. \n",
      "Here's an updated CSS code:\n",
      "``` #article-txt { min-height: inherit; position: absolute; top: 50%; right: 0; transform: translateY(-50%); z-index: 1; } \n",
      "#article-img { display: block; position: relative; z-index: 0; } ``` \n",
      "The `top: 50%` positions the top edge of the text div at 50% of the parent element's height. \n",
      "The `transform: translateY(-50%)` centers the text div vertically. \n",
      "The `z-index: 1` sets the text div to be positioned on top of the image, which has a default `z-index` of 0. \n",
      "To make the text div overhang the bottom-right corner of the image, you can adjust the `right` value of the text div to \n",
      "control how much of the div sticks out past the right edge of the image: \n",
      "``` #article-txt { min-height: inherit; position: absolute; top: 50%; right: -20%; transform: translateY(-50%); z-index: 1; } ``` \n",
      "In this example, the text div's right edge is positioned 20% to the left of the right edge of the parent element, which would \n",
      "make it overhang the bottom-right corner of the image. Adjust the value as needed to achieve your desired effect\n"
     ]
    }
   ],
   "source": [
    "print(actual_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda566f4",
   "metadata": {},
   "source": [
    "# Inference without fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f6cd1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/envs/ctp/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda113.so\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/ctp/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /opt/conda/envs/ctp did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 113\n",
      "CUDA SETUP: Loading binary /opt/conda/envs/ctp/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda113.so...\n",
      "[2023-07-14 03:25:48,099] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b36079ca4bcf487eaad77c72cbd217ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(load)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(load,\n",
    "                                              torch_dtype=torch.bfloat16,\n",
    "                                              low_cpu_mem_usage=True,\n",
    "                                              trust_remote_code=True,\n",
    "                                              quantization_config=bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f16d1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inf_without_ft(test_ip):\n",
    "    start_time= time.time()\n",
    "    fmt_test_ip = preprocess_test_ip(test_ip)\n",
    "    encoding = tokenizer(fmt_test_ip, return_tensors=\"pt\").to(device)\n",
    "    encoding['decoder_input_ids'] = encoding['input_ids'].clone()\n",
    "    outputs = model.generate(**encoding, max_length=max_len)\n",
    "    resp = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    stop_time=time.time()\n",
    "    duration =stop_time - start_time\n",
    "    return resp, duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5b7cbe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "resp, duration =inf_without_ft(test_ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a07801fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.437084436416626 s\n",
      ":I want to drop a specific row from a dataset. Provide me all the possible ways to achieve the task.\n"
     ]
    }
   ],
   "source": [
    "print(str(duration)+' s')\n",
    "print(resp[len(fmt_test_ip):])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9334f22",
   "metadata": {},
   "source": [
    "# Inference with fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c29c2f38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29bbba44ad6d4591bad4b1b6291b7ff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import BitsAndBytesConfig\n",
    "import os\n",
    "import torch\n",
    "\n",
    "save_dir=\"saved_models/full_instruct_set\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "final_checkpoint_dir = os.path.join(save_dir, \"final_checkpoint\")\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "\n",
    "max_len = 512\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    load,\n",
    "    return_dict=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    load_in_8bit=True\n",
    "    # quantization_config=bnb_config\n",
    ")\n",
    "ft_model = PeftModel.from_pretrained(base_model, final_checkpoint_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(final_checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37f53fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inf_with_ft(test_ip):\n",
    "    start_time= time.time()\n",
    "    fmt_test_ip = preprocess_test_ip(test_ip)\n",
    "    encoding = tokenizer(fmt_test_ip, return_tensors=\"pt\").to(device)\n",
    "    encoding['decoder_input_ids'] = encoding['input_ids'].clone()\n",
    "    outputs = ft_model.generate(**encoding, max_length=max_len)\n",
    "    resp = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    stop_time=time.time()\n",
    "    duration =stop_time - start_time\n",
    "    return resp, duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24284c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "resp, duration =inf_with_ft(test_ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7520d24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123.7676203250885 s\n",
      ":DROP TABLE my_table;\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\\n"
     ]
    }
   ],
   "source": [
    "print(str(duration)+' s')\n",
    "print(resp[len(fmt_test_ip):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdaef3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123.74796605110168 s\n",
      ":df.drop(['col1', 'col2'], axis=1)\n",
      "df = pd.DataFrame({'A': [1, 2, 3, 4, 5, 6], 'B': [1, 2, 3, 4, 5, 6], 'C': [1, 2, 3, 4, 5, 6]}).set_index('A')\n",
      "\n",
      "# drop the row that has the value NaN in any column.\n",
      "df.dropna()\n",
      "# or, if you want to drop a specific column, use the axis parameter.\n",
      "df.drop('B', axis=1)\n",
      "# or, if you want to drop a specific row, use the index parameter.\n",
      "df.drop(index=0)\n",
      "# or, if you want to drop a specific column, use the columns parameter.\n",
      "df.drop(columns=['B', 'C'])\n",
      "# or, if you want to drop a specific row, use the row parameter.\n",
      "df.drop(row=0)\n",
      "# or, if you want to drop a specific value, use the threshold parameter.\n",
      "df.drop(threshold=1)\n",
      "# or, if you want to drop a specific value, use the subset parameter.\n",
      "df.drop(subset=['B', 'C'])\n",
      "# or, if you want to drop a specific value, use the how parameter.\n",
      "df.drop(how='any')\n",
      "# or, if you want to drop a specific value, use the subset parameter.\n",
      "df.drop(subset=['B', 'C'], how='any')\n",
      "# or, if you want to drop a specific value, use the threshold parameter.\n",
      "df.drop(threshold=1, subset=['B', 'C'])\n",
      "# or, if you want to drop a specific value, use the how parameter.\n",
      "df.drop(how='any', subset=['B', 'C'])\n",
      "# or, if you want to drop a specific value, use the threshold parameter.\n",
      "df.drop(threshold=1, how='any', subset=['B', 'C'])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(str(duration)+' s')\n",
    "print(resp[len(fmt_test_ip):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d25186d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123.92510867118835 s\n",
      ":DROP TABLE my_table;\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\\n"
     ]
    }
   ],
   "source": [
    "print(str(duration)+' s')\n",
    "print(resp[len(fmt_test_ip):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "239a2820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.99185061454773 s\n",
      ":There are several ways to drop a specific row from a dataset. Here are some examples:\n",
      "\n",
      "1. Drop a specific row by its index: You can use the `drop()` method to drop a specific row by its index. For example, if you have a dataset with 10 rows and you want to drop the row at index 5, you can use the following code:\n",
      "\n",
      "```python\n",
      "df = pd.DataFrame({'A': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
      " 'B': [10, 9, 8, 7, 6, 5, 4, 3, 2, 1]})\n",
      "\n",
      "df.drop(df.index[5], inplace=True)\n",
      "```\n",
      "\n",
      "2. Drop a specific row by its label: You can also use the `drop()` method to drop a specific row by its label. For example, if you have a dataset with 10 rows and you want to drop the row with label 'B', you can use the following code:\n",
      "\n",
      "```python\n",
      "df = pd.DataFrame({'A': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
      " 'B': [10, 9, 8, 7, 6, 5, 4, 3, 2, 1]})\n",
      "\n",
      "df.drop(df['B'], inplace=True)\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(str(duration)+' s')\n",
    "print(resp[len(fmt_test_ip):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "940a4509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    A   B\n",
       "0   1  10\n",
       "1   2   9\n",
       "2   3   8\n",
       "3   4   7\n",
       "4   5   6\n",
       "6   7   4\n",
       "7   8   3\n",
       "8   9   2\n",
       "9  10   1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'A': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    " 'B': [10, 9, 8, 7, 6, 5, 4, 3, 2, 1]})\n",
    "\n",
    "df.drop(df.index[5], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9950305",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98529bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83cbed0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68723cc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ac43b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303.4228594303131\n",
      "\n",
      "\n",
      "# In[ ]:\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time= time.time()\n",
    "ip = \"\"\" How to Implement using recursion and cut-off cycle of the counter (like `for i: = 1 downto N do <operator>`) ?\"\"\"\n",
    "\n",
    "encoding = tokenizer(ip, return_tensors=\"pt\").to(device)\n",
    "encoding['decoder_input_ids'] = encoding['input_ids'].clone()\n",
    "outputs = ft_model.generate(**encoding, max_length=1024)\n",
    "stop_time=time.time()\n",
    "duration =stop_time - start_time\n",
    "print(duration)\n",
    "op = tokenizer.decode(outputs[0], skip_special_tokens=True)[len(ip):]\n",
    "print(truncate(op))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4a114cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "# In[ ]:\n",
      "\n",
      "\n",
      "# Solution\n",
      "\n",
      "# Import the required libraries\n",
      "import json\n",
      "\n",
      "# Create a JSON object\n",
      "json_object = json.dumps({\"value\": \"aész\"})\n",
      "\n",
      "# Print the JSON object\n",
      "print(json_object)\n",
      "\n",
      "# Output:\n",
      "# > > RES: {\"value\":\"aész\"}\n",
      "\n",
      "# The JSON object is now encoded with unicode values in the UTF-8 special characters, like the `json_encode(array(\"value\", \"aész\"));`\n",
      "\n",
      "# You can also use the `json.loads()` function to convert a JSON string to a Python object.\n",
      "\n",
      "# In[ ]:\n",
      "\n",
      "\n",
      "# Import the required libraries\n",
      "import json\n",
      "\n",
      "# Create a JSON string\n",
      "json_string = '{\"value\": \"aész\"}'\n",
      "\n",
      "# Convert the JSON string to a Python object\n",
      "json_object = json.loads(json_string)\n",
      "\n",
      "# Print the Python object\n",
      "print(json_object)\n",
      "\n",
      "# Output:\n",
      "# > > RES: {\"value\":\"aész\"}\n",
      "\n",
      "# The JSON object is now a Python object with unicode values in the UTF-8 special characters, like the `json_encode(array(\"value\", \"aész\"));`\n",
      "\n",
      "# You can also use the `json.dumps()` function to convert a Python object to a JSON string.\n",
      "\n",
      "# In[ ]:\n",
      "\n",
      "\n",
      "# Import the required libraries\n",
      "import json\n",
      "\n",
      "# Create a Python object\n",
      "json_object = {\"value\": \"aész\"}\n",
      "\n",
      "# Convert the Python object to a JSON string\n",
      "json_string = json.dumps(json_object)\n",
      "\n",
      "# Print the JSON string\n",
      "print(json_string)\n",
      "\n",
      "# Output:\n",
      "# > > RES: {\"value\":\"aész\"}\n",
      "\n",
      "# The JSON object is now a JSON string with unicode values in the UTF-8 special characters, like the `json_encode(array(\"value\", \"aész\"));`\n",
      "\n",
      "# You can also use the `json.load()` function to convert a JSON file to a Python object.\n",
      "\n",
      "# In[ ]:\n",
      "\n",
      "\n",
      "# Import the required libraries\n",
      "import json\n",
      "\n",
      "# Open the JSON file\n",
      "with open(\"my_json_file.json\", \"r\") as f:\n",
      "    # Convert the JSON file to a Python object\n",
      "    json_object = json.load(f)\n",
      "\n",
      "# Print the Python object\n",
      "print(json_object)\n",
      "\n",
      "# Output:\n",
      "# > > RES: {\"value\":\"aész\"}\n",
      "\n",
      "# The JSON object is now a Python object with unicode values in the UTF-8 special characters, like the `json_encode(array(\"value\", \"aész\"));`\n",
      "\n",
      "# You can also use the `json.dump()` function to convert a Python object to a JSON file.\n",
      "\n",
      "# In[ ]:\n",
      "\n",
      "\n",
      "# Import the required libraries\n",
      "import json\n",
      "\n",
      "# Create a Python object\n",
      "json_object = {\"value\": \"aész\"}\n",
      "\n",
      "# Open the JSON file\n",
      "with open(\"my_json_file.json\", \"w\") as f:\n",
      "    # Convert the Python object to a JSON file\n",
      "    json.dump(json_object, f)\n",
      "\n",
      "# Print the JSON file\n",
      "print(json_object)\n",
      "\n",
      "# Output:\n",
      "# > > RES: {\"value\":\"aész\"}\n",
      "\n",
      "# The JSON object is now a JSON file with unicode values in the UTF-8 special characters, like the `json_encode(array(\"value\", \"aész\"));`\n",
      "\n",
      "# You can also use the `json.load()` function to convert a JSON file to a Python object.\n",
      "\n",
      "# In[ ]:\n",
      "\n",
      "\n",
      "# Import the required libraries\n",
      "import json\n",
      "\n",
      "# Open the JSON file\n",
      "with open(\"my_json_file.json\", \"r\") as f:\n",
      "    # Convert the JSON file to a Python object\n",
      "    json_object = json\n"
     ]
    }
   ],
   "source": [
    "print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635cbe6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfcac75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baae62a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8d708e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c9502d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Error no file named pytorch_model.bin, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory saved_models/instruct_set.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ft_model \u001b[38;5;241m=\u001b[39m  \u001b[43mAutoModelForSeq2SeqLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:479\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m get_class_from_dynamic_module(\n\u001b[1;32m    476\u001b[0m         class_ref, pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    477\u001b[0m     )\n\u001b[1;32m    478\u001b[0m     _ \u001b[38;5;241m=\u001b[39m hub_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_revision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    483\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/Salesforce/codet5p-16b/4481e2dd073f8ac9f8351b6cc0c5958e911f96f9/modeling_codet5p.py:860\u001b[0m, in \u001b[0;36mCodeT5pEncoderDecoderModel.from_pretrained\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    855\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    856\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFast initialization is currently not supported for EncoderDecoderModel. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    857\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFalling back to slow initialization...\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    858\u001b[0m     )\n\u001b[1;32m    859\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_fast_init\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 860\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:2475\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2470\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   2471\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError no file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(SAFE_WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m found in directory\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2472\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2473\u001b[0m         )\n\u001b[1;32m   2474\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2475\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   2476\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError no file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTF2_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2477\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTF_WEIGHTS_NAME\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.index\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFLAX_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m found in directory\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2478\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2479\u001b[0m         )\n\u001b[1;32m   2480\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(subfolder, pretrained_model_name_or_path)):\n\u001b[1;32m   2481\u001b[0m     archive_file \u001b[38;5;241m=\u001b[39m pretrained_model_name_or_path\n",
      "\u001b[0;31mOSError\u001b[0m: Error no file named pytorch_model.bin, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory saved_models/instruct_set."
     ]
    }
   ],
   "source": [
    "ft_model =  AutoModelForSeq2SeqLM.from_pretrained(save_dir,local_files_only=True,config=config,trust_remote_code=True,\n",
    "                                              revision=\"main\", \n",
    "                                              low_cpu_mem_usage=True, \n",
    "                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c3e1c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
