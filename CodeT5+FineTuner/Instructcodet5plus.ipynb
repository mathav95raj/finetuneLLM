{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc7db8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/envs/ctp/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda113.so\n",
      "/opt/conda/envs/ctp/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /opt/conda/envs/ctp did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 113\n",
      "CUDA SETUP: Loading binary /opt/conda/envs/ctp/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda113.so...\n",
      "[2023-07-20 16:09:24,684] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "{'batch_size_per_replica': 1,\n",
      " 'cache_data': 'cache_data/instructions',\n",
      " 'data_num': -1,\n",
      " 'data_range': 5000,\n",
      " 'deepspeed': None,\n",
      " 'eightbit': True,\n",
      " 'epochs': 1,\n",
      " 'fourbit': False,\n",
      " 'fp16': False,\n",
      " 'freeze_decoder': False,\n",
      " 'grad_acc_steps': 16,\n",
      " 'instruct_data_path': 'crumb/Clean-Instruct-440k',\n",
      " 'load': 'Salesforce/codet5p-16b',\n",
      " 'local_rank': 0,\n",
      " 'log_freq': 10,\n",
      " 'lora': True,\n",
      " 'lora_rank': 4,\n",
      " 'lr': 2e-05,\n",
      " 'lr_warmup_steps': 30,\n",
      " 'max_len': 1024,\n",
      " 'precision': 'float16',\n",
      " 'save_dir': 'saved_models/experimental',\n",
      " 'save_freq': 500,\n",
      " 'split': 'train'}\n",
      "Found cached dataset parquet (/home/unnati/.cache/huggingface/datasets/crumb___parquet/crumb--Clean-Instruct-440k-cbf10416ad8c4248/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "Dataset Size: 961.89 MB\n",
      "  ==> Loaded 5000 samples                                                       \n",
      "Loading checkpoint shards: 100%|██████████████████| 5/5 [00:19<00:00,  3.92s/it]\n",
      "Model memory footprint:  15.92 GB\n",
      "  ==> Loaded model from Salesforce/codet5p-16b, model parameter count 15.36 B\n",
      "PEFT Model memory footprint:  15.93 GB\n",
      "trainable params: 3.57 M || all params: 15.36 B || trainable%: 0.02269289989066938\n",
      "None\n",
      "Starting main loop\n",
      "/home/unnati/.local/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|                                                   | 0/156 [00:00<?, ?it/s]/opt/conda/envs/ctp/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/unnati/finetuneLLM/CodeT5+FineTuner/ift.py\", line 307, in <module>\n",
      "    main(args)\n",
      "  File \"/home/unnati/finetuneLLM/CodeT5+FineTuner/ift.py\", line 262, in main\n",
      "    run_training(args, model, train_data, tokenizer)\n",
      "  File \"/home/unnati/finetuneLLM/CodeT5+FineTuner/ift.py\", line 112, in run_training\n",
      "    trainer.train()\n",
      "  File \"/home/unnati/.local/lib/python3.10/site-packages/transformers/trainer.py\", line 1537, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/home/unnati/.local/lib/python3.10/site-packages/transformers/trainer.py\", line 1802, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs)\n",
      "  File \"/home/unnati/.local/lib/python3.10/site-packages/transformers/trainer.py\", line 2647, in training_step\n",
      "    loss = self.compute_loss(model, inputs)\n",
      "  File \"/home/unnati/.local/lib/python3.10/site-packages/transformers/trainer.py\", line 2672, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/peft/peft_model.py\", line 1015, in forward\n",
      "    return self.base_model(\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
      "    output = old_forward(*args, **kwargs)\n",
      "  File \"/home/unnati/.cache/huggingface/modules/transformers_modules/Salesforce/codet5p-16b/4481e2dd073f8ac9f8351b6cc0c5958e911f96f9/modeling_codet5p.py\", line 915, in forward\n",
      "    decoder_outputs = self.decoder(\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
      "    output = old_forward(*args, **kwargs)\n",
      "  File \"/home/unnati/.cache/huggingface/modules/transformers_modules/Salesforce/codet5p-16b/4481e2dd073f8ac9f8351b6cc0c5958e911f96f9/modeling_codet5p.py\", line 679, in forward\n",
      "    transformer_outputs = self.transformer(\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
      "    output = old_forward(*args, **kwargs)\n",
      "  File \"/home/unnati/.cache/huggingface/modules/transformers_modules/Salesforce/codet5p-16b/4481e2dd073f8ac9f8351b6cc0c5958e911f96f9/modeling_codet5p.py\", line 567, in forward\n",
      "    outputs = block(\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
      "    output = old_forward(*args, **kwargs)\n",
      "  File \"/home/unnati/.cache/huggingface/modules/transformers_modules/Salesforce/codet5p-16b/4481e2dd073f8ac9f8351b6cc0c5958e911f96f9/modeling_codet5p.py\", line 329, in forward\n",
      "    attn_outputs = self.attn(\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
      "    output = old_forward(*args, **kwargs)\n",
      "  File \"/home/unnati/.cache/huggingface/modules/transformers_modules/Salesforce/codet5p-16b/4481e2dd073f8ac9f8351b6cc0c5958e911f96f9/modeling_codet5p.py\", line 208, in forward\n",
      "    qkv = self.qkv_proj(hidden_states)\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/peft/tuners/lora.py\", line 952, in forward\n",
      "    result = super().forward(x)\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/bitsandbytes/nn/modules.py\", line 402, in forward\n",
      "    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py\", line 562, in matmul\n",
      "    return MatMul8bitLt.apply(A, B, out, bias, state)\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py\", line 400, in forward\n",
      "    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/bitsandbytes/functional.py\", line 1712, in igemmlt\n",
      "    out, Sout = get_transform_buffer(\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/bitsandbytes/functional.py\", line 459, in get_transform_buffer\n",
      "    return init_func((rows, cols), dtype=dtype, device=device), state\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.41 GiB total capacity; 37.81 GiB already allocated; 134.50 MiB free; 38.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  0%|                                                   | 0/156 [00:01<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "!python ift.py --max-len 512\\\n",
    "    --instruct-data-path crumb/Clean-Instruct-440k\\\n",
    "    --split train\\\n",
    "    --data_range 5000\\\n",
    "    --load Salesforce/codet5p-16b\\\n",
    "    --batch-size-per-replica 1\\\n",
    "    --eightbit\\\n",
    "    --precision float16\\\n",
    "    --lora --lora_rank 4\n",
    "    # --freeze_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cae44b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-20 16:20:42,997] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/envs/ctp/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda113.so\n",
      "/opt/conda/envs/ctp/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /opt/conda/envs/ctp did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 113\n",
      "CUDA SETUP: Loading binary /opt/conda/envs/ctp/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda113.so...\n",
      "[2023-07-20 16:20:45,276] [WARNING] [runner.py:196:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
      "[2023-07-20 16:20:45,276] [INFO] [runner.py:555:main] cmd = /opt/conda/envs/ctp/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None ift.py --max-len 512 --instruct-data-path crumb/Clean-Instruct-440k --split train --data_range 5000 --load Salesforce/codet5p-16b --batch-size-per-replica 1 --eightbit --precision float16 --lora --lora_rank 4 --fp16 --deepspeed ds.json\n",
      "[2023-07-20 16:20:46,369] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/envs/ctp/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda113.so\n",
      "/opt/conda/envs/ctp/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /opt/conda/envs/ctp did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 113\n",
      "CUDA SETUP: Loading binary /opt/conda/envs/ctp/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda113.so...\n",
      "[2023-07-20 16:20:48,229] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1]}\n",
      "[2023-07-20 16:20:48,229] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0\n",
      "[2023-07-20 16:20:48,229] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})\n",
      "[2023-07-20 16:20:48,229] [INFO] [launch.py:163:main] dist_world_size=2\n",
      "[2023-07-20 16:20:48,229] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/envs/ctp/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda113.so\n",
      "/opt/conda/envs/ctp/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /opt/conda/envs/ctp did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 113\n",
      "CUDA SETUP: Loading binary /opt/conda/envs/ctp/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda113.so...\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/envs/ctp/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda113.so\n",
      "/opt/conda/envs/ctp/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /opt/conda/envs/ctp did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 113\n",
      "CUDA SETUP: Loading binary /opt/conda/envs/ctp/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda113.so...\n",
      "[2023-07-20 16:20:50,685] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2023-07-20 16:20:50,690] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "{'batch_size_per_replica': 1,\n",
      " 'cache_data': 'cache_data/instructions',\n",
      " 'data_num': -1,\n",
      " 'data_range': 5000,\n",
      " 'deepspeed': 'ds.json',\n",
      " 'eightbit': True,\n",
      " 'epochs': 1,\n",
      " 'fourbit': False,\n",
      " 'fp16': True,\n",
      " 'freeze_decoder': False,\n",
      " 'grad_acc_steps': 16,\n",
      " 'instruct_data_path': 'crumb/Clean-Instruct-440k',\n",
      " 'load': 'Salesforce/codet5p-16b',\n",
      " 'local_rank': 1,\n",
      " 'log_freq': 10,\n",
      " 'lora': True,\n",
      " 'lora_rank': 4,\n",
      " 'lr': 2e-05,\n",
      " 'lr_warmup_steps': 30,\n",
      " 'max_len': 512,\n",
      " 'precision': 'float16',\n",
      " 'save_dir': 'saved_models/experimental',\n",
      " 'save_freq': 500,\n",
      " 'split': 'train'}\n",
      "{'batch_size_per_replica': 1,\n",
      " 'cache_data': 'cache_data/instructions',\n",
      " 'data_num': -1,\n",
      " 'data_range': 5000,\n",
      " 'deepspeed': 'ds.json',\n",
      " 'eightbit': True,\n",
      " 'epochs': 1,\n",
      " 'fourbit': False,\n",
      " 'fp16': True,\n",
      " 'freeze_decoder': False,\n",
      " 'grad_acc_steps': 16,\n",
      " 'instruct_data_path': 'crumb/Clean-Instruct-440k',\n",
      " 'load': 'Salesforce/codet5p-16b',\n",
      " 'local_rank': 0,\n",
      " 'log_freq': 10,\n",
      " 'lora': True,\n",
      " 'lora_rank': 4,\n",
      " 'lr': 2e-05,\n",
      " 'lr_warmup_steps': 30,\n",
      " 'max_len': 512,\n",
      " 'precision': 'float16',\n",
      " 'save_dir': 'saved_models/experimental',\n",
      " 'save_freq': 500,\n",
      " 'split': 'train'}\n",
      "Found cached dataset parquet (/home/unnati/.cache/huggingface/datasets/crumb___parquet/crumb--Clean-Instruct-440k-cbf10416ad8c4248/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "Dataset Size: 961.89 MB\n",
      "Found cached dataset parquet (/home/unnati/.cache/huggingface/datasets/crumb___parquet/crumb--Clean-Instruct-440k-cbf10416ad8c4248/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "Dataset Size: 961.89 MB\n",
      "Map (num_proc=64):  94%|██████████▎| 4688/5000 [00:03<00:00, 1960.87 examples/s]  ==> Loaded 5000 samples\n",
      "  ==> Loaded 5000 samples                                                       \n",
      "Loading checkpoint shards: 100%|██████████████████| 5/5 [00:19<00:00,  3.96s/it]\n",
      "Model memory footprint:  15.92 GB\n",
      "  ==> Loaded model from Salesforce/codet5p-16b, model parameter count 15.36 B\n",
      "Loading checkpoint shards: 100%|██████████████████| 5/5 [00:19<00:00,  3.95s/it]\n",
      "Model memory footprint:  15.92 GB\n",
      "  ==> Loaded model from Salesforce/codet5p-16b, model parameter count 15.36 B\n",
      "PEFT Model memory footprint:  15.93 GB\n",
      "trainable params: 3.57 M || all params: 15.36 B || trainable%: 0.02269289989066938\n",
      "None\n",
      "Starting main loop\n",
      "[2023-07-20 16:21:47,974] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[2023-07-20 16:21:47,975] [INFO] [comm.py:594:init_distributed] cdb=None\n",
      "[2023-07-20 16:21:47,975] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "PEFT Model memory footprint:  15.93 GB\n",
      "trainable params: 3.57 M || all params: 15.36 B || trainable%: 0.02269289989066938\n",
      "None\n",
      "Starting main loop\n",
      "[2023-07-20 16:21:48,423] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[2023-07-20 16:21:48,424] [INFO] [comm.py:594:init_distributed] cdb=None\n",
      "Installed CUDA version 11.3 does not match the version torch was compiled with 11.6 but since the APIs are compatible, accepting this combination\n",
      "Installed CUDA version 11.3 does not match the version torch was compiled with 11.6 but since the APIs are compatible, accepting this combination\n",
      "Installed CUDA version 11.3 does not match the version torch was compiled with 11.6 but since the APIs are compatible, accepting this combination\n",
      "Using /home/unnati/.cache/torch_extensions/py310_cu116 as PyTorch extensions root...\n",
      "Installed CUDA version 11.3 does not match the version torch was compiled with 11.6 but since the APIs are compatible, accepting this combination\n",
      "Using /home/unnati/.cache/torch_extensions/py310_cu116 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/unnati/.cache/torch_extensions/py310_cu116/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.323974370956421 seconds\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.349724531173706 seconds\n",
      "Rank: 0 partition count [2] and sizes[(1871872, False)] \n",
      "Rank: 1 partition count [2] and sizes[(1871872, False)] \n",
      "  0%|                                                   | 0/156 [00:00<?, ?it/s]/opt/conda/envs/ctp/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/unnati/finetuneLLM/CodeT5+FineTuner/ift.py\", line 307, in <module>\n",
      "    main(args)\n",
      "  File \"/home/unnati/finetuneLLM/CodeT5+FineTuner/ift.py\", line 262, in main\n",
      "    run_training(args, model, train_data, tokenizer)\n",
      "  File \"/home/unnati/finetuneLLM/CodeT5+FineTuner/ift.py\", line 112, in run_training\n",
      "    trainer.train()\n",
      "  File \"/home/unnati/.local/lib/python3.10/site-packages/transformers/trainer.py\", line 1537, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/home/unnati/.local/lib/python3.10/site-packages/transformers/trainer.py\", line 1802, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs)\n",
      "  File \"/home/unnati/.local/lib/python3.10/site-packages/transformers/trainer.py\", line 2647, in training_step\n",
      "    loss = self.compute_loss(model, inputs)\n",
      "  File \"/home/unnati/.local/lib/python3.10/site-packages/transformers/trainer.py\", line 2672, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
      "    ret_val = func(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 1735, in forward\n",
      "    loss = self.module(*inputs, **kwargs)\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/peft/peft_model.py\", line 1015, in forward\n",
      "    return self.base_model(\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
      "    output = old_forward(*args, **kwargs)\n",
      "  File \"/home/unnati/.cache/huggingface/modules/transformers_modules/Salesforce/codet5p-16b/4481e2dd073f8ac9f8351b6cc0c5958e911f96f9/modeling_codet5p.py\", line 888, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
      "    output = old_forward(*args, **kwargs)\n",
      "  File \"/home/unnati/.cache/huggingface/modules/transformers_modules/Salesforce/codet5p-16b/4481e2dd073f8ac9f8351b6cc0c5958e911f96f9/modeling_codet5p.py\", line 522, in forward\n",
      "    inputs_embeds = self.wte(input_ids)\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
      "    output = old_forward(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/torch/nn/modules/sparse.py\", line 160, in forward\n",
      "    return F.embedding(\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/torch/nn/functional.py\", line 2210, in embedding\n",
      "    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n",
      "RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:0! (when checking argument for argument index in method wrapper__index_select)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/unnati/finetuneLLM/CodeT5+FineTuner/ift.py\", line 307, in <module>\n",
      "    main(args)\n",
      "  File \"/home/unnati/finetuneLLM/CodeT5+FineTuner/ift.py\", line 262, in main\n",
      "    run_training(args, model, train_data, tokenizer)\n",
      "  File \"/home/unnati/finetuneLLM/CodeT5+FineTuner/ift.py\", line 112, in run_training\n",
      "    trainer.train()\n",
      "  File \"/home/unnati/.local/lib/python3.10/site-packages/transformers/trainer.py\", line 1537, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/home/unnati/.local/lib/python3.10/site-packages/transformers/trainer.py\", line 1802, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs)\n",
      "  File \"/home/unnati/.local/lib/python3.10/site-packages/transformers/trainer.py\", line 2647, in training_step\n",
      "    loss = self.compute_loss(model, inputs)\n",
      "  File \"/home/unnati/.local/lib/python3.10/site-packages/transformers/trainer.py\", line 2672, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
      "    ret_val = func(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 1735, in forward\n",
      "    loss = self.module(*inputs, **kwargs)\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/peft/peft_model.py\", line 1015, in forward\n",
      "    return self.base_model(\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
      "    output = old_forward(*args, **kwargs)\n",
      "  File \"/home/unnati/.cache/huggingface/modules/transformers_modules/Salesforce/codet5p-16b/4481e2dd073f8ac9f8351b6cc0c5958e911f96f9/modeling_codet5p.py\", line 915, in forward\n",
      "    decoder_outputs = self.decoder(\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
      "    output = old_forward(*args, **kwargs)\n",
      "  File \"/home/unnati/.cache/huggingface/modules/transformers_modules/Salesforce/codet5p-16b/4481e2dd073f8ac9f8351b6cc0c5958e911f96f9/modeling_codet5p.py\", line 679, in forward\n",
      "    transformer_outputs = self.transformer(\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
      "    output = old_forward(*args, **kwargs)\n",
      "  File \"/home/unnati/.cache/huggingface/modules/transformers_modules/Salesforce/codet5p-16b/4481e2dd073f8ac9f8351b6cc0c5958e911f96f9/modeling_codet5p.py\", line 567, in forward\n",
      "    outputs = block(\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
      "    output = old_forward(*args, **kwargs)\n",
      "  File \"/home/unnati/.cache/huggingface/modules/transformers_modules/Salesforce/codet5p-16b/4481e2dd073f8ac9f8351b6cc0c5958e911f96f9/modeling_codet5p.py\", line 339, in forward\n",
      "    feed_forward_hidden_states = self.mlp(hidden_states)\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
      "    output = old_forward(*args, **kwargs)\n",
      "  File \"/home/unnati/.cache/huggingface/modules/transformers_modules/Salesforce/codet5p-16b/4481e2dd073f8ac9f8351b6cc0c5958e911f96f9/modeling_codet5p.py\", line 291, in forward\n",
      "    hidden_states = self.fc_out(hidden_states)\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
      "    output = old_forward(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/bitsandbytes/nn/modules.py\", line 402, in forward\n",
      "    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py\", line 562, in matmul\n",
      "    return MatMul8bitLt.apply(A, B, out, bias, state)\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py\", line 343, in forward\n",
      "    state.CxB, state.SB = F.transform(state.CB, to_order=formatB)\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/bitsandbytes/functional.py\", line 2072, in transform\n",
      "    if out is None: out, new_state = get_transform_buffer(state[0], A.dtype, A.device, to_order, state[1], transpose)\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/bitsandbytes/functional.py\", line 469, in get_transform_buffer\n",
      "    return init_func((rows, cols), dtype=dtype, device=device), state\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.41 GiB total capacity; 20.62 GiB already allocated; 71.56 MiB free; 20.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  0%|                                                   | 0/156 [00:00<?, ?it/s]\n",
      "[2023-07-20 16:21:58,306] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 11533\n",
      "[2023-07-20 16:21:58,311] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 11534\n",
      "[2023-07-20 16:21:58,311] [ERROR] [launch.py:321:sigkill_handler] ['/opt/conda/envs/ctp/bin/python3.10', '-u', 'ift.py', '--local_rank=1', '--max-len', '512', '--instruct-data-path', 'crumb/Clean-Instruct-440k', '--split', 'train', '--data_range', '5000', '--load', 'Salesforce/codet5p-16b', '--batch-size-per-replica', '1', '--eightbit', '--precision', 'float16', '--lora', '--lora_rank', '4', '--fp16', '--deepspeed', 'ds.json'] exits with return code = 1\n"
     ]
    }
   ],
   "source": [
    "!deepspeed --num_gpus 2 ift.py --max-len 512\\\n",
    "    --instruct-data-path crumb/Clean-Instruct-440k\\\n",
    "    --split train\\\n",
    "    --data_range 5000\\\n",
    "    --load Salesforce/codet5p-16b\\\n",
    "    --batch-size-per-replica 1\\\n",
    "    --eightbit\\\n",
    "    --precision float16\\\n",
    "    --lora --lora_rank 4\\\n",
    "    --fp16\\\n",
    "    --deepspeed ds.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a8ee5053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/envs/ctp/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda113.so\n",
      "/opt/conda/envs/ctp/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /opt/conda/envs/ctp did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 113\n",
      "CUDA SETUP: Loading binary /opt/conda/envs/ctp/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda113.so...\n",
      "bin /opt/conda/envs/ctp/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda113.so\n",
      "/opt/conda/envs/ctp/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /opt/conda/envs/ctp did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 113\n",
      "CUDA SETUP: Loading binary /opt/conda/envs/ctp/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda113.so...\n",
      "[2023-07-20 16:27:24,616] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2023-07-20 16:27:24,617] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "{'batch_size_per_replica': 1,\n",
      " 'cache_data': 'cache_data/instructions',\n",
      " 'data_num': -1,\n",
      " 'data_range': 5000,\n",
      " 'deepspeed': None,\n",
      " 'eightbit': True,\n",
      " 'epochs': 1,\n",
      " 'fourbit': False,\n",
      " 'fp16': True,\n",
      " 'freeze_decoder': False,\n",
      " 'grad_acc_steps': 16,\n",
      " 'instruct_data_path': 'crumb/Clean-Instruct-440k',\n",
      " 'load': 'Salesforce/codet5p-16b',\n",
      " 'local_rank': 0,\n",
      " 'log_freq': 10,\n",
      " 'lora': True,\n",
      " 'lora_rank': 4,\n",
      " 'lr': 2e-05,\n",
      " 'lr_warmup_steps': 30,\n",
      " 'max_len': 512,\n",
      " 'precision': 'float16',\n",
      " 'save_dir': 'saved_models/experimental',\n",
      " 'save_freq': 500,\n",
      " 'split': 'train'}\n",
      "{'batch_size_per_replica': 1,\n",
      " 'cache_data': 'cache_data/instructions',\n",
      " 'data_num': -1,\n",
      " 'data_range': 5000,\n",
      " 'deepspeed': None,\n",
      " 'eightbit': True,\n",
      " 'epochs': 1,\n",
      " 'fourbit': False,\n",
      " 'fp16': True,\n",
      " 'freeze_decoder': False,\n",
      " 'grad_acc_steps': 16,\n",
      " 'instruct_data_path': 'crumb/Clean-Instruct-440k',\n",
      " 'load': 'Salesforce/codet5p-16b',\n",
      " 'local_rank': 0,\n",
      " 'log_freq': 10,\n",
      " 'lora': True,\n",
      " 'lora_rank': 4,\n",
      " 'lr': 2e-05,\n",
      " 'lr_warmup_steps': 30,\n",
      " 'max_len': 512,\n",
      " 'precision': 'float16',\n",
      " 'save_dir': 'saved_models/experimental',\n",
      " 'save_freq': 500,\n",
      " 'split': 'train'}\n",
      "Found cached dataset parquet (/home/unnati/.cache/huggingface/datasets/crumb___parquet/crumb--Clean-Instruct-440k-cbf10416ad8c4248/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "Dataset Size: 961.89 MB\n",
      "Found cached dataset parquet (/home/unnati/.cache/huggingface/datasets/crumb___parquet/crumb--Clean-Instruct-440k-cbf10416ad8c4248/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "Dataset Size: 961.89 MB\n",
      "Map (num_proc=64):  97%|██████████▋| 4844/5000 [00:03<00:00, 2341.57 examples/s]  ==> Loaded 5000 samples\n",
      "  ==> Loaded 5000 samples                                                       \n",
      "Loading checkpoint shards: 100%|██████████████████| 5/5 [00:19<00:00,  3.94s/it]\n",
      "Model memory footprint:  15.92 GB\n",
      "  ==> Loaded model from Salesforce/codet5p-16b, model parameter count 15.36 B\n",
      "Loading checkpoint shards: 100%|██████████████████| 5/5 [00:19<00:00,  3.98s/it]\n",
      "Model memory footprint:  15.92 GB\n",
      "  ==> Loaded model from Salesforce/codet5p-16b, model parameter count 15.36 B\n",
      "PEFT Model memory footprint:  15.93 GB\n",
      "trainable params: 3.57 M || all params: 15.36 B || trainable%: 0.02269289989066938\n",
      "None\n",
      "Starting main loop\n",
      "/home/unnati/.local/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "PEFT Model memory footprint:  15.93 GB\n",
      "trainable params: 3.57 M || all params: 15.36 B || trainable%: 0.02269289989066938\n",
      "None\n",
      "Starting main loop\n",
      "/home/unnati/.local/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|                                                   | 0/156 [00:00<?, ?it/s]/opt/conda/envs/ctp/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/opt/conda/envs/ctp/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "{'loss': 4.3942, 'learning_rate': 6.666666666666667e-07, 'epoch': 0.01}         \n",
      "{'loss': 50.5742, 'learning_rate': 6.666666666666667e-06, 'epoch': 0.06}        \n",
      "  9%|███▊                                      | 14/156 [03:58<40:00, 16.91s/it]^C\n",
      "WARNING:torch.distributed.elastic.agent.server.api:Received 2 death signal, shutting down workers\n",
      "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 22835 closing signal SIGINT\n",
      "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 22836 closing signal SIGINT\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/unnati/finetuneLLM/CodeT5+FineTuner/ift.py\", line 307, in <module>\n",
      "    main(args)\n",
      "  File \"/home/unnati/finetuneLLM/CodeT5+FineTuner/ift.py\", line 262, in main\n",
      "    run_training(args, model, train_data, tokenizer)\n",
      "  File \"/home/unnati/finetuneLLM/CodeT5+FineTuner/ift.py\", line 112, in run_training\n",
      "    trainer.train()\n",
      "  File \"/home/unnati/.local/lib/python3.10/site-packages/transformers/trainer.py\", line 1537, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/home/unnati/.local/lib/python3.10/site-packages/transformers/trainer.py\", line 1802, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs)\n",
      "  File \"/home/unnati/.local/lib/python3.10/site-packages/transformers/trainer.py\", line 2658, in training_step\n",
      "    self.accelerator.backward(loss)\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/accelerate/accelerator.py\", line 1842, in backward\n",
      "    self.scaler.scale(loss).backward(**kwargs)\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/torch/_tensor.py\", line 488, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 197, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/unnati/finetuneLLM/CodeT5+FineTuner/ift.py\", line 307, in <module>\n",
      "    main(args)\n",
      "  File \"/home/unnati/finetuneLLM/CodeT5+FineTuner/ift.py\", line 262, in main\n",
      "    run_training(args, model, train_data, tokenizer)\n",
      "  File \"/home/unnati/finetuneLLM/CodeT5+FineTuner/ift.py\", line 112, in run_training\n",
      "    trainer.train()\n",
      "  File \"/home/unnati/.local/lib/python3.10/site-packages/transformers/trainer.py\", line 1537, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/home/unnati/.local/lib/python3.10/site-packages/transformers/trainer.py\", line 1802, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs)\n",
      "  File \"/home/unnati/.local/lib/python3.10/site-packages/transformers/trainer.py\", line 2658, in training_step\n",
      "    self.accelerator.backward(loss)\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/accelerate/accelerator.py\", line 1842, in backward\n",
      "    self.scaler.scale(loss).backward(**kwargs)\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/torch/_tensor.py\", line 488, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/opt/conda/envs/ctp/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 197, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n",
      "  9%|███▊                                      | 14/156 [04:11<42:30, 17.96s/it]\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nproc_per_node 2 ift.py --max-len 512\\\n",
    "    --instruct-data-path crumb/Clean-Instruct-440k\\\n",
    "    --split train\\\n",
    "    --data_range 5000\\\n",
    "    --load Salesforce/codet5p-16b\\\n",
    "    --batch-size-per-replica 1\\\n",
    "    --eightbit\\\n",
    "    --precision float16\\\n",
    "    --lora --lora_rank 4 --fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c341a1ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a189f2b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078c5b4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4487d391",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d519a8a5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install --user --upgrade git+https://github.com/huggingface/transformers.git\n",
    "!pip install --upgrade git+https://github.com/huggingface/accelerate.git\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24ebef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ff1a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bitsandbytes\n",
    "!pip install git+https://github.com/huggingface/peft.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26c49f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b699e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, TrainingArguments, Trainer, AutoConfig\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b389266",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_num=-1\n",
    "max_len = 512\n",
    "cache_data = 'cache_data/full_instruct_set'\n",
    "load = 'Salesforce/codet5p-16b'\n",
    "device = \"cuda\"\n",
    "# Training\n",
    "epochs=1\n",
    "lr=2e-5\n",
    "lr_warmup_steps=30\n",
    "batch_size_per_replica=1\n",
    "grad_acc_steps=16\n",
    "local_rank=-1\n",
    "deepspeed=\"ds.json\"\n",
    "fp16=True\n",
    "\n",
    "# Logging and stuff\n",
    "save_dir=\"saved_models/full_instruct_set\"\n",
    "log_freq=10\n",
    "save_freq=500\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "final_checkpoint_dir = os.path.join(save_dir, \"final_checkpoint\")\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0eca5ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8cacc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(model, train_data):\n",
    "    print(f\"Starting main loop\")\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        report_to='tensorboard',\n",
    "        output_dir=save_dir,\n",
    "        overwrite_output_dir=False,\n",
    "\n",
    "        do_train=True,\n",
    "        save_strategy='epoch',\n",
    "\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=batch_size_per_replica,\n",
    "        gradient_accumulation_steps=grad_acc_steps,\n",
    "        learning_rate=lr,\n",
    "        weight_decay=0.05,\n",
    "        warmup_steps=lr_warmup_steps,\n",
    "\n",
    "        logging_dir=save_dir,\n",
    "        logging_first_step=True,\n",
    "        logging_steps=log_freq,\n",
    "        save_total_limit=1,\n",
    "\n",
    "        dataloader_drop_last=True,\n",
    "        dataloader_num_workers=2,\n",
    "\n",
    "        local_rank=local_rank,\n",
    "        # deepspeed=deepspeed,\n",
    "#         fp16=fp16,\n",
    "        bf16 = True\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_data,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    if local_rank in [0, -1]:\n",
    "        model.save_pretrained(final_checkpoint_dir)\n",
    "        print(f'  ==> Finish training and save to {final_checkpoint_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624ad6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for deepspeed\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '9994' # modify if RuntimeError: Address already in use\n",
    "os.environ['RANK'] = \"0\"\n",
    "os.environ['LOCAL_RANK'] = \"0\"\n",
    "os.environ['WORLD_SIZE'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37956e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(load)\n",
    "config = AutoConfig.from_pretrained(load, trust_remote_code=True, revision=\"main\")\n",
    "config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "config.pad_token_id = tokenizer.pad_token_id\n",
    "#for deepspeed\n",
    "config.max_position_embeddings = max_len\n",
    "# print('Model hidden size: ', config.cross_attention_hidden_size)\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "def get_model_size(model):\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    model_size = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    return \"{}M\".format(round(model_size / 1e+6))\n",
    "\n",
    "def freeze_decoder_except_xattn_codegen(model):\n",
    "    print(f'Para before freezing: {model.num_parameters()}, trainable para: {get_model_size(model)}')\n",
    "    \n",
    "    for param in model.decoder.parameters():\n",
    "#         print(param, param.requires_grad)\n",
    "        param.requires_grad = False\n",
    "\n",
    "    num_decoder_layers = model.decoder.config.n_layer\n",
    "    for i in range(num_decoder_layers):\n",
    "        each_decoder_layer = model.decoder.transformer.h[i]\n",
    "        if hasattr(each_decoder_layer, 'crossattention'):\n",
    "            each_decoder_layer.crossattention.to(torch.float32)\n",
    "            continue\n",
    "#             for param in each_decoder_layer.crossattention.parameters():\n",
    "#                 param.requires_grad = True\n",
    "        if hasattr(each_decoder_layer, 'alpha_xattn'):\n",
    "            each_decoder_layer.alpha_xattn.to(torch.float32)\n",
    "#             each_decoder_layer.alpha_xattn.requires_grad = True\n",
    "            continue\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    print(f'Para after freezing: {model.num_parameters()}, trainable para: {get_model_size(model)}')\n",
    "\n",
    "def convert_size(size_bytes):\n",
    "   if size_bytes == 0:\n",
    "       return \"0B\"\n",
    "   size_name = (\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\", \"EB\", \"ZB\", \"YB\")\n",
    "   i = int(math.floor(math.log(size_bytes, 1024)))\n",
    "   p = math.pow(1024, i)\n",
    "   s = round(size_bytes / p, 2)\n",
    "   return \"%s %s\" % (s, size_name[i])\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    prompt_input, prompt_no_input = PROMPT_DICT[\"prompt_input\"], PROMPT_DICT[\"prompt_no_input\"]\n",
    "    source = [prompt_input.format_map({'instruction': instruct, 'input': inp}) if inp != ''\n",
    "                      else prompt_no_input.format_map({'instruction': instruct})\n",
    "                      for instruct, inp in zip(examples[\"instruction\"], examples[\"input\"])]\n",
    "    target = [src + output + tokenizer.eos_token for src, output in zip(source, examples[\"output\"])]\n",
    "\n",
    "    model_inputs = tokenizer(source, max_length=max_len, padding=\"max_length\", truncation=True)\n",
    "    labels = tokenizer(target, max_length=max_len, padding=\"max_length\", truncation=True)\n",
    "    model_inputs[\"decoder_input_ids\"] = copy.deepcopy(labels[\"input_ids\"])\n",
    "\n",
    "    # changing labels: convert all tokens in the duplicate prefix prompt and the padding part to -100\n",
    "    eos_token_id = tokenizer.eos_token_id\n",
    "    for x, y in zip(model_inputs[\"input_ids\"], labels[\"input_ids\"]):\n",
    "        label_prefix_len = x.index(eos_token_id) if eos_token_id in x else len(x)\n",
    "        y[:label_prefix_len] = [-100] * label_prefix_len\n",
    "\n",
    "        if eos_token_id in y:\n",
    "            pad_len = len(y) - y.index(eos_token_id) - 1\n",
    "            if pad_len > 0:\n",
    "                y[y.index(eos_token_id) + 1:] = [-100] * pad_len\n",
    "\n",
    "    # shift labels to the right as the decoder input and add decoder start token id\n",
    "    decoder_start_id = tokenizer.eos_token_id\n",
    "    for z in model_inputs[\"decoder_input_ids\"]:\n",
    "        z[1:] = z[:-1]\n",
    "        z[0] = decoder_start_id\n",
    "\n",
    "    model_inputs[\"labels\"] = copy.deepcopy(labels[\"input_ids\"])\n",
    "    model_inputs[\"decoder_attention_mask\"] = labels[\"attention_mask\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "\n",
    "def load_tokenize_data():\n",
    "#     if os.path.exists(cache_data):\n",
    "#         train_data = load_from_disk(cache_data)\n",
    "#         print(f'  ==> Loaded {len(train_data)} samples')\n",
    "# #         res =  convert_size(train_data.size_in_bytes)\n",
    "# #         print('Dataset Size:', res)\n",
    "#         return train_data, config\n",
    "#     else:\n",
    "        datasets = load_dataset(\"crumb/Clean-Instruct-440k\", split=\"train\")\n",
    "#         datasets = load_dataset(\"semeru/text-code-codesummarization\", split=\"validation\")\n",
    "        datasets = datasets.select(range(200000))\n",
    "        res =  convert_size(datasets.size_in_bytes)\n",
    "        print('Dataset Size:', res)\n",
    "        train_data = datasets.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            remove_columns=datasets.column_names,\n",
    "            num_proc=64,\n",
    "            load_from_cache_file=False,\n",
    "        )\n",
    "        print(f'  ==> Loaded {len(train_data)} samples')\n",
    "        # train_data.save_to_disk(cache_data)\n",
    "        # print(f'  ==> Saved to {cache_data}')\n",
    "\n",
    "        return train_data, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99044549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abce7518",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, config = load_tokenize_data()\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e4971c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#LORA\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "# lora_config = LoraConfig(\n",
    "#     r=16,\n",
    "#     lora_alpha=32,\n",
    "#     target_modules=[\"q_proj\",\"v_proj\"],\n",
    "#     lora_dropout=0.05,\n",
    "#     bias=\"none\",\n",
    "#     task_type=TaskType.SEQ_2_SEQ_LM\n",
    "# )\n",
    "\n",
    "\n",
    "#QLORA\n",
    "\n",
    "\n",
    "\n",
    "qlora_config = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    target_modules=[\"q_proj\",\"v_proj\"]\n",
    ")\n",
    "\n",
    "\n",
    "if data_num != -1:\n",
    "    train_data = train_data.select([i for i in range(data_num)])\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(load,config=config,trust_remote_code=True,\n",
    "                                              revision=\"main\",\n",
    "                                              torch_dtype = torch.bfloat16,\n",
    "                                              load_in_8bit=True,\n",
    "                                              low_cpu_mem_usage=True, \n",
    "                                              quantization_config=bnb_config)\n",
    "# freeze_decoder_except_xattn_codegen(model)\n",
    "print('Model: ', convert_size(model.get_memory_footprint()))\n",
    "model = get_peft_model(model, qlora_config)\n",
    "print('PEFT Model: ',convert_size(model.get_memory_footprint()))\n",
    "print(model.print_trainable_parameters())\n",
    "\n",
    "print(f\"  ==> Loaded model from {load}, model size {model.num_parameters()}\")\n",
    "\n",
    "run_training(model, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61aca55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(final_checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2be312",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > urqts.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2cf530",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf6a7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "import time, os\n",
    "\n",
    "load = \"Salesforce/instructcodet5p-16b\"\n",
    "device = \"cuda\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n",
    "    ),\n",
    "}\n",
    "max_len = 512\n",
    "\n",
    "import re\n",
    "def truncate(completion):\n",
    "    import re\n",
    "    \n",
    "    def find_re(string, pattern, start_pos):\n",
    "        m = pattern.search(string, start_pos)\n",
    "        return m.start() if m else -1\n",
    "\n",
    "    terminals = [re.compile(r, re.MULTILINE) for r in [re.escape('<|end|>'),\"^'''\", '^\"\"\"', '\\n\\n\\n']]\n",
    "\n",
    "    prints = list(re.finditer('^print', completion, re.MULTILINE))\n",
    "    if len(prints) > 1:\n",
    "        completion = completion[:prints[1].start()]\n",
    "\n",
    "    defs = list(re.finditer('^def', completion, re.MULTILINE))\n",
    "    if len(defs) > 1:\n",
    "        completion = completion[:defs[1].start()]\n",
    "\n",
    "    start_pos = 0\n",
    "\n",
    "    terminals_pos = [pos for pos in [find_re(completion, terminal, start_pos) for terminal in terminals] if pos != -1]\n",
    "    if len(terminals_pos) > 0:\n",
    "        return completion[:min(terminals_pos)]\n",
    "    else:\n",
    "        return completion  \n",
    "\n",
    "def preprocess_test_ip(test_ip):\n",
    "    prompt_input, prompt_no_input = PROMPT_DICT[\"prompt_input\"], PROMPT_DICT[\"prompt_no_input\"]\n",
    "    instruct, inp = test_ip[\"instruction\"], test_ip[\"input\"]\n",
    "    if inp!=\"\":\n",
    "        source = prompt_input.format_map({'instruction': instruct, 'input': inp})\n",
    "    else:\n",
    "        source = prompt_no_input.format_map({'instruction': instruct})\n",
    "    return source\n",
    "\n",
    "test_ip = {\"instruction\":\"\"\"I want to drop a specific row from a dataset. Provide me all the possible ways to achieve the task.\n",
    "\"\"\", \"input\":\"\"}\n",
    "\n",
    "fmt_test_ip = preprocess_test_ip(test_ip)\n",
    "print(fmt_test_ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e61c201",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_output = \"\"\"\\\n",
    "\"To position the text div from the bottom, you can use the CSS property `top` instead of `bottom`. \n",
    "This will position the top part of the div at a certain distance from the top of the parent element. \n",
    "To make the div overlap the image, you can also use the CSS property `z-index` to ensure that the text div is positioned on \n",
    "top of the image. \n",
    "Here's an updated CSS code:\n",
    "``` #article-txt { min-height: inherit; position: absolute; top: 50%; right: 0; transform: translateY(-50%); z-index: 1; } \n",
    "#article-img { display: block; position: relative; z-index: 0; } ``` \n",
    "The `top: 50%` positions the top edge of the text div at 50% of the parent element's height. \n",
    "The `transform: translateY(-50%)` centers the text div vertically. \n",
    "The `z-index: 1` sets the text div to be positioned on top of the image, which has a default `z-index` of 0. \n",
    "To make the text div overhang the bottom-right corner of the image, you can adjust the `right` value of the text div to \n",
    "control how much of the div sticks out past the right edge of the image: \n",
    "``` #article-txt { min-height: inherit; position: absolute; top: 50%; right: -20%; transform: translateY(-50%); z-index: 1; } ``` \n",
    "In this example, the text div's right edge is positioned 20% to the left of the right edge of the parent element, which would \n",
    "make it overhang the bottom-right corner of the image. Adjust the value as needed to achieve your desired effect\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8e41ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(actual_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda566f4",
   "metadata": {},
   "source": [
    "# Inference without fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6cd1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(load)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(load,\n",
    "                                              torch_dtype=torch.bfloat16,\n",
    "                                              low_cpu_mem_usage=True,\n",
    "                                              trust_remote_code=True,\n",
    "                                              quantization_config=bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f16d1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inf_without_ft(test_ip):\n",
    "    start_time= time.time()\n",
    "    fmt_test_ip = preprocess_test_ip(test_ip)\n",
    "    encoding = tokenizer(fmt_test_ip, return_tensors=\"pt\").to(device)\n",
    "    encoding['decoder_input_ids'] = encoding['input_ids'].clone()\n",
    "    outputs = model.generate(**encoding, max_length=max_len)\n",
    "    resp = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    stop_time=time.time()\n",
    "    duration =stop_time - start_time\n",
    "    return resp, duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b7cbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp, duration =inf_without_ft(test_ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07801fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(duration)+' s')\n",
    "print(resp[len(fmt_test_ip):])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9334f22",
   "metadata": {},
   "source": [
    "# Inference with fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29c2f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import BitsAndBytesConfig\n",
    "import os\n",
    "import torch\n",
    "\n",
    "save_dir=\"saved_models/full_instruct_set\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "final_checkpoint_dir = os.path.join(save_dir, \"final_checkpoint\")\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "\n",
    "max_len = 512\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    load,\n",
    "    return_dict=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    load_in_8bit=True\n",
    "    # quantization_config=bnb_config\n",
    ")\n",
    "ft_model = PeftModel.from_pretrained(base_model, final_checkpoint_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(final_checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f53fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inf_with_ft(test_ip):\n",
    "    start_time= time.time()\n",
    "    fmt_test_ip = preprocess_test_ip(test_ip)\n",
    "    encoding = tokenizer(fmt_test_ip, return_tensors=\"pt\").to(device)\n",
    "    encoding['decoder_input_ids'] = encoding['input_ids'].clone()\n",
    "    outputs = ft_model.generate(**encoding, max_length=max_len)\n",
    "    resp = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    stop_time=time.time()\n",
    "    duration =stop_time - start_time\n",
    "    return resp, duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24284c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp, duration =inf_with_ft(test_ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7520d24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(duration)+' s')\n",
    "print(resp[len(fmt_test_ip):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaef3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(duration)+' s')\n",
    "print(resp[len(fmt_test_ip):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25186d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(duration)+' s')\n",
    "print(resp[len(fmt_test_ip):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239a2820",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(duration)+' s')\n",
    "print(resp[len(fmt_test_ip):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940a4509",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'A': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    " 'B': [10, 9, 8, 7, 6, 5, 4, 3, 2, 1]})\n",
    "\n",
    "df.drop(df.index[5], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9950305",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98529bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83cbed0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68723cc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac43b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time= time.time()\n",
    "ip = \"\"\" How to Implement using recursion and cut-off cycle of the counter (like `for i: = 1 downto N do <operator>`) ?\"\"\"\n",
    "\n",
    "encoding = tokenizer(ip, return_tensors=\"pt\").to(device)\n",
    "encoding['decoder_input_ids'] = encoding['input_ids'].clone()\n",
    "outputs = ft_model.generate(**encoding, max_length=1024)\n",
    "stop_time=time.time()\n",
    "duration =stop_time - start_time\n",
    "print(duration)\n",
    "op = tokenizer.decode(outputs[0], skip_special_tokens=True)[len(ip):]\n",
    "print(truncate(op))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a114cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635cbe6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfcac75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baae62a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8d708e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9502d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model =  AutoModelForSeq2SeqLM.from_pretrained(save_dir,local_files_only=True,config=config,trust_remote_code=True,\n",
    "                                              revision=\"main\", \n",
    "                                              low_cpu_mem_usage=True, \n",
    "                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c3e1c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
