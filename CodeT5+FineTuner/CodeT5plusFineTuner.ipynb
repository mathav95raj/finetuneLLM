{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d519a8a5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install --user --upgrade git+https://github.com/huggingface/transformers.git\n",
    "!pip install --upgrade git+https://github.com/huggingface/accelerate.git\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24ebef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ff1a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bitsandbytes\n",
    "!pip install git+https://github.com/huggingface/peft.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f26c49f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b699e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/envs/ctp/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda113.so\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 113\n",
      "CUDA SETUP: Loading binary /opt/conda/envs/ctp/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda113.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/ctp/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /opt/conda/envs/ctp did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-14 06:34:42,479] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, TrainingArguments, Trainer, AutoConfig\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b389266",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_num=-1\n",
    "max_source_len = 512\n",
    "max_target_len=512\n",
    "cache_data = 'cache_data/summarize_python'\n",
    "load = 'Salesforce/codet5p-16b'\n",
    "# Training\n",
    "epochs=1\n",
    "lr=5e-3\n",
    "lr_warmup_steps=200\n",
    "batch_size_per_replica=1\n",
    "grad_acc_steps=16\n",
    "local_rank=-1\n",
    "deepspeed=\"ds.json\"\n",
    "fp16=True\n",
    "# Logging and stuff\n",
    "save_dir=\"saved_models/normal_fine_tune\"\n",
    "log_freq=10\n",
    "save_freq=500\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba8cacc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(model, train_data):\n",
    "    print(f\"Starting main loop\")\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        report_to='tensorboard',\n",
    "        output_dir=save_dir,\n",
    "        overwrite_output_dir=False,\n",
    "\n",
    "        do_train=True,\n",
    "        save_strategy='epoch',\n",
    "\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=batch_size_per_replica,\n",
    "        gradient_accumulation_steps=grad_acc_steps,\n",
    "        learning_rate=lr,\n",
    "        weight_decay=0.05,\n",
    "        warmup_steps=lr_warmup_steps,\n",
    "\n",
    "        logging_dir=save_dir,\n",
    "        logging_first_step=True,\n",
    "        logging_steps=log_freq,\n",
    "        save_total_limit=1,\n",
    "\n",
    "        dataloader_drop_last=True,\n",
    "        dataloader_num_workers=2,\n",
    "\n",
    "        local_rank=local_rank,\n",
    "#         deepspeed=deepspeed,\n",
    "#         fp16=fp16,\n",
    "        bf16 = True\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_data,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    if local_rank in [0, -1]:\n",
    "        final_checkpoint_dir = os.path.join(save_dir, \"final_checkpoint\")\n",
    "        model.save_pretrained(final_checkpoint_dir)\n",
    "        print(f'  ==> Finish training and save to {final_checkpoint_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "624ad6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for deepspeed\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '9994' # modify if RuntimeError: Address already in use\n",
    "os.environ['RANK'] = \"0\"\n",
    "os.environ['LOCAL_RANK'] = \"0\"\n",
    "os.environ['WORLD_SIZE'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37956e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(load)\n",
    "config = AutoConfig.from_pretrained(load, trust_remote_code=True, revision=\"main\")\n",
    "config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "config.pad_token_id = tokenizer.pad_token_id\n",
    "#for deepspeed\n",
    "config.max_position_embeddings = max_source_len\n",
    "# print('Model hidden size: ', config.cross_attention_hidden_size)\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "def get_model_size(model):\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    model_size = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    return \"{}M\".format(round(model_size / 1e+6))\n",
    "\n",
    "def freeze_decoder_except_xattn_codegen(model):\n",
    "    print(f'Para before freezing: {model.num_parameters()}, trainable para: {get_model_size(model)}')\n",
    "    for param in model.decoder.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    num_decoder_layers = model.decoder.config.n_layer\n",
    "    for i in range(num_decoder_layers):\n",
    "        each_decoder_layer = model.decoder.transformer.h[i]\n",
    "        if hasattr(each_decoder_layer, 'crossattention'):\n",
    "            for param in each_decoder_layer.crossattention.parameters():\n",
    "                param.requires_grad = True\n",
    "            each_decoder_layer.crossattention.to(torch.float32)\n",
    "\n",
    "        if hasattr(each_decoder_layer, 'alpha_xattn'):\n",
    "            each_decoder_layer.alpha_xattn.requires_grad = True\n",
    "    print(f'Para after freezing: {model.num_parameters()}, trainable para: {get_model_size(model)}')\n",
    "\n",
    "def convert_size(size_bytes):\n",
    "   if size_bytes == 0:\n",
    "       return \"0B\"\n",
    "   size_name = (\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\", \"EB\", \"ZB\", \"YB\")\n",
    "   i = int(math.floor(math.log(size_bytes, 1024)))\n",
    "   p = math.pow(1024, i)\n",
    "   s = round(size_bytes / p, 2)\n",
    "   return \"%s %s\" % (s, size_name[i])\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # source = [ex for ex in examples[\"instruction\"]]\n",
    "    source = [ex for ex in examples[\"input\"]]\n",
    "    target = [ex for ex in examples[\"output\"]]\n",
    "\n",
    "    model_inputs = tokenizer(source, max_length=max_source_len, padding=\"max_length\", truncation=True)\n",
    "    labels = tokenizer(target, max_length=max_target_len, padding=\"max_length\", truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"].copy()\n",
    "    model_inputs[\"labels\"] = [\n",
    "        [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in model_inputs[\"labels\"]\n",
    "    ]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def load_tokenize_data():\n",
    "#     if os.path.exists(cache_data):\n",
    "#         train_data = load_from_disk(cache_data)\n",
    "#         print(f'  ==> Loaded {len(train_data)} samples')\n",
    "# #         res =  convert_size(train_data.size_in_bytes)\n",
    "# #         print('Dataset Size:', res)\n",
    "#         return train_data, config\n",
    "#     else:\n",
    "        # datasets = load_dataset(\"crumb/Clean-Instruct-440k\", split=\"train\")\n",
    "        datasets = load_dataset(\"semeru/text-code-codesummarization\", split=\"validation\")\n",
    "        datasets = datasets.select(range(5000))\n",
    "        res =  convert_size(datasets.size_in_bytes)\n",
    "        print('Dataset Size:', res)\n",
    "        train_data = datasets.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            remove_columns=datasets.column_names,\n",
    "            num_proc=64,\n",
    "            load_from_cache_file=False,\n",
    "        )\n",
    "        print(f'  ==> Loaded {len(train_data)} samples')\n",
    "        # train_data.save_to_disk(cache_data)\n",
    "        # print(f'  ==> Saved to {cache_data}')\n",
    "\n",
    "        return train_data, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99044549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62e4971c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/unnati/.cache/huggingface/datasets/semeru___parquet/semeru--text-code-codesummarization-8e2c65491861176a/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Size: 57.16 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "528ce62e53e8469dbb3aa6f3e000739e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=64):   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Loaded 5000 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6b03e85e25841cea353aedbd2ae7e69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  15.92 GB\n",
      "PEFT Model:  15.93 GB\n",
      "trainable params: 3,743,744 || all params: 16,497,424,384 || trainable%: 0.02269289989066938\n",
      "None\n",
      "  ==> Loaded model from Salesforce/codet5p-16b, model size 16497424384\n",
      "Starting main loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/unnati/.local/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/ctp/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/opt/conda/envs/ctp/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11' max='312' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 11/312 04:07 < 2:17:57, 0.04 it/s, Epoch 0.03/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.419300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data, config = load_tokenize_data()\n",
    "\n",
    "#LORA\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "# lora_config = LoraConfig(\n",
    "#     r=16,\n",
    "#     lora_alpha=32,\n",
    "#     target_modules=[\"q_proj\",\"v_proj\"],\n",
    "#     lora_dropout=0.05,\n",
    "#     bias=\"none\",\n",
    "#     task_type=TaskType.SEQ_2_SEQ_LM\n",
    "# )\n",
    "\n",
    "\n",
    "#QLORA\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "qlora_config = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    target_modules=[\"q_proj\",\"v_proj\"]\n",
    ")\n",
    "\n",
    "\n",
    "if data_num != -1:\n",
    "    train_data = train_data.select([i for i in range(data_num)])\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(load,config=config,trust_remote_code=True,\n",
    "                                              revision=\"main\",\n",
    "                                              torch_dtype = torch.bfloat16, \n",
    "                                              low_cpu_mem_usage=True, \n",
    "                                              load_in_8bit=True)\n",
    "                                            #   quantization_config=bnb_config)\n",
    "# freeze_decoder_except_xattn_codegen(model)\n",
    "print('Model: ', convert_size(model.get_memory_footprint()))\n",
    "model = get_peft_model(model, qlora_config)\n",
    "print('PEFT Model: ',convert_size(model.get_memory_footprint()))\n",
    "print(model.print_trainable_parameters())\n",
    "\n",
    "print(f\"  ==> Loaded model from {load}, model size {model.num_parameters()}\")\n",
    "\n",
    "run_training(model, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4272d99d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d186a72b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dcf65c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329a90e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2be312",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > urqts.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9502d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
