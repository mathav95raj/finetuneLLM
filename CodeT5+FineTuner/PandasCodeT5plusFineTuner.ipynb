{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d519a8a5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install --user --upgrade git+https://github.com/huggingface/transformers.git\n",
    "!pip install --upgrade git+https://github.com/huggingface/accelerate.git\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24ebef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ff1a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bitsandbytes\n",
    "!pip install git+https://github.com/huggingface/peft.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f26c49f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b699e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/envs/ctp/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda113.so\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 113\n",
      "CUDA SETUP: Loading binary /opt/conda/envs/ctp/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda113.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/ctp/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /opt/conda/envs/ctp did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-13 18:05:11,927] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, TrainingArguments, Trainer, AutoConfig\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b389266",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_num=-1\n",
    "max_source_len = 512\n",
    "max_target_len=512\n",
    "#cache_data = 'cache_data/summarize_python'\n",
    "cache_data = 'batchsize_experiments/pandu'\n",
    "load = 'Salesforce/codet5p-16b'\n",
    "# load = \"HuggingFaceH4/starchat-alpha\"\n",
    "# Training\n",
    "epochs=10\n",
    "lr=5e-3\n",
    "lr_warmup_steps=200\n",
    "batch_size_per_replica=1\n",
    "grad_acc_steps=16\n",
    "local_rank=-1\n",
    "deepspeed=\"ds.json\"\n",
    "fp16=True\n",
    "\n",
    "# Logging and stuff\n",
    "save_dir=\"saved_models/summarize_python\"\n",
    "log_freq=10\n",
    "save_freq=500\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba8cacc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(model, train_data):\n",
    "    print(f\"Starting main loop\")\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        report_to='tensorboard',\n",
    "        output_dir=save_dir,\n",
    "        overwrite_output_dir=False,\n",
    "\n",
    "        do_train=True,\n",
    "        save_strategy='epoch',\n",
    "\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=batch_size_per_replica,\n",
    "        gradient_accumulation_steps=grad_acc_steps,\n",
    "        learning_rate=lr,\n",
    "        weight_decay=0.05,\n",
    "        warmup_steps=lr_warmup_steps,\n",
    "\n",
    "        logging_dir=save_dir,\n",
    "        logging_first_step=True,\n",
    "        logging_steps=log_freq,\n",
    "        save_total_limit=1,\n",
    "\n",
    "        dataloader_drop_last=True,\n",
    "        dataloader_num_workers=2,\n",
    "\n",
    "        local_rank=local_rank,\n",
    "#         deepspeed=deepspeed,\n",
    "#         fp16=fp16,\n",
    "        bf16 = True\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_data,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    if local_rank in [0, -1]:\n",
    "        final_checkpoint_dir = os.path.join(save_dir, \"final_checkpoint\")\n",
    "        model.save_pretrained(final_checkpoint_dir)\n",
    "        print(f'  ==> Finish training and save to {final_checkpoint_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "624ad6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for deepspeed\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '9994' # modify if RuntimeError: Address already in use\n",
    "os.environ['RANK'] = \"0\"\n",
    "os.environ['LOCAL_RANK'] = \"0\"\n",
    "os.environ['WORLD_SIZE'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37956e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(load)\n",
    "config = AutoConfig.from_pretrained(load, trust_remote_code=True, revision=\"main\")\n",
    "config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "config.pad_token_id = tokenizer.pad_token_id\n",
    "#for deepspeed\n",
    "config.max_position_embeddings = 512\n",
    "# print('Model hidden size: ', config.cross_attention_hidden_size)\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "def get_model_size(model):\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    model_size = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    return \"{}M\".format(round(model_size / 1e+6))\n",
    "\n",
    "def freeze_decoder_except_xattn_codegen(model):\n",
    "    print(f'Para before freezing: {model.num_parameters()}, trainable para: {get_model_size(model)}')\n",
    "    for param in model.decoder.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    num_decoder_layers = model.decoder.config.n_layer\n",
    "    for i in range(num_decoder_layers):\n",
    "        each_decoder_layer = model.decoder.transformer.h[i]\n",
    "        if hasattr(each_decoder_layer, 'crossattention'):\n",
    "            for param in each_decoder_layer.crossattention.parameters():\n",
    "                param.requires_grad = True\n",
    "            each_decoder_layer.crossattention.to(torch.float32)\n",
    "\n",
    "        if hasattr(each_decoder_layer, 'alpha_xattn'):\n",
    "            each_decoder_layer.alpha_xattn.requires_grad = True\n",
    "    print(f'Para after freezing: {model.num_parameters()}, trainable para: {get_model_size(model)}')\n",
    "\n",
    "def convert_size(size_bytes):\n",
    "   if size_bytes == 0:\n",
    "       return \"0B\"\n",
    "   size_name = (\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\", \"EB\", \"ZB\", \"YB\")\n",
    "   i = int(math.floor(math.log(size_bytes, 512)))\n",
    "   p = math.pow(1024, i)\n",
    "   s = round(size_bytes / p, 2)\n",
    "   return \"%s %s\" % (s, size_name[i])\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    source = [ex for ex in examples[\"question\"]]\n",
    "#     source = [ex for ex in examples[\"input\"]]\n",
    "    target = [ex for ex in examples[\"answer\"]]\n",
    "\n",
    "    model_inputs = tokenizer(source, max_length=max_source_len, padding=\"max_length\", truncation=True)\n",
    "    labels = tokenizer(target, max_length=max_target_len, padding=\"max_length\", truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"].copy()\n",
    "    model_inputs[\"labels\"] = [\n",
    "        [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in model_inputs[\"labels\"]\n",
    "    ]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def load_tokenize_data():\n",
    "#     if os.path.exists(cache_data):\n",
    "#         train_data = load_from_disk(cache_data)\n",
    "#         print(f'  ==> Loaded {len(train_data)} samples')\n",
    "# #         res =  convert_size(train_data.size_in_bytes)\n",
    "# #         print('Dataset Size:', res)\n",
    "#         return train_data, config\n",
    "#     else:\n",
    "        datasets = load_from_disk(\"/home/unnati/batchsize_experiments/pandu\")\n",
    "#         datasets = load_dataset(\"semeru/text-code-codesummarization\", split=\"validation\")\n",
    "        #datasets = datasets.select(range(20))\n",
    "        #res =  convert_size(datasets.size_in_bytes)\n",
    "        #print('Dataset Size:', res)\n",
    "        train_data = datasets.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            remove_columns=datasets.column_names,\n",
    "            num_proc=64,\n",
    "            load_from_cache_file=False,\n",
    "        )\n",
    "        print(f'  ==> Loaded {len(train_data)} samples')\n",
    "        # train_data.save_to_disk(cache_data)\n",
    "        # print(f'  ==> Saved to {cache_data}')\n",
    "\n",
    "        return train_data, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c024381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer'],\n",
      "    num_rows: 197\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "datasets = load_from_disk(\"/home/unnati/batchsize_experiments/pandu\")\n",
    "print(datasets )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9d05012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question\n"
     ]
    }
   ],
   "source": [
    "print(datasets['question'][0])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1443969c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answers\n"
     ]
    }
   ],
   "source": [
    "print(datasets['answer'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99044549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62e4971c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=64):   0%|          | 0/197 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Loaded 197 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/197 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Saved to batchsize_experiments/pandu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2320156761ce4da19defacd76f3d888c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  8.41 GB\n",
      "PEFT Model:  8.42 GB\n",
      "trainable params: 3,743,744 || all params: 8,434,923,520 || trainable%: 0.04438385233871095\n",
      "None\n",
      "  ==> Loaded model from Salesforce/codet5p-16b, model size 8434923520\n",
      "Starting main loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/unnati/.local/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 24:25, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.550400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.739300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.505700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.411900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.205700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.021300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.902400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.790900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.654800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.754700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.690900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.709300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.706200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Finish training and save to saved_models/summarize_python/final_checkpoint\n"
     ]
    }
   ],
   "source": [
    "train_data, config = load_tokenize_data()\n",
    "\n",
    "#LORA\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "# lora_config = LoraConfig(\n",
    "#     r=16,\n",
    "#     lora_alpha=32,\n",
    "#     target_modules=[\"q_proj\",\"v_proj\"],\n",
    "#     lora_dropout=0.05,\n",
    "#     bias=\"none\",\n",
    "#     task_type=TaskType.SEQ_2_SEQ_LM\n",
    "# )\n",
    "\n",
    "\n",
    "#QLORA\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "qlora_config = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    target_modules=[\"q_proj\",\"v_proj\"]\n",
    ")\n",
    "\n",
    "\n",
    "if data_num != -1:\n",
    "    train_data = train_data.select([i for i in range(data_num)])\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(load,config=config,trust_remote_code=True,\n",
    "                                              revision=\"main\", \n",
    "#                                               low_cpu_mem_usage=True, \n",
    "                                              quantization_config=bnb_config)\n",
    "# freeze_decoder_except_xattn_codegen(model)\n",
    "print('Model: ', convert_size(model.get_memory_footprint()))\n",
    "model = get_peft_model(model, qlora_config)\n",
    "print('PEFT Model: ',convert_size(model.get_memory_footprint()))\n",
    "print(model.print_trainable_parameters())\n",
    "\n",
    "print(f\"  ==> Loaded model from {load}, model size {model.num_parameters()}\")\n",
    "\n",
    "run_training(model, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4272d99d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('saved_models/summarize_python/final_checkpoint/tokenizer_config.json',\n",
       " 'saved_models/summarize_python/final_checkpoint/special_tokens_map.json',\n",
       " 'saved_models/summarize_python/final_checkpoint/vocab.json',\n",
       " 'saved_models/summarize_python/final_checkpoint/merges.txt',\n",
       " 'saved_models/summarize_python/final_checkpoint/added_tokens.json',\n",
       " 'saved_models/summarize_python/final_checkpoint/tokenizer.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('saved_models/summarize_python/final_checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d186a72b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1aca1e78",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c9502d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "import time, os\n",
    "\n",
    "load = \"Salesforce/instructcodet5p-16b\"\n",
    "device = \"cuda\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "max_len = 512\n",
    "\n",
    "import re\n",
    "def truncate(completion):\n",
    "    import re\n",
    "    \n",
    "    def find_re(string, pattern, start_pos):\n",
    "        m = pattern.search(string, start_pos)\n",
    "        return m.start() if m else -1\n",
    "\n",
    "    terminals = [re.compile(r, re.MULTILINE) for r in [re.escape('<|end|>'),\"^'''\", '^\"\"\"', '\\n\\n\\n']]\n",
    "\n",
    "    prints = list(re.finditer('^print', completion, re.MULTILINE))\n",
    "    if len(prints) > 1:\n",
    "        completion = completion[:prints[1].start()]\n",
    "\n",
    "    defs = list(re.finditer('^def', completion, re.MULTILINE))\n",
    "    if len(defs) > 1:\n",
    "        completion = completion[:defs[1].start()]\n",
    "\n",
    "    start_pos = 0\n",
    "\n",
    "    terminals_pos = [pos for pos in [find_re(completion, terminal, start_pos) for terminal in terminals] if pos != -1]\n",
    "    if len(terminals_pos) > 0:\n",
    "        return completion[:min(terminals_pos)]\n",
    "    else:\n",
    "        return completion  \n",
    "\n",
    "def preprocess_function(examples):\n",
    "    source = [ex for ex in examples[\"question\"]]\n",
    "#     source = [ex for ex in examples[\"input\"]]\n",
    "    target = [ex for ex in examples[\"answer\"]]\n",
    "\n",
    "    model_inputs = tokenizer(source, max_length=max_source_len, padding=\"max_length\", truncation=True)\n",
    "    labels = tokenizer(target, max_length=max_target_len, padding=\"max_length\", truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"].copy()\n",
    "    model_inputs[\"labels\"] = [\n",
    "        [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in model_inputs[\"labels\"]\n",
    "    ]\n",
    "    return model_inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71df6bb",
   "metadata": {},
   "source": [
    "# Inference without fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72804916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c18d5a6524e4034ac6dff1fe8fae7df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(load)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(load,\n",
    "                                              torch_dtype=torch.float16,\n",
    "                                              low_cpu_mem_usage=True,\n",
    "                                              trust_remote_code=True,\n",
    "                                              quantization_config=bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15552ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inf_without_ft(test_ip):\n",
    "    start_time= time.time()\n",
    "    fmt_test_ip = preprocess_function(test_ip)\n",
    "    encoding = tokenizer(fmt_test_ip, return_tensors=\"pt\").to(device)\n",
    "    encoding['decoder_input_ids'] = encoding['input_ids'].clone()\n",
    "    outputs = model.generate(**encoding, max_length=512)\n",
    "    resp = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    stop_time=time.time()\n",
    "    duration =stop_time - start_time\n",
    "    return resp, duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f38ac565",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ip = \"\"\"i want to drop a specific row from the dataframe. Provide me all the possible ways to to achive the task. Dataframe is stored in a variable df\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f964bff",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m resp, duration \u001b[38;5;241m=\u001b[39m\u001b[43minf_without_ft\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_ip\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mstr\u001b[39m(duration)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m s\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(resp[\u001b[38;5;28mlen\u001b[39m(fmt_test_ip):])\n",
      "Cell \u001b[0;32mIn[23], line 3\u001b[0m, in \u001b[0;36minf_without_ft\u001b[0;34m(test_ip)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minf_without_ft\u001b[39m(test_ip):\n\u001b[1;32m      2\u001b[0m     start_time\u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 3\u001b[0m     fmt_test_ip \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_ip\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m tokenizer(fmt_test_ip, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      5\u001b[0m     encoding[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecoder_input_ids\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m encoding[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mclone()\n",
      "Cell \u001b[0;32mIn[22], line 44\u001b[0m, in \u001b[0;36mpreprocess_function\u001b[0;34m(examples)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_function\u001b[39m(examples):\n\u001b[0;32m---> 44\u001b[0m     source \u001b[38;5;241m=\u001b[39m [ex \u001b[38;5;28;01mfor\u001b[39;00m ex \u001b[38;5;129;01min\u001b[39;00m \u001b[43mexamples\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m]\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m#     source = [ex for ex in examples[\"input\"]]\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     target \u001b[38;5;241m=\u001b[39m [ex \u001b[38;5;28;01mfor\u001b[39;00m ex \u001b[38;5;129;01min\u001b[39;00m examples[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "resp, duration =inf_without_ft(test_ip)\n",
    "print(str(duration)+' s')\n",
    "print(resp[len(fmt_test_ip):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830418ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
